<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Penalized regressions and sparse hedging for minimum variance portfolios | Machine Learning for Factor Investing</title>
  <meta name="description" content="Chapter 6 Penalized regressions and sparse hedging for minimum variance portfolios | Machine Learning for Factor Investing" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Penalized regressions and sparse hedging for minimum variance portfolios | Machine Learning for Factor Investing" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Penalized regressions and sparse hedging for minimum variance portfolios | Machine Learning for Factor Investing" />
  
  
  

<meta name="author" content="Guillaume Coqueret and Tony Guida" />


<meta name="date" content="2020-04-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="Data.html"/>
<link rel="next" href="trees.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="part"><span><b>I Introduction</b></span></li>
<li class="chapter" data-level="1" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="preface.html"><a href="preface.html#what-this-book-is-not-about"><i class="fa fa-check"></i><b>1.1</b> What this book is not about</a></li>
<li class="chapter" data-level="1.2" data-path="preface.html"><a href="preface.html#the-targeted-audience"><i class="fa fa-check"></i><b>1.2</b> The targeted audience</a></li>
<li class="chapter" data-level="1.3" data-path="preface.html"><a href="preface.html#how-this-book-is-structured"><i class="fa fa-check"></i><b>1.3</b> How this book is structured</a></li>
<li class="chapter" data-level="1.4" data-path="preface.html"><a href="preface.html#companion-website"><i class="fa fa-check"></i><b>1.4</b> Companion website</a></li>
<li class="chapter" data-level="1.5" data-path="preface.html"><a href="preface.html#why-r"><i class="fa fa-check"></i><b>1.5</b> Why R?</a></li>
<li class="chapter" data-level="1.6" data-path="preface.html"><a href="preface.html#coding-instructions"><i class="fa fa-check"></i><b>1.6</b> Coding instructions</a></li>
<li class="chapter" data-level="1.7" data-path="preface.html"><a href="preface.html#acknowledgements"><i class="fa fa-check"></i><b>1.7</b> Acknowledgements</a></li>
<li class="chapter" data-level="1.8" data-path="preface.html"><a href="preface.html#future-developments"><i class="fa fa-check"></i><b>1.8</b> Future developments</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="notdata.html"><a href="notdata.html"><i class="fa fa-check"></i><b>2</b> Notations and data</a><ul>
<li class="chapter" data-level="2.1" data-path="notdata.html"><a href="notdata.html#notations"><i class="fa fa-check"></i><b>2.1</b> Notations</a></li>
<li class="chapter" data-level="2.2" data-path="notdata.html"><a href="notdata.html#dataset"><i class="fa fa-check"></i><b>2.2</b> Dataset</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>3</b> Introduction</a><ul>
<li class="chapter" data-level="3.1" data-path="intro.html"><a href="intro.html#context"><i class="fa fa-check"></i><b>3.1</b> Context</a></li>
<li class="chapter" data-level="3.2" data-path="intro.html"><a href="intro.html#portfolio-construction-the-workflow"><i class="fa fa-check"></i><b>3.2</b> Portfolio construction: the workflow</a></li>
<li class="chapter" data-level="3.3" data-path="intro.html"><a href="intro.html#machine-learning-is-no-magic-wand"><i class="fa fa-check"></i><b>3.3</b> Machine Learning is no Magic Wand</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="factor.html"><a href="factor.html"><i class="fa fa-check"></i><b>4</b> Factor investing and asset pricing anomalies</a><ul>
<li class="chapter" data-level="4.1" data-path="factor.html"><a href="factor.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="factor.html"><a href="factor.html#detecting-anomalies"><i class="fa fa-check"></i><b>4.2</b> Detecting anomalies</a><ul>
<li class="chapter" data-level="4.2.1" data-path="factor.html"><a href="factor.html#simple-portfolio-sorts"><i class="fa fa-check"></i><b>4.2.1</b> Simple portfolio sorts</a></li>
<li class="chapter" data-level="4.2.2" data-path="factor.html"><a href="factor.html#factors"><i class="fa fa-check"></i><b>4.2.2</b> Factors</a></li>
<li class="chapter" data-level="4.2.3" data-path="factor.html"><a href="factor.html#predictive-regressions-sorts-and-p-value-issues"><i class="fa fa-check"></i><b>4.2.3</b> Predictive regressions, sorts, and p-value issues</a></li>
<li class="chapter" data-level="4.2.4" data-path="factor.html"><a href="factor.html#fama-macbeth-regressions"><i class="fa fa-check"></i><b>4.2.4</b> Fama-Macbeth regressions</a></li>
<li class="chapter" data-level="4.2.5" data-path="factor.html"><a href="factor.html#factor-competition"><i class="fa fa-check"></i><b>4.2.5</b> Factor competition</a></li>
<li class="chapter" data-level="4.2.6" data-path="factor.html"><a href="factor.html#advanced-techniques"><i class="fa fa-check"></i><b>4.2.6</b> Advanced techniques</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="factor.html"><a href="factor.html#factors-or-characteristics"><i class="fa fa-check"></i><b>4.3</b> Factors or characteristics?</a></li>
<li class="chapter" data-level="4.4" data-path="factor.html"><a href="factor.html#hot-topics-momentum-timing-and-esg"><i class="fa fa-check"></i><b>4.4</b> Hot topics: momentum, timing and ESG</a><ul>
<li class="chapter" data-level="4.4.1" data-path="factor.html"><a href="factor.html#factor-momentum"><i class="fa fa-check"></i><b>4.4.1</b> Factor momentum</a></li>
<li class="chapter" data-level="4.4.2" data-path="factor.html"><a href="factor.html#factor-timing"><i class="fa fa-check"></i><b>4.4.2</b> Factor timing</a></li>
<li class="chapter" data-level="4.4.3" data-path="factor.html"><a href="factor.html#the-green-factors"><i class="fa fa-check"></i><b>4.4.3</b> The green factors</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="factor.html"><a href="factor.html#the-link-with-machine-learning"><i class="fa fa-check"></i><b>4.5</b> The link with machine learning</a><ul>
<li class="chapter" data-level="4.5.1" data-path="factor.html"><a href="factor.html#a-short-list-of-recent-references"><i class="fa fa-check"></i><b>4.5.1</b> A short list of recent references</a></li>
<li class="chapter" data-level="4.5.2" data-path="factor.html"><a href="factor.html#explicit-connections-with-asset-pricing-models"><i class="fa fa-check"></i><b>4.5.2</b> Explicit connections with asset pricing models</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="factor.html"><a href="factor.html#coding-exercises"><i class="fa fa-check"></i><b>4.6</b> Coding exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Data.html"><a href="Data.html"><i class="fa fa-check"></i><b>5</b> Data preprocessing</a><ul>
<li class="chapter" data-level="5.1" data-path="Data.html"><a href="Data.html#know-your-data"><i class="fa fa-check"></i><b>5.1</b> Know your data</a></li>
<li class="chapter" data-level="5.2" data-path="Data.html"><a href="Data.html#missing-data"><i class="fa fa-check"></i><b>5.2</b> Missing data</a></li>
<li class="chapter" data-level="5.3" data-path="Data.html"><a href="Data.html#outlier-detection"><i class="fa fa-check"></i><b>5.3</b> Outlier detection</a></li>
<li class="chapter" data-level="5.4" data-path="Data.html"><a href="Data.html#feateng"><i class="fa fa-check"></i><b>5.4</b> Feature engineering</a><ul>
<li class="chapter" data-level="5.4.1" data-path="Data.html"><a href="Data.html#feature-selection"><i class="fa fa-check"></i><b>5.4.1</b> Feature selection</a></li>
<li class="chapter" data-level="5.4.2" data-path="Data.html"><a href="Data.html#scaling"><i class="fa fa-check"></i><b>5.4.2</b> Scaling the predictors</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="Data.html"><a href="Data.html#labelling"><i class="fa fa-check"></i><b>5.5</b> Labelling</a><ul>
<li class="chapter" data-level="5.5.1" data-path="Data.html"><a href="Data.html#simple-labels"><i class="fa fa-check"></i><b>5.5.1</b> Simple labels</a></li>
<li class="chapter" data-level="5.5.2" data-path="Data.html"><a href="Data.html#categorical-labels"><i class="fa fa-check"></i><b>5.5.2</b> Categorical labels</a></li>
<li class="chapter" data-level="5.5.3" data-path="Data.html"><a href="Data.html#the-triple-barrier-method"><i class="fa fa-check"></i><b>5.5.3</b> The triple barrier method</a></li>
<li class="chapter" data-level="5.5.4" data-path="Data.html"><a href="Data.html#filtering-the-sample"><i class="fa fa-check"></i><b>5.5.4</b> Filtering the sample</a></li>
<li class="chapter" data-level="5.5.5" data-path="Data.html"><a href="Data.html#horizons"><i class="fa fa-check"></i><b>5.5.5</b> Return horizons</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="Data.html"><a href="Data.html#pers"><i class="fa fa-check"></i><b>5.6</b> Handling persistence</a></li>
<li class="chapter" data-level="5.7" data-path="Data.html"><a href="Data.html#extensions"><i class="fa fa-check"></i><b>5.7</b> Extensions</a><ul>
<li class="chapter" data-level="5.7.1" data-path="Data.html"><a href="Data.html#transforming-features"><i class="fa fa-check"></i><b>5.7.1</b> Transforming features</a></li>
<li class="chapter" data-level="5.7.2" data-path="Data.html"><a href="Data.html#macrovar"><i class="fa fa-check"></i><b>5.7.2</b> Macro-economic variables</a></li>
<li class="chapter" data-level="5.7.3" data-path="Data.html"><a href="Data.html#active-learning"><i class="fa fa-check"></i><b>5.7.3</b> Active learning</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="Data.html"><a href="Data.html#additional-code-and-results"><i class="fa fa-check"></i><b>5.8</b> Additional code and results</a><ul>
<li class="chapter" data-level="5.8.1" data-path="Data.html"><a href="Data.html#impact-of-rescaling-graphical-representation"><i class="fa fa-check"></i><b>5.8.1</b> Impact of rescaling: graphical representation</a></li>
<li class="chapter" data-level="5.8.2" data-path="Data.html"><a href="Data.html#impact-of-rescaling-toy-example"><i class="fa fa-check"></i><b>5.8.2</b> Impact of rescaling: toy example</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="Data.html"><a href="Data.html#coding-exercises-1"><i class="fa fa-check"></i><b>5.9</b> Coding exercises</a></li>
</ul></li>
<li class="part"><span><b>II Common supervised algorithms</b></span></li>
<li class="chapter" data-level="6" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>6</b> Penalized regressions and sparse hedging for minimum variance portfolios</a><ul>
<li class="chapter" data-level="6.1" data-path="lasso.html"><a href="lasso.html#penalised-regressions"><i class="fa fa-check"></i><b>6.1</b> Penalised regressions</a><ul>
<li class="chapter" data-level="6.1.1" data-path="lasso.html"><a href="lasso.html#penreg"><i class="fa fa-check"></i><b>6.1.1</b> Simple regressions</a></li>
<li class="chapter" data-level="6.1.2" data-path="lasso.html"><a href="lasso.html#forms-of-penalizations"><i class="fa fa-check"></i><b>6.1.2</b> Forms of penalizations</a></li>
<li class="chapter" data-level="6.1.3" data-path="lasso.html"><a href="lasso.html#illustrations"><i class="fa fa-check"></i><b>6.1.3</b> Illustrations</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="lasso.html"><a href="lasso.html#sparse-hedging-for-minimum-variance-portfolios"><i class="fa fa-check"></i><b>6.2</b> Sparse hedging for minimum variance portfolios</a><ul>
<li class="chapter" data-level="6.2.1" data-path="lasso.html"><a href="lasso.html#presentation-and-derivations"><i class="fa fa-check"></i><b>6.2.1</b> Presentation and derivations</a></li>
<li class="chapter" data-level="6.2.2" data-path="lasso.html"><a href="lasso.html#sparseex"><i class="fa fa-check"></i><b>6.2.2</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="lasso.html"><a href="lasso.html#predictive-regressions"><i class="fa fa-check"></i><b>6.3</b> Predictive regressions</a><ul>
<li class="chapter" data-level="6.3.1" data-path="lasso.html"><a href="lasso.html#literature-review-and-principle"><i class="fa fa-check"></i><b>6.3.1</b> Literature review and principle</a></li>
<li class="chapter" data-level="6.3.2" data-path="lasso.html"><a href="lasso.html#code-and-results"><i class="fa fa-check"></i><b>6.3.2</b> Code and results</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="lasso.html"><a href="lasso.html#coding-exercise"><i class="fa fa-check"></i><b>6.4</b> Coding exercise</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="trees.html"><a href="trees.html"><i class="fa fa-check"></i><b>7</b> Tree-based methods</a><ul>
<li class="chapter" data-level="7.1" data-path="trees.html"><a href="trees.html#simple-trees"><i class="fa fa-check"></i><b>7.1</b> Simple trees</a><ul>
<li class="chapter" data-level="7.1.1" data-path="trees.html"><a href="trees.html#principle"><i class="fa fa-check"></i><b>7.1.1</b> Principle</a></li>
<li class="chapter" data-level="7.1.2" data-path="trees.html"><a href="trees.html#treeclass"><i class="fa fa-check"></i><b>7.1.2</b> Further details on classification</a></li>
<li class="chapter" data-level="7.1.3" data-path="trees.html"><a href="trees.html#pruning-criteria"><i class="fa fa-check"></i><b>7.1.3</b> Pruning criteria</a></li>
<li class="chapter" data-level="7.1.4" data-path="trees.html"><a href="trees.html#code-and-interpretation"><i class="fa fa-check"></i><b>7.1.4</b> Code and interpretation</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="trees.html"><a href="trees.html#random-forests"><i class="fa fa-check"></i><b>7.2</b> Random forests</a><ul>
<li class="chapter" data-level="7.2.1" data-path="trees.html"><a href="trees.html#principle-1"><i class="fa fa-check"></i><b>7.2.1</b> Principle</a></li>
<li class="chapter" data-level="7.2.2" data-path="trees.html"><a href="trees.html#code-and-results-1"><i class="fa fa-check"></i><b>7.2.2</b> Code and results</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="trees.html"><a href="trees.html#adaboost"><i class="fa fa-check"></i><b>7.3</b> Boosted trees: Adaboost</a><ul>
<li class="chapter" data-level="7.3.1" data-path="trees.html"><a href="trees.html#methodology"><i class="fa fa-check"></i><b>7.3.1</b> Methodology</a></li>
<li class="chapter" data-level="7.3.2" data-path="trees.html"><a href="trees.html#illustration"><i class="fa fa-check"></i><b>7.3.2</b> Illustration</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="trees.html"><a href="trees.html#boosted-trees-extreme-gradient-boosting"><i class="fa fa-check"></i><b>7.4</b> Boosted trees: extreme gradient boosting</a><ul>
<li class="chapter" data-level="7.4.1" data-path="trees.html"><a href="trees.html#managing-loss"><i class="fa fa-check"></i><b>7.4.1</b> Managing Loss</a></li>
<li class="chapter" data-level="7.4.2" data-path="trees.html"><a href="trees.html#penalisation"><i class="fa fa-check"></i><b>7.4.2</b> Penalisation</a></li>
<li class="chapter" data-level="7.4.3" data-path="trees.html"><a href="trees.html#aggregation"><i class="fa fa-check"></i><b>7.4.3</b> Aggregation</a></li>
<li class="chapter" data-level="7.4.4" data-path="trees.html"><a href="trees.html#tree-structure"><i class="fa fa-check"></i><b>7.4.4</b> Tree structure</a></li>
<li class="chapter" data-level="7.4.5" data-path="trees.html"><a href="trees.html#boostext"><i class="fa fa-check"></i><b>7.4.5</b> Extensions</a></li>
<li class="chapter" data-level="7.4.6" data-path="trees.html"><a href="trees.html#boostcode"><i class="fa fa-check"></i><b>7.4.6</b> Code and results</a></li>
<li class="chapter" data-level="7.4.7" data-path="trees.html"><a href="trees.html#instweight"><i class="fa fa-check"></i><b>7.4.7</b> Instance weighting</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="trees.html"><a href="trees.html#discussion"><i class="fa fa-check"></i><b>7.5</b> Discussion</a></li>
<li class="chapter" data-level="7.6" data-path="trees.html"><a href="trees.html#coding-exercises-2"><i class="fa fa-check"></i><b>7.6</b> Coding exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="NN.html"><a href="NN.html"><i class="fa fa-check"></i><b>8</b> Neural networks</a><ul>
<li class="chapter" data-level="8.1" data-path="NN.html"><a href="NN.html#the-original-perceptron"><i class="fa fa-check"></i><b>8.1</b> The original perceptron</a></li>
<li class="chapter" data-level="8.2" data-path="NN.html"><a href="NN.html#multilayer-perceptron-mlp"><i class="fa fa-check"></i><b>8.2</b> Multilayer perceptron (MLP)</a><ul>
<li class="chapter" data-level="8.2.1" data-path="NN.html"><a href="NN.html#introduction-and-notations"><i class="fa fa-check"></i><b>8.2.1</b> Introduction and notations</a></li>
<li class="chapter" data-level="8.2.2" data-path="NN.html"><a href="NN.html#universal-approximation"><i class="fa fa-check"></i><b>8.2.2</b> Universal approximation</a></li>
<li class="chapter" data-level="8.2.3" data-path="NN.html"><a href="NN.html#backprop"><i class="fa fa-check"></i><b>8.2.3</b> Learning via back-propagation</a></li>
<li class="chapter" data-level="8.2.4" data-path="NN.html"><a href="NN.html#further-details-on-classification"><i class="fa fa-check"></i><b>8.2.4</b> Further details on classification</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="NN.html"><a href="NN.html#howdeep"><i class="fa fa-check"></i><b>8.3</b> How deep should we go? And other practical issues</a><ul>
<li class="chapter" data-level="8.3.1" data-path="NN.html"><a href="NN.html#architectural-choices"><i class="fa fa-check"></i><b>8.3.1</b> Architectural choices</a></li>
<li class="chapter" data-level="8.3.2" data-path="NN.html"><a href="NN.html#frequency-of-weight-updates-and-learning-duration"><i class="fa fa-check"></i><b>8.3.2</b> Frequency of weight updates and learning duration</a></li>
<li class="chapter" data-level="8.3.3" data-path="NN.html"><a href="NN.html#penalizations-and-dropout"><i class="fa fa-check"></i><b>8.3.3</b> Penalizations and dropout</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="NN.html"><a href="NN.html#code-samples-and-comments-for-vanilla-mlp"><i class="fa fa-check"></i><b>8.4</b> Code samples and comments for vanilla MLP</a><ul>
<li class="chapter" data-level="8.4.1" data-path="NN.html"><a href="NN.html#regression-example"><i class="fa fa-check"></i><b>8.4.1</b> Regression example</a></li>
<li class="chapter" data-level="8.4.2" data-path="NN.html"><a href="NN.html#classification-example"><i class="fa fa-check"></i><b>8.4.2</b> Classification example</a></li>
<li class="chapter" data-level="8.4.3" data-path="NN.html"><a href="NN.html#custloss"><i class="fa fa-check"></i><b>8.4.3</b> Custom losses</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="NN.html"><a href="NN.html#recurrent-networks"><i class="fa fa-check"></i><b>8.5</b> Recurrent networks</a><ul>
<li class="chapter" data-level="8.5.1" data-path="NN.html"><a href="NN.html#presentation"><i class="fa fa-check"></i><b>8.5.1</b> Presentation</a></li>
<li class="chapter" data-level="8.5.2" data-path="NN.html"><a href="NN.html#code-and-results-2"><i class="fa fa-check"></i><b>8.5.2</b> Code and results</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="NN.html"><a href="NN.html#other-common-architectures"><i class="fa fa-check"></i><b>8.6</b> Other common architectures</a><ul>
<li class="chapter" data-level="8.6.1" data-path="NN.html"><a href="NN.html#generative-aversarial-networks"><i class="fa fa-check"></i><b>8.6.1</b> Generative adversarial networks</a></li>
<li class="chapter" data-level="8.6.2" data-path="NN.html"><a href="NN.html#autoencoders"><i class="fa fa-check"></i><b>8.6.2</b> Auto-encoders</a></li>
<li class="chapter" data-level="8.6.3" data-path="NN.html"><a href="NN.html#a-word-on-convolutional-networks"><i class="fa fa-check"></i><b>8.6.3</b> A word on convolutional networks</a></li>
<li class="chapter" data-level="8.6.4" data-path="NN.html"><a href="NN.html#advanced-architectures"><i class="fa fa-check"></i><b>8.6.4</b> Advanced architectures</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="NN.html"><a href="NN.html#coding-exercise-1"><i class="fa fa-check"></i><b>8.7</b> Coding exercise</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="svm.html"><a href="svm.html"><i class="fa fa-check"></i><b>9</b> Support vector machines</a><ul>
<li class="chapter" data-level="9.1" data-path="svm.html"><a href="svm.html#svm-for-classification"><i class="fa fa-check"></i><b>9.1</b> SVM for classification</a></li>
<li class="chapter" data-level="9.2" data-path="svm.html"><a href="svm.html#svm-for-regression"><i class="fa fa-check"></i><b>9.2</b> SVM for regression</a></li>
<li class="chapter" data-level="9.3" data-path="svm.html"><a href="svm.html#practice"><i class="fa fa-check"></i><b>9.3</b> Practice</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bayes.html"><a href="bayes.html"><i class="fa fa-check"></i><b>10</b> Bayesian methods</a><ul>
<li class="chapter" data-level="10.1" data-path="bayes.html"><a href="bayes.html#the-bayesian-framework"><i class="fa fa-check"></i><b>10.1</b> The Bayesian framework</a></li>
<li class="chapter" data-level="10.2" data-path="bayes.html"><a href="bayes.html#bayesian-sampling"><i class="fa fa-check"></i><b>10.2</b> Bayesian sampling</a><ul>
<li class="chapter" data-level="10.2.1" data-path="bayes.html"><a href="bayes.html#gibbs-sampling"><i class="fa fa-check"></i><b>10.2.1</b> Gibbs sampling</a></li>
<li class="chapter" data-level="10.2.2" data-path="bayes.html"><a href="bayes.html#metropolis-hastings-sampling"><i class="fa fa-check"></i><b>10.2.2</b> Metropolis-Hastings sampling</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="bayes.html"><a href="bayes.html#bayesian-linear-regression"><i class="fa fa-check"></i><b>10.3</b> Bayesian linear regression</a></li>
<li class="chapter" data-level="10.4" data-path="bayes.html"><a href="bayes.html#naive-bayes-classifier"><i class="fa fa-check"></i><b>10.4</b> Naive Bayes classifier</a></li>
<li class="chapter" data-level="10.5" data-path="bayes.html"><a href="bayes.html#BART"><i class="fa fa-check"></i><b>10.5</b> Bayesian additive trees</a><ul>
<li class="chapter" data-level="10.5.1" data-path="bayes.html"><a href="bayes.html#general-formulation"><i class="fa fa-check"></i><b>10.5.1</b> General formulation</a></li>
<li class="chapter" data-level="10.5.2" data-path="bayes.html"><a href="bayes.html#priors"><i class="fa fa-check"></i><b>10.5.2</b> Priors</a></li>
<li class="chapter" data-level="10.5.3" data-path="bayes.html"><a href="bayes.html#sampling-and-predictions"><i class="fa fa-check"></i><b>10.5.3</b> Sampling and predictions</a></li>
<li class="chapter" data-level="10.5.4" data-path="bayes.html"><a href="bayes.html#code"><i class="fa fa-check"></i><b>10.5.4</b> Code</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III From predictions to portfolios</b></span></li>
<li class="chapter" data-level="11" data-path="valtune.html"><a href="valtune.html"><i class="fa fa-check"></i><b>11</b> Validating and tuning</a><ul>
<li class="chapter" data-level="11.1" data-path="valtune.html"><a href="valtune.html#mlmetrics"><i class="fa fa-check"></i><b>11.1</b> Learning metrics</a><ul>
<li class="chapter" data-level="11.1.1" data-path="valtune.html"><a href="valtune.html#regression-analysis"><i class="fa fa-check"></i><b>11.1.1</b> Regression analysis</a></li>
<li class="chapter" data-level="11.1.2" data-path="valtune.html"><a href="valtune.html#classification-analysis"><i class="fa fa-check"></i><b>11.1.2</b> Classification analysis</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="valtune.html"><a href="valtune.html#validation"><i class="fa fa-check"></i><b>11.2</b> Validation</a><ul>
<li class="chapter" data-level="11.2.1" data-path="valtune.html"><a href="valtune.html#the-variance-bias-tradeoff-theory"><i class="fa fa-check"></i><b>11.2.1</b> The variance-bias tradeoff: theory</a></li>
<li class="chapter" data-level="11.2.2" data-path="valtune.html"><a href="valtune.html#the-variance-bias-tradeoff-illustration"><i class="fa fa-check"></i><b>11.2.2</b> The variance-bias tradeoff: illustration</a></li>
<li class="chapter" data-level="11.2.3" data-path="valtune.html"><a href="valtune.html#the-risk-of-overfitting-principle"><i class="fa fa-check"></i><b>11.2.3</b> The risk of overfitting: principle</a></li>
<li class="chapter" data-level="11.2.4" data-path="valtune.html"><a href="valtune.html#the-risk-of-overfitting-some-solutions"><i class="fa fa-check"></i><b>11.2.4</b> The risk of overfitting: some solutions</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="valtune.html"><a href="valtune.html#the-search-for-good-hyperparameters"><i class="fa fa-check"></i><b>11.3</b> The search for good hyperparameters</a><ul>
<li class="chapter" data-level="11.3.1" data-path="valtune.html"><a href="valtune.html#methods"><i class="fa fa-check"></i><b>11.3.1</b> Methods</a></li>
<li class="chapter" data-level="11.3.2" data-path="valtune.html"><a href="valtune.html#example-grid-search"><i class="fa fa-check"></i><b>11.3.2</b> Example: grid search</a></li>
<li class="chapter" data-level="11.3.3" data-path="valtune.html"><a href="valtune.html#example-bayesian-optimization"><i class="fa fa-check"></i><b>11.3.3</b> Example: Bayesian optimization</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="valtune.html"><a href="valtune.html#short-discussion-on-validation-in-backtests"><i class="fa fa-check"></i><b>11.4</b> Short discussion on validation in backtests</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ensemble.html"><a href="ensemble.html"><i class="fa fa-check"></i><b>12</b> Ensemble models</a><ul>
<li class="chapter" data-level="12.1" data-path="ensemble.html"><a href="ensemble.html#linear-ensembles"><i class="fa fa-check"></i><b>12.1</b> Linear ensembles</a><ul>
<li class="chapter" data-level="12.1.1" data-path="ensemble.html"><a href="ensemble.html#principles"><i class="fa fa-check"></i><b>12.1.1</b> Principles</a></li>
<li class="chapter" data-level="12.1.2" data-path="ensemble.html"><a href="ensemble.html#example"><i class="fa fa-check"></i><b>12.1.2</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="ensemble.html"><a href="ensemble.html#stacked-ensembles"><i class="fa fa-check"></i><b>12.2</b> Stacked ensembles</a><ul>
<li class="chapter" data-level="12.2.1" data-path="ensemble.html"><a href="ensemble.html#two-stage-training"><i class="fa fa-check"></i><b>12.2.1</b> Two stage training</a></li>
<li class="chapter" data-level="12.2.2" data-path="ensemble.html"><a href="ensemble.html#code-and-results-3"><i class="fa fa-check"></i><b>12.2.2</b> Code and results</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="ensemble.html"><a href="ensemble.html#extensions-1"><i class="fa fa-check"></i><b>12.3</b> Extensions</a><ul>
<li class="chapter" data-level="12.3.1" data-path="ensemble.html"><a href="ensemble.html#exogenous-variables"><i class="fa fa-check"></i><b>12.3.1</b> Exogenous variables</a></li>
<li class="chapter" data-level="12.3.2" data-path="ensemble.html"><a href="ensemble.html#shrinking-inter-model-correlations"><i class="fa fa-check"></i><b>12.3.2</b> Shrinking inter-model correlations</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="ensemble.html"><a href="ensemble.html#exercise"><i class="fa fa-check"></i><b>12.4</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="backtest.html"><a href="backtest.html"><i class="fa fa-check"></i><b>13</b> Portfolio backtesting</a><ul>
<li class="chapter" data-level="13.1" data-path="backtest.html"><a href="backtest.html#protocol"><i class="fa fa-check"></i><b>13.1</b> Setting the protocol</a></li>
<li class="chapter" data-level="13.2" data-path="backtest.html"><a href="backtest.html#turning-signals-into-portfolio-weights"><i class="fa fa-check"></i><b>13.2</b> Turning signals into portfolio weights</a></li>
<li class="chapter" data-level="13.3" data-path="backtest.html"><a href="backtest.html#perfmet"><i class="fa fa-check"></i><b>13.3</b> Performance metrics</a><ul>
<li class="chapter" data-level="13.3.1" data-path="backtest.html"><a href="backtest.html#discussion-1"><i class="fa fa-check"></i><b>13.3.1</b> Discussion</a></li>
<li class="chapter" data-level="13.3.2" data-path="backtest.html"><a href="backtest.html#pure-performance-and-risk-indicators"><i class="fa fa-check"></i><b>13.3.2</b> Pure performance and risk indicators</a></li>
<li class="chapter" data-level="13.3.3" data-path="backtest.html"><a href="backtest.html#factor-based-evaluation"><i class="fa fa-check"></i><b>13.3.3</b> Factor-based evaluation</a></li>
<li class="chapter" data-level="13.3.4" data-path="backtest.html"><a href="backtest.html#risk-adjusted-measures"><i class="fa fa-check"></i><b>13.3.4</b> Risk-adjusted measures</a></li>
<li class="chapter" data-level="13.3.5" data-path="backtest.html"><a href="backtest.html#transaction-costs-and-turnover"><i class="fa fa-check"></i><b>13.3.5</b> Transaction costs and turnover</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="backtest.html"><a href="backtest.html#common-errors-and-issues"><i class="fa fa-check"></i><b>13.4</b> Common errors and issues</a><ul>
<li class="chapter" data-level="13.4.1" data-path="backtest.html"><a href="backtest.html#forward-looking-data"><i class="fa fa-check"></i><b>13.4.1</b> Forward looking data</a></li>
<li class="chapter" data-level="13.4.2" data-path="backtest.html"><a href="backtest.html#backtest-overfitting"><i class="fa fa-check"></i><b>13.4.2</b> Backtest overfitting</a></li>
<li class="chapter" data-level="13.4.3" data-path="backtest.html"><a href="backtest.html#simple-safeguards"><i class="fa fa-check"></i><b>13.4.3</b> Simple safeguards</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="backtest.html"><a href="backtest.html#implication-of-non-stationarity-forecasting-is-hard"><i class="fa fa-check"></i><b>13.5</b> Implication of non-stationarity: forecasting is hard</a><ul>
<li class="chapter" data-level="13.5.1" data-path="backtest.html"><a href="backtest.html#general-comments"><i class="fa fa-check"></i><b>13.5.1</b> General comments</a></li>
<li class="chapter" data-level="13.5.2" data-path="backtest.html"><a href="backtest.html#the-no-free-lunch-theorem"><i class="fa fa-check"></i><b>13.5.2</b> The no free lunch theorem</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="backtest.html"><a href="backtest.html#example-1"><i class="fa fa-check"></i><b>13.6</b> Example</a></li>
<li class="chapter" data-level="13.7" data-path="backtest.html"><a href="backtest.html#coding-exercises-3"><i class="fa fa-check"></i><b>13.7</b> Coding exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Further important topics</b></span></li>
<li class="chapter" data-level="14" data-path="interp.html"><a href="interp.html"><i class="fa fa-check"></i><b>14</b> Interpretability</a><ul>
<li class="chapter" data-level="14.1" data-path="interp.html"><a href="interp.html#global-interpretations"><i class="fa fa-check"></i><b>14.1</b> Global interpretations</a><ul>
<li class="chapter" data-level="14.1.1" data-path="interp.html"><a href="interp.html#surr"><i class="fa fa-check"></i><b>14.1.1</b> Simple models as surrogates</a></li>
<li class="chapter" data-level="14.1.2" data-path="interp.html"><a href="interp.html#variable-importance"><i class="fa fa-check"></i><b>14.1.2</b> Variable importance (tree-based)</a></li>
<li class="chapter" data-level="14.1.3" data-path="interp.html"><a href="interp.html#variable-importance-agnostic"><i class="fa fa-check"></i><b>14.1.3</b> Variable importance (agnostic)</a></li>
<li class="chapter" data-level="14.1.4" data-path="interp.html"><a href="interp.html#partial-dependence-plot"><i class="fa fa-check"></i><b>14.1.4</b> Partial dependence plot</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="interp.html"><a href="interp.html#local-interpretations"><i class="fa fa-check"></i><b>14.2</b> Local interpretations</a><ul>
<li class="chapter" data-level="14.2.1" data-path="interp.html"><a href="interp.html#lime"><i class="fa fa-check"></i><b>14.2.1</b> LIME</a></li>
<li class="chapter" data-level="14.2.2" data-path="interp.html"><a href="interp.html#shapley-values"><i class="fa fa-check"></i><b>14.2.2</b> Shapley values</a></li>
<li class="chapter" data-level="14.2.3" data-path="interp.html"><a href="interp.html#breakdown"><i class="fa fa-check"></i><b>14.2.3</b> Breakdown</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="causality.html"><a href="causality.html"><i class="fa fa-check"></i><b>15</b> Two key concepts: causality and non-stationarity</a><ul>
<li class="chapter" data-level="15.1" data-path="causality.html"><a href="causality.html#causality-1"><i class="fa fa-check"></i><b>15.1</b> Causality</a><ul>
<li class="chapter" data-level="15.1.1" data-path="causality.html"><a href="causality.html#granger"><i class="fa fa-check"></i><b>15.1.1</b> Granger causality</a></li>
<li class="chapter" data-level="15.1.2" data-path="causality.html"><a href="causality.html#causal-additive-models"><i class="fa fa-check"></i><b>15.1.2</b> Causal additive models</a></li>
<li class="chapter" data-level="15.1.3" data-path="causality.html"><a href="causality.html#structural-time-series-models"><i class="fa fa-check"></i><b>15.1.3</b> Structural time-series models</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="causality.html"><a href="causality.html#nonstat"><i class="fa fa-check"></i><b>15.2</b> Dealing with changing environments</a><ul>
<li class="chapter" data-level="15.2.1" data-path="causality.html"><a href="causality.html#non-stationarity-yet-another-illustration"><i class="fa fa-check"></i><b>15.2.1</b> Non-stationarity: yet another illustration</a></li>
<li class="chapter" data-level="15.2.2" data-path="causality.html"><a href="causality.html#online-learning"><i class="fa fa-check"></i><b>15.2.2</b> Online learning</a></li>
<li class="chapter" data-level="15.2.3" data-path="causality.html"><a href="causality.html#homogeneous-transfer-learning"><i class="fa fa-check"></i><b>15.2.3</b> Homogeneous transfer learning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="unsup.html"><a href="unsup.html"><i class="fa fa-check"></i><b>16</b> Unsupervised learning</a><ul>
<li class="chapter" data-level="16.1" data-path="unsup.html"><a href="unsup.html#corpred"><i class="fa fa-check"></i><b>16.1</b> The problem with correlated predictors</a></li>
<li class="chapter" data-level="16.2" data-path="unsup.html"><a href="unsup.html#principal-component-analysis-and-autoencoders"><i class="fa fa-check"></i><b>16.2</b> Principal component analysis and autoencoders</a><ul>
<li class="chapter" data-level="16.2.1" data-path="unsup.html"><a href="unsup.html#a-bit-of-algebra"><i class="fa fa-check"></i><b>16.2.1</b> A bit of algebra</a></li>
<li class="chapter" data-level="16.2.2" data-path="unsup.html"><a href="unsup.html#pca"><i class="fa fa-check"></i><b>16.2.2</b> PCA</a></li>
<li class="chapter" data-level="16.2.3" data-path="unsup.html"><a href="unsup.html#ae"><i class="fa fa-check"></i><b>16.2.3</b> Autoencoders</a></li>
<li class="chapter" data-level="16.2.4" data-path="unsup.html"><a href="unsup.html#application"><i class="fa fa-check"></i><b>16.2.4</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="unsup.html"><a href="unsup.html#clustering-via-k-means"><i class="fa fa-check"></i><b>16.3</b> Clustering via k-means</a></li>
<li class="chapter" data-level="16.4" data-path="unsup.html"><a href="unsup.html#nearest-neighbors"><i class="fa fa-check"></i><b>16.4</b> Nearest neighbors</a></li>
<li class="chapter" data-level="16.5" data-path="unsup.html"><a href="unsup.html#coding-exercise-2"><i class="fa fa-check"></i><b>16.5</b> Coding exercise</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="RL.html"><a href="RL.html"><i class="fa fa-check"></i><b>17</b> Reinforcement learning</a><ul>
<li class="chapter" data-level="17.1" data-path="RL.html"><a href="RL.html#theoretical-layout"><i class="fa fa-check"></i><b>17.1</b> Theoretical layout</a><ul>
<li class="chapter" data-level="17.1.1" data-path="RL.html"><a href="RL.html#general-framework"><i class="fa fa-check"></i><b>17.1.1</b> General framework</a></li>
<li class="chapter" data-level="17.1.2" data-path="RL.html"><a href="RL.html#q-learning"><i class="fa fa-check"></i><b>17.1.2</b> Q-learning</a></li>
<li class="chapter" data-level="17.1.3" data-path="RL.html"><a href="RL.html#sarsa"><i class="fa fa-check"></i><b>17.1.3</b> SARSA</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="RL.html"><a href="RL.html#the-curse-of-dimensionality"><i class="fa fa-check"></i><b>17.2</b> The curse of dimensionality</a></li>
<li class="chapter" data-level="17.3" data-path="RL.html"><a href="RL.html#policy-gradient"><i class="fa fa-check"></i><b>17.3</b> Policy gradient</a><ul>
<li class="chapter" data-level="17.3.1" data-path="RL.html"><a href="RL.html#principle-2"><i class="fa fa-check"></i><b>17.3.1</b> Principle</a></li>
<li class="chapter" data-level="17.3.2" data-path="RL.html"><a href="RL.html#extensions-2"><i class="fa fa-check"></i><b>17.3.2</b> Extensions</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="RL.html"><a href="RL.html#simple-examples"><i class="fa fa-check"></i><b>17.4</b> Simple examples</a><ul>
<li class="chapter" data-level="17.4.1" data-path="RL.html"><a href="RL.html#q-learning-with-simulations"><i class="fa fa-check"></i><b>17.4.1</b> Q-learning with simulations</a></li>
<li class="chapter" data-level="17.4.2" data-path="RL.html"><a href="RL.html#RLemp2"><i class="fa fa-check"></i><b>17.4.2</b> Q-learning with market data</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="RL.html"><a href="RL.html#concluding-remarks"><i class="fa fa-check"></i><b>17.5</b> Concluding remarks</a></li>
<li class="chapter" data-level="17.6" data-path="RL.html"><a href="RL.html#exercises"><i class="fa fa-check"></i><b>17.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Appendix</b></span></li>
<li class="chapter" data-level="18" data-path="data-description.html"><a href="data-description.html"><i class="fa fa-check"></i><b>18</b> Data Description</a></li>
<li class="chapter" data-level="19" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html"><i class="fa fa-check"></i><b>19</b> Solution to exercises</a><ul>
<li class="chapter" data-level="19.1" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-4"><i class="fa fa-check"></i><b>19.1</b> Chapter 4</a></li>
<li class="chapter" data-level="19.2" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-5"><i class="fa fa-check"></i><b>19.2</b> Chapter 5</a></li>
<li class="chapter" data-level="19.3" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-6"><i class="fa fa-check"></i><b>19.3</b> Chapter 6</a></li>
<li class="chapter" data-level="19.4" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-7"><i class="fa fa-check"></i><b>19.4</b> Chapter 7</a></li>
<li class="chapter" data-level="19.5" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-8-the-autoencoder-model"><i class="fa fa-check"></i><b>19.5</b> Chapter 8: the autoencoder model</a></li>
<li class="chapter" data-level="19.6" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-9"><i class="fa fa-check"></i><b>19.6</b> Chapter 9</a></li>
<li class="chapter" data-level="19.7" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-12-ensemble-neural-network"><i class="fa fa-check"></i><b>19.7</b> Chapter 12: ensemble neural network</a></li>
<li class="chapter" data-level="19.8" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-13"><i class="fa fa-check"></i><b>19.8</b> Chapter 13</a><ul>
<li class="chapter" data-level="19.8.1" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#ew-portfolios-with-the-tidyverse"><i class="fa fa-check"></i><b>19.8.1</b> EW portfolios with the tidyverse</a></li>
<li class="chapter" data-level="19.8.2" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#advanced-weighting-function"><i class="fa fa-check"></i><b>19.8.2</b> Advanced weighting function</a></li>
<li class="chapter" data-level="19.8.3" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#functional-programming-in-the-backtest"><i class="fa fa-check"></i><b>19.8.3</b> Functional programming in the backtest</a></li>
</ul></li>
<li class="chapter" data-level="19.9" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-16"><i class="fa fa-check"></i><b>19.9</b> Chapter 16</a></li>
<li class="chapter" data-level="19.10" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-17"><i class="fa fa-check"></i><b>19.10</b> Chapter 17</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning for Factor Investing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lasso" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Penalized regressions and sparse hedging for minimum variance portfolios</h1>
<p>In this chapter, we introduce the widespread concept of regularisation for linear models. There are in fact several possible applications for these models. The first one is straightforward: resort to penalizations to improve the robustness of factor-based predictive regressions. The outcome can then be used to fuel an allocation scheme. For instance, <span class="citation">Han et al. (<a href="#ref-han2018firm">2019</a>)</span> and <span class="citation">Rapach and Zhou (<a href="#ref-rapach2019time">2019</a>)</span> use penalized regressions to improve stock return prediction when combining forecasts that emanate from individual characteristics.</p>
<p>Similar ideas can be developed for macroeconomic predictions for instance, as in <span class="citation">Uematsu and Tanaka (<a href="#ref-uematsu2019high">2019</a>)</span>.
The second application stems from a less known result which originates from <span class="citation">Stevens (<a href="#ref-stevens1998inverse">1998</a>)</span>. It links the weights of optimal mean-variance portfolios to particular cross-sectional regressions. The idea is then different and the purpose is to improve the quality of mean-variance driven portfolio weights. We present the two approach below after an introduction on regularization techniques for linear models.</p>
<p>Other examples of financial applications of penalization can be found in <span class="citation">dâ€™Aspremont (<a href="#ref-d2011identifying">2011</a>)</span>, <span class="citation">Ban, El Karoui, and Lim (<a href="#ref-ban2016machine">2016</a>)</span> and <span class="citation">Kremer et al. (<a href="#ref-kremer2019sparse">2019</a>)</span>. In any case, the idea is the same as in the seminal paper <span class="citation">Tibshirani (<a href="#ref-tibshirani1996regression">1996</a>)</span>: standard (unconstrained) optimization programs may lead to noisy estimates, thus adding a structuring constraint helps remove some noise (at the cost of a possible bias). For instance, <span class="citation">Kremer et al. (<a href="#ref-kremer2019sparse">2019</a>)</span> use this concept to build more robust mean-variance (<span class="citation">Markowitz (<a href="#ref-markowitz1952portfolio">1952</a>)</span>) portfolios and <span class="citation">Freyberger, Neuhierl, and Weber (<a href="#ref-freyberger2020dissecting">2020</a>)</span> use it to single out the characteristics that <em>really</em> help explain the cross-section of equity returns.</p>
<div id="penalised-regressions" class="section level2">
<h2><span class="header-section-number">6.1</span> Penalised regressions</h2>
<div id="penreg" class="section level3">
<h3><span class="header-section-number">6.1.1</span> Simple regressions</h3>
<p>The ideas behind linear models are at least two centuries old (<span class="citation">Legendre (<a href="#ref-legendre1805nouvelles">1805</a>)</span> is an early reference on least squares optimization). Given a matrix of predictors <span class="math inline">\(\textbf{X}\)</span>, we seek to decompose the output vector <span class="math inline">\(\textbf{y}\)</span> as a linear function of the columns of <span class="math inline">\(\textbf{X}\)</span> (written <span class="math inline">\(\textbf{X}\boldsymbol{\beta}\)</span>) plus an error term <span class="math inline">\(\boldsymbol{\epsilon}\)</span>: <span class="math inline">\(\textbf{y}=\textbf{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}\)</span>.</p>
<p>The best choice of <span class="math inline">\(\boldsymbol{\beta}\)</span> is naturally the one that minimizes the error. For analytical tractability, it is the sum of squared errors that is minimized: <span class="math inline">\(L=\boldsymbol{\epsilon}&#39;\boldsymbol{\epsilon}=\sum_{i=1}^I\epsilon_i^2\)</span>. The loss <span class="math inline">\(L\)</span> is called the sum of squared residuals (SSR). In order to find the optimal <span class="math inline">\(\boldsymbol{\beta}\)</span>, it is imperative to differentiate this loss <span class="math inline">\(L\)</span> with respect to <span class="math inline">\(\boldsymbol{\beta}\)</span> because the first order condition requires that the gradient be equal to zero:
<span class="math display">\[\begin{align*}
\nabla_{\boldsymbol{\beta}} L&amp;=\frac{\partial}{\partial \boldsymbol{\beta}}(\textbf{y}-\textbf{X}\boldsymbol{\beta})&#39;(\textbf{y}-\textbf{X}\boldsymbol{\beta})=\frac{\partial}{\partial \boldsymbol{\beta}}\boldsymbol{\beta}&#39;\textbf{X}&#39;\textbf{X}\boldsymbol{\beta}-2\textbf{y}&#39;\textbf{X}\boldsymbol{\beta} \\
&amp;=2\textbf{X}&#39;\textbf{X}\boldsymbol{\beta}  -2\textbf{X}&#39;\textbf{y}
\end{align*}\]</span>
so that the first order condition <span class="math inline">\(\nabla_{\boldsymbol{\beta}}=\textbf{0}\)</span> is satisfied if
<span class="math display" id="eq:regbeta">\[\begin{equation}
\tag{6.1}
\boldsymbol{\beta}^*=(\textbf{X}&#39;\textbf{X})^{-1}\textbf{X}&#39;\textbf{y},
\end{equation}\]</span>
which is known as the standard <strong>ordinary least squares</strong> (OLS) solution of the linear model. If the matrix <span class="math inline">\(\textbf{X}\)</span> has dimensions <span class="math inline">\(I \times K\)</span>, then the <span class="math inline">\(\textbf{X}&#39;\textbf{X}\)</span> can only be inverted if the number of rows <span class="math inline">\(I\)</span> is strictly superior to the number of columns <span class="math inline">\(K\)</span>. In some cases, that may not hold: there are more predictors than instances and there is no unique value of <span class="math inline">\(\boldsymbol{\beta}\)</span> that minimizes the loss. If <span class="math inline">\(\textbf{X}&#39;\textbf{X}\)</span> is nonsingular (or positive definite), then the second order condition ensures that <span class="math inline">\(\boldsymbol{\beta}^*\)</span> yields a global minimum for the loss <span class="math inline">\(L\)</span> (the second order derivative of <span class="math inline">\(L\)</span> with respect to <span class="math inline">\(\boldsymbol{\beta}\)</span>, the Hessian matrix, is exactly <span class="math inline">\(\textbf{X}&#39;\textbf{X}\)</span>).</p>
<p>Up to now, we have made no distributional assumption on any of the above quantities. Standard assumptions are the following:<br />
- <span class="math inline">\(\mathbb{E}[\textbf{y}|\textbf{X}]=\textbf{X}\boldsymbol{\beta}\)</span>: <strong>linear shape for the regression function</strong>;<br />
- <span class="math inline">\(\mathbb{E}[\boldsymbol{\epsilon}|\textbf{X}]=\textbf{0}\)</span>: errors are <strong>independent of predictors</strong>;<br />
- <span class="math inline">\(\mathbb{E}[\boldsymbol{\epsilon}\boldsymbol{\epsilon}&#39;| \textbf{X}]=\sigma^2\textbf{I}\)</span>: <strong>homoscedasticity</strong> - errors are uncorrelated and have identical variance;<br />
- the <span class="math inline">\(\epsilon_i\)</span> are normally distributed.</p>
<p>Under these hypotheses, it is possible to perform statistical tests related to the <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> coefficients. We refer to Chapters 2 to 4 in <span class="citation">Greene (<a href="#ref-greene2018econometric">2018</a>)</span> for a thorough treatment on linear models as well as to Chapter 5 of the same book for details on the corresponding tests.</p>
</div>
<div id="forms-of-penalizations" class="section level3">
<h3><span class="header-section-number">6.1.2</span> Forms of penalizations</h3>
<p>Penalised regressions have been popularised since the seminal work of <span class="citation">Tibshirani (<a href="#ref-tibshirani1996regression">1996</a>)</span>. The idea is to impose a constraint on the coefficients of the regression, namely that their total magnitude be restrained. In his original paper, <span class="citation">Tibshirani (<a href="#ref-tibshirani1996regression">1996</a>)</span> proposes to estimate the following model (LASSO):
<span class="math display" id="eq:lasso1">\[\begin{equation}
\tag{6.2}
y_i = \sum_{j=1}^J \beta_jx_{i,j} + \epsilon_i, \quad i =1,\dots,I, \quad \text{s.t.} \quad \sum_{j=1}^J |\beta_j| &lt; \delta, 
\end{equation}\]</span>
for some strictly positive constant <span class="math inline">\(\delta\)</span>. Under least square minimisation, this amounts to solve the Lagrangian formulation:
<span class="math display" id="eq:lasso2">\[\begin{equation}
\tag{6.3}
\underset{\mathbf{\beta}}{\min} \, \left\{ \sum_{i=1}^I\left(y_i - \sum_{j=1}^J \beta_jx_{i,j} \right)^2+\lambda \sum_{j=1}^J |\beta_j| \right\},
\end{equation}\]</span>
for some value <span class="math inline">\(\lambda&gt;0\)</span> which naturally depends on <span class="math inline">\(\delta\)</span> (the lower the <span class="math inline">\(\delta\)</span>, the higher the <span class="math inline">\(\lambda\)</span>: the constraint is more binding). This specification seems close to the ridge regression (<span class="math inline">\(L^2\)</span> regularisation), which is in fact anterior to the Lasso:
<span class="math display" id="eq:ridge">\[\begin{equation}
\tag{6.4}
\underset{\mathbf{\beta}}{\min} \, \left\{ \sum_{i=1}^I\left(y_i - \sum_{j=1}^J\beta_jx_{i,j} \right)^2+\lambda \sum_{j=1}^J \beta_j^2 \right\},
\end{equation}\]</span>
and which is equivalent to estimating the following model
<span class="math display" id="eq:ridge6">\[\begin{equation}
\tag{6.5}
y_i = \sum_{j=1}^J \beta_jx_{i,j} + \epsilon_i, \quad i =1,\dots,I, \quad \text{s.t.} \quad \sum_{j=1}^J \beta_j^2 &lt; \delta, 
\end{equation}\]</span>
but the outcome is in fact quite different, which justifies a separate treatment. Mechanically, as <span class="math inline">\(\lambda\)</span> increases (or as <span class="math inline">\(\delta\)</span> in <a href="lasso.html#eq:ridge6">(6.5)</a> <em>decreases</em>), the coefficients of the ridge regression all slowly decrease in magnitude towards zero. In the case of the LASSO, the convergence is somewhat more brutal as some coefficients shrink to zero very quickly. For <span class="math inline">\(\lambda\)</span> sufficiently large, only one coefficient will remain nonzero, while in the ridge regression, the zero value is only reached asymptotically for all coefficients.</p>
<p>To depict the difference between the Lasso and the ridge regression, let us consider the case of <span class="math inline">\(K=2\)</span> predictors which is shown in Figure <a href="lasso.html#fig:lassoridge">6.1</a>. The optimal unconstrained solution <span class="math inline">\(\boldsymbol{\beta}^*\)</span> is pictured in red in the middle of the space. The problem is naturally that it does not satisfy the imposed conditions. These constraints are shown in light grey: they take the shape of a square <span class="math inline">\(|\beta_1|+|\beta_2| \le \delta\)</span> in the case of the Lasso and a circle <span class="math inline">\(\beta_1^2+\beta_2^2 \le \delta\)</span> for the ridge regression. In order to satisfy these constraints, the optimization need to look in the vicinity of <span class="math inline">\(\boldsymbol{\beta}^*\)</span> by allowing for larger error levels. These error levels are shown as orange ellipsoids in the figure. When the requirement on the error is loose enough, one ellipsoid touches the acceptable boundary (in grey) and this is where the constrained solution is located.</p>
<div class="figure" style="text-align: center"><span id="fig:lassoridge"></span>
<img src="images/lassoridge.png" alt="Schematic view of Lasso (left) versus ridge (right) regressions." width="800px" />
<p class="caption">
FIGURE 6.1: Schematic view of Lasso (left) versus ridge (right) regressions.
</p>
</div>
<p>Both methods work when the number of exogenous variables surpasses that of observations, i.e., in the case where classical regressions are ill-defined. This is easy to see in the case of the ridge regression for which the OLS solution is simply
<span class="math display">\[\hat{\boldsymbol{\beta}}=(\mathbf{X}&#39;\mathbf{X}+\lambda \mathbf{I}_N)^{-1}\mathbf{X}&#39;\mathbf{Y}.\]</span>
The additional term <span class="math inline">\(\lambda \mathbf{I}_N\)</span> compared to Equation <a href="lasso.html#eq:regbeta">(6.1)</a> ensures that the inverse matrix is well-defined whenever <span class="math inline">\(\lambda&gt;0\)</span>. As <span class="math inline">\(\lambda\)</span> increases, the magnitudes of the <span class="math inline">\(\hat{\beta}_i\)</span> decrease, which explains why penalizations are sometimes referred to as <strong>shrinkage</strong> methods (the estimated coefficients see their values shrink).</p>
<p><span class="citation">Zou and Hastie (<a href="#ref-zou2005regularization">2005</a>)</span> propose to benefit from the best of both worlds when combining both penalisations in a convex manner (which they call the <strong>elasticnet</strong>):
<span class="math display" id="eq:elasticnet">\[\begin{equation}
\tag{6.6}
y_i = \sum_{j=1}^J \beta_jx_{i,j} + \epsilon_i, \quad \text{s.t.} \quad \alpha \sum_{j=1}^J |\beta_j| +(1-\alpha)\sum_{j=1}^J \beta_j^2&lt; \delta, \quad i =1,\dots,N,
\end{equation}\]</span>
which is the solved as
<span class="math display" id="eq:elastic">\[\begin{equation}
\tag{6.7}
\underset{\mathbf{\beta}}{\min} \, \left\{ \sum_{i=1}^I\left(y_i - \sum_{j=1}^J\beta_jx_{i,j} \right)^2+\lambda \left(\alpha\sum_{j=1}^J |\beta_j|+ (1-\alpha)\sum_{j=1}^J \beta_j^2\right) \right\},
\end{equation}\]</span></p>
<p>The main advantage of the LASSO compared to the ridge regression is its selection capability. Indeed, given a very large number of variables (or predictors), the LASSO will progressively rule-out those that are the least relevant. The elasticnet preserves this selection ability and <span class="citation">Zou and Hastie (<a href="#ref-zou2005regularization">2005</a>)</span> argue that in some cases, it is even more effective than the LASSO. The parameter <span class="math inline">\(\alpha \in [0,1]\)</span> tunes the smoothness of convergence (of the coefficients) towards zero. The closer <span class="math inline">\(\alpha\)</span> is to zero the smoother the convergence.</p>
</div>
<div id="illustrations" class="section level3">
<h3><span class="header-section-number">6.1.3</span> Illustrations</h3>
<p>We begin with simple illustrations of penalized regressions. We start with the LASSO. The original implementation by the authors is in R, which is practical. The syntax is slightly different, compared to usual linear models. The illustrations are run on the whole dataset. First, we estimate the coefficients. By default, the function chooses a large array of penalization values so that the results for different penalization intensities (<span class="math inline">\(\lambda\)</span>) can be shown immediately.</p>

<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb31-1" data-line-number="1"><span class="kw">library</span>(glmnet)</a>
<a class="sourceLine" id="cb31-2" data-line-number="2">y_penalized &lt;-<span class="st"> </span>data_ml<span class="op">$</span>R1M_Usd                              <span class="co"># Dependent variable</span></a>
<a class="sourceLine" id="cb31-3" data-line-number="3">x_penalized &lt;-<span class="st"> </span>data_ml <span class="op">%&gt;%</span><span class="st">                                  </span><span class="co"># Predictors</span></a>
<a class="sourceLine" id="cb31-4" data-line-number="4"><span class="st">    </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="kw">all_of</span>(features)) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.matrix</span>() </a>
<a class="sourceLine" id="cb31-5" data-line-number="5">fit_lasso &lt;-<span class="st"> </span><span class="kw">glmnet</span>(x_penalized, y_penalized, <span class="dt">alpha =</span> <span class="dv">1</span>)    <span class="co"># Model alpha = 1: LASSO</span></a></code></pre></div>
<p></p>
<p>Once the coefficients are computed, they require some wrangling before plotting. Also, there are too many of them, so we only plot a subset of them.</p>

<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb32-1" data-line-number="1">lasso_res &lt;-<span class="st"> </span><span class="kw">summary</span>(fit_lasso<span class="op">$</span>beta)                        <span class="co"># Extract LASSO coefs</span></a>
<a class="sourceLine" id="cb32-2" data-line-number="2">lambda &lt;-<span class="st"> </span>fit_lasso<span class="op">$</span>lambda                                  <span class="co"># Values of the penalisation const</span></a>
<a class="sourceLine" id="cb32-3" data-line-number="3">lasso_res<span class="op">$</span>Lambda &lt;-<span class="st"> </span>lambda[lasso_res<span class="op">$</span>j]                     <span class="co"># Put the labels where they belong</span></a>
<a class="sourceLine" id="cb32-4" data-line-number="4">lasso_res<span class="op">$</span>Feature &lt;-<span class="st"> </span>features[lasso_res<span class="op">$</span>i] <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.factor</span>()  <span class="co"># Add names of variables to output</span></a>
<a class="sourceLine" id="cb32-5" data-line-number="5">lasso_res[<span class="dv">1</span><span class="op">:</span><span class="dv">120</span>,] <span class="op">%&gt;%</span><span class="st">                                       </span><span class="co"># Take the first 120 estimates</span></a>
<a class="sourceLine" id="cb32-6" data-line-number="6"><span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Lambda, <span class="dt">y =</span> x, <span class="dt">color =</span> Feature)) <span class="op">+</span><span class="st">       </span><span class="co"># Plot!</span></a>
<a class="sourceLine" id="cb32-7" data-line-number="7"><span class="st">    </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span><span class="kw">coord_fixed</span>(<span class="fl">0.25</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;beta&quot;</span>) <span class="op">+</span><span class="st">        </span><span class="co"># Change aspect ratio of graph</span></a>
<a class="sourceLine" id="cb32-8" data-line-number="8"><span class="st">    </span><span class="kw">theme</span>(<span class="dt">legend.text =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">7</span>))             <span class="co"># Reduce legend font size</span></a></code></pre></div>
<div class="figure"><span id="fig:lassoresults"></span>
<img src="ML_factor_files/figure-html/lassoresults-1.png" alt="LASSO model. The dependent variable is the 1 month ahead return." width="400px" />
<p class="caption">
FIGURE 6.2: LASSO model. The dependent variable is the 1 month ahead return.
</p>
</div>
<p></p>
<p>The graph plots the evolution of coefficients as the penalization intensity, <span class="math inline">\(\lambda\)</span>, increases. For some characteristics, like Ebit_Ta (in orange), the convergence to zero is rapid. Other variables resist the penalization longer, like Mkt_Cap_3M_Usd, which is the last one to vanish. Essentially, this means that at the first order, this variable is an important driver of future 1 month returns - in our sample. Moreover, the negative sign of its coefficient are a confirmation (again, in this sample) of the size anomaly, according to which small firms experience higher future returns compared to their larger counterparts.</p>
<p>Next, we turn to ridge regressions.</p>

<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb33-1" data-line-number="1">fit_ridge &lt;-<span class="st"> </span><span class="kw">glmnet</span>(x_penalized, y_penalized, <span class="dt">alpha =</span> <span class="dv">0</span>)                  <span class="co"># alpha = 0: ridge</span></a>
<a class="sourceLine" id="cb33-2" data-line-number="2">ridge_res &lt;-<span class="st"> </span><span class="kw">summary</span>(fit_ridge<span class="op">$</span>beta)                                      <span class="co"># Extract ridge coefs</span></a>
<a class="sourceLine" id="cb33-3" data-line-number="3">lambda &lt;-<span class="st"> </span>fit_ridge<span class="op">$</span>lambda                                                <span class="co"># Penalisation const</span></a>
<a class="sourceLine" id="cb33-4" data-line-number="4">ridge_res<span class="op">$</span>Feature &lt;-<span class="st"> </span>features[ridge_res<span class="op">$</span>i] <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.factor</span>()</a>
<a class="sourceLine" id="cb33-5" data-line-number="5">ridge_res<span class="op">$</span>Lambda &lt;-<span class="st"> </span>lambda[ridge_res<span class="op">$</span>j]                                   <span class="co"># Set labels right</span></a>
<a class="sourceLine" id="cb33-6" data-line-number="6">ridge_res <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb33-7" data-line-number="7"><span class="st">    </span><span class="kw">filter</span>(Feature <span class="op">%in%</span><span class="st"> </span><span class="kw">levels</span>(<span class="kw">droplevels</span>(lasso_res<span class="op">$</span>Feature[<span class="dv">1</span><span class="op">:</span><span class="dv">120</span>]))) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># Keep same features </span></a>
<a class="sourceLine" id="cb33-8" data-line-number="8"><span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Lambda, <span class="dt">y =</span> x, <span class="dt">color =</span> Feature)) <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;beta&quot;</span>) <span class="op">+</span><span class="st">      </span><span class="co"># Plot!</span></a>
<a class="sourceLine" id="cb33-9" data-line-number="9"><span class="st">    </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span><span class="kw">scale_x_log10</span>() <span class="op">+</span><span class="st"> </span><span class="kw">coord_fixed</span>(<span class="dv">45</span>) <span class="op">+</span><span class="st">                     </span><span class="co"># Aspect ratio </span></a>
<a class="sourceLine" id="cb33-10" data-line-number="10"><span class="st">    </span><span class="kw">theme</span>(<span class="dt">legend.text =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">7</span>))</a></code></pre></div>
<div class="figure"><span id="fig:sparseridge"></span>
<img src="ML_factor_files/figure-html/sparseridge-1.png" alt="Ridge regression. The dependent variable is the 1 month ahead return." width="576" />
<p class="caption">
FIGURE 6.3: Ridge regression. The dependent variable is the 1 month ahead return.
</p>
</div>
<p></p>
<p>In Figure <a href="lasso.html#fig:sparseridge">6.3</a>, the convergence to zero is much smoother. We underline that the x-axis (penalization intensities) have a log-scale. This allows to see the early patterns (close to zero, to the left) more clearly. As in the previous figure, the Mkt_Cap_3M_Usd predictor clearly dominates, with again large negative coefficients. Nonetheless, as <span class="math inline">\(\lambda\)</span> increases, its domination over the other predictor fades.</p>
<p>By definition, the elasticnet will produce curves that behave like a blend of the two above approaches. Nonetheless, as long as <span class="math inline">\(\alpha &gt;0\)</span>, the selective property of the LASSO will be preserved: some features will see their coefficients shrink rapidly to zero. In fact, the strength of the LASSO is such that a balanced mix of the two penalization is not reached at <span class="math inline">\(\alpha = 1/2\)</span>, but rather at a much smaller value (possibly below 0.1).</p>
</div>
</div>
<div id="sparse-hedging-for-minimum-variance-portfolios" class="section level2">
<h2><span class="header-section-number">6.2</span> Sparse hedging for minimum variance portfolios</h2>
<div id="presentation-and-derivations" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Presentation and derivations</h3>
<p>The idea of constructing sparse portfolios is not new per se (see, e.g., <span class="citation">Brodie et al. (<a href="#ref-brodie2009sparse">2009</a>)</span>, <span class="citation">Fastrich, Paterlini, and Winker (<a href="#ref-fastrich2015constructing">2015</a>)</span>) and the link with the selective property of the LASSO is rather straightforward in classical quadratic programs. Note that the choice of the <span class="math inline">\(L^1\)</span> norm is imperative because when enforcing a simple <span class="math inline">\(L^2\)</span> norm, the diversification of the portfolio increases (see <span class="citation">Coqueret (<a href="#ref-coqueret2015diversified">2015</a>)</span>).</p>
<p>The idea behind this section stems from <span class="citation">Goto and Xu (<a href="#ref-goto2015improving">2015</a>)</span> but the cornerstone result was first published by <span class="citation">Stevens (<a href="#ref-stevens1998inverse">1998</a>)</span> and we present it below. We provide details because the derivations are not commonplace in the literature.</p>
<p>In usual mean-variance allocations, one core ingredient is the inverse covariance matrix of assets <span class="math inline">\(\mathbf{\Sigma}^{-1}\)</span>. For instance, the maximum Sharpe ratio (MSR) portfolio is given by</p>
<p><span class="math display" id="eq:MSR">\[\begin{equation}
\tag{6.8}
\mathbf{w}^{\text{MSR}}=\frac{\mathbf{\Sigma}^{-1}\boldsymbol{\mu}}{\mathbf{1}&#39;\mathbf{\Sigma}^{-1}\boldsymbol{\mu}},
\end{equation}\]</span>
where <span class="math inline">\(\mathbf{\mu}\)</span> is the vector of expected (excess) returns. Taking <span class="math inline">\(\mathbf{\mu}=\mathbf{1}\)</span> yields the minimum variance portfolio, which is agnostic in terms of the first moment of expected returns (and, as such, usually more robust than most alternatives which try to estimate <span class="math inline">\(\boldsymbol{\mu}\)</span> - and often fail).</p>
<p>Usually, the traditional way is to estimate <span class="math inline">\(\boldsymbol{\Sigma}\)</span> and to invert it to get the MSR weights. However, several approaches aim at estimating <span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span>  and we present one of them below. We proceed one asset at a time, that is, one line of <span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span> at a time.<br />
If we decompose the matrix <span class="math inline">\(\mathbf{\Sigma}\)</span> into:
<span class="math display">\[\mathbf{\Sigma}= \left[\begin{array}{cc} \sigma^2 &amp; \mathbf{c}&#39; \\
\mathbf{c}&amp; \mathbf{C}\end{array} \right],\]</span>
classical partitioning results (e.g., Schur complements) imply
<span class="math display">\[\small \mathbf{\Sigma}^{-1}= \left[\begin{array}{cc} (\sigma^2 -\mathbf{c}&#39;\mathbf{C}^{-1}\mathbf{c})^{-1} &amp; - (\sigma^2 -\mathbf{c}&#39;\mathbf{C}^{-1}\mathbf{c})^{-1}\mathbf{c}&#39;\mathbf{C}^{-1} \\
- (\sigma^2 -\mathbf{c}&#39;\mathbf{C}^{-1}\mathbf{c})^{-1}\mathbf{C}^{-1}\mathbf{c}&amp; \mathbf{C}^{-1}+ (\sigma^2 -\mathbf{c}&#39;\mathbf{C}^{-1}\mathbf{c})^{-1}\mathbf{C}^{-1}\mathbf{cc}&#39;\mathbf{C}^{-1}\end{array} \right].\]</span>
We are interested in the first line, which has 2 components: the factor <span class="math inline">\((\sigma^2 -\mathbf{c}&#39;\mathbf{C}^{-1}\mathbf{c})^{-1}\)</span> and the line vector <span class="math inline">\(\mathbf{c}&#39;\mathbf{C}^{-1}\)</span>. <span class="math inline">\(\mathbf{C}\)</span> is the covariance matrix of assets <span class="math inline">\(2\)</span> to <span class="math inline">\(N\)</span> and <span class="math inline">\(\mathbf{c}\)</span> is the covariance between the first asset and all other assets. The first line of <span class="math inline">\(\mathbf{\Sigma}^{-1}\)</span> is
<span class="math display" id="eq:sparse1">\[\begin{equation}
\tag{6.9}
(\sigma^2 -\mathbf{c}&#39;\mathbf{C}^{-1}\mathbf{c})^{-1} \left[1 \quad  \underbrace{-\mathbf{c}&#39;\mathbf{C}^{-1}}_{N-1 \text{ terms}} \right]. 
\end{equation}\]</span></p>
<p>We now consider an alternative setting. We regress the returns of the first asset on those of all other assets:
<span class="math display" id="eq:sparseeq">\[\begin{equation}
\tag{6.10}
r_{1,t}=a_1+\sum_{n=2}^N\beta_{1|n}r_{n,t}+\epsilon_t, \quad \text{ i.e., } \quad  \mathbf{r}_1=a_1\mathbf{1}_T+\mathbf{R}_{-1}\mathbf{\beta}_1+\epsilon_1,
\end{equation}\]</span>
where <span class="math inline">\(\mathbf{R}_{-1}\)</span> gathers the returns of all assets except the first one. The OLS estimator for <span class="math inline">\(\mathbf{\beta}_1\)</span> is
<span class="math display" id="eq:sparse2">\[\begin{equation}
\tag{6.11}
\hat{\mathbf{\beta}}_{1}=\mathbf{C}^{-1}\mathbf{c},
\end{equation}\]</span></p>
<p>and this is the partitioned form (when a constant is included to the regression) stemming from the Frisch-Waugh-Lovell theorem (see Chapter 3 in <span class="citation">Greene (<a href="#ref-greene2018econometric">2018</a>)</span>).</p>
<p>In addition,
<span class="math display" id="eq:sparse3">\[\begin{equation}
\tag{6.12}
(1-R^2)\sigma_{\mathbf{r}_1}^2=\sigma_{\mathbf{r}_1}^2- \mathbf{c}&#39;\mathbf{C}^{-1}\mathbf{c} =\sigma^2_{\epsilon_1}.
\end{equation}\]</span>
The proof of this last fact is given below.</p>
<p>With <span class="math inline">\(\mathbf{X}\)</span> being the concatenation of <span class="math inline">\(\mathbf{1}_T\)</span> with returns <span class="math inline">\(\mathbf{R}_{-1}\)</span> and with <span class="math inline">\(\mathbf{y}=\mathbf{r}_1\)</span>, the classical expression of the <span class="math inline">\(R^2\)</span> is <span class="math display">\[R^2=1-\frac{\mathbf{\epsilon}&#39;\mathbf{\epsilon}}{T\sigma_Y^2}=1-\frac{\mathbf{y}&#39;\mathbf{y}-\hat{\mathbf{\beta}&#39;}\mathbf{X}&#39;\mathbf{X}\hat{\mathbf{\beta}}}{T\sigma_Y^2}=1-\frac{\mathbf{y}&#39;\mathbf{y}-\mathbf{y}&#39;\mathbf{X}\hat{\mathbf{\beta}}}{T\sigma_Y^2},\]</span>
with fitted values <span class="math inline">\(\mathbf{X}\hat{\mathbf{\beta}}= \hat{a_1}\mathbf{1}_T+\mathbf{R}_{-1}\mathbf{C}^{-1}\mathbf{c}\)</span>. Hence,
<span class="math display">\[\begin{align*}
T\sigma_{\mathbf{r}_1}^2R^2&amp;=T\sigma_{\mathbf{r}_1}^2-\mathbf{r}&#39;_1\mathbf{r}_1+\hat{a_1}\mathbf{1}&#39;_T\mathbf{r}_1+\mathbf{r}&#39;_1\mathbf{R}_{-1}\mathbf{C}^{-1}\mathbf{c} \\
T(1-R^2)\sigma_{\mathbf{r}_1}^2&amp;=\mathbf{r}&#39;_1\mathbf{r}_1-\hat{a_1}\mathbf{1}&#39;_T\mathbf{r}_1-\left(\mathbf{\tilde{r}}_1+\frac{\mathbf{1}_T\mathbf{1}&#39;_T}{T}\mathbf{r}_1\right)&#39;\left(\tilde{\mathbf{R}}_{-1}+\frac{\mathbf{1}_T\mathbf{1}&#39;_T}{T}\mathbf{R}_{-1}\right)\mathbf{C}^{-1}\mathbf{c} \\
T(1-R^2)\sigma_{\mathbf{r}_1}^2&amp;=\mathbf{r}&#39;_1\mathbf{r}_1-\hat{a_1}\mathbf{1}&#39;_T\mathbf{r}_1-T\mathbf{c}&#39;\mathbf{C}^{-1}\mathbf{c} -\mathbf{r}&#39;_1\frac{\mathbf{1}_T\mathbf{1}&#39;_T}{T}\mathbf{R}_{-1} \mathbf{C}^{-1}\mathbf{c} \\
T(1-R^2)\sigma_{\mathbf{r}_1}^2&amp;=\mathbf{r}&#39;_1\mathbf{r}_1-\frac{(\mathbf{1}&#39;_T\mathbf{r}_1)^2}{T}- T\mathbf{c}&#39;\mathbf{C}^{-1}\mathbf{c} \\
(1-R^2)\sigma_{\mathbf{r}_1}^2&amp;=\sigma_{\mathbf{r}_1}^2- \mathbf{c}&#39;\mathbf{C}^{-1}\mathbf{c} 
\end{align*}\]</span>
where in the fourth equality we have plugged <span class="math inline">\(\hat{a}_1=\frac{\mathbf{1&#39;}_T}{T}(\mathbf{r}_1-\mathbf{R}_{-1}\mathbf{C}^{-1}\mathbf{c})\)</span>. Note: there is probably a simpler proof - see e.g.Â Section 3.5 in <span class="citation">Greene (<a href="#ref-greene2018econometric">2018</a>)</span>.</p>
<p>Combining (), () and (), we get that the first line of <span class="math inline">\(\mathbf{\Sigma}^{-1}\)</span> is equal to
<span class="math display" id="eq:sparsehedgeeq2">\[\begin{equation}
\tag{6.13}
\frac{1}{\sigma^2_{\epsilon_1}}\times \left[ 1 \quad  -\hat{\boldsymbol{\beta}}_1&#39;\right].
\end{equation}\]</span></p>
<p>Given the first line of <span class="math inline">\(\mathbf{\Sigma}^{-1}\)</span>, it suffices to multiply by <span class="math inline">\(\boldsymbol{\mu}\)</span> to get the portfolio weight in the first asset (up to a scaling constant).</p>
<p>There is a nice economic intuition behind the above results which justifies the term â€œsparse hedgingâ€. We take the case of the minimum variance portfolio, for which <span class="math inline">\(\boldsymbol{\mu}=\boldsymbol{1}\)</span>. In Equation <a href="lasso.html#eq:sparseeq">(6.10)</a>, we try to explain the return of asset 1 with that of all other assets. In the above equation, up to a scaling constant, the portfolio has a unit position in the first asset and <span class="math inline">\(-\hat{\boldsymbol{\beta}}_1\)</span> positions in all other assets. Hence, the purpose of all other assets is clearly to hedge the return of the first one. In fact, these positions are aimed at minimizing the squared errors of the aggregate portfolio for asset one (these errors a exactly <span class="math inline">\(\mathbf{\epsilon}_1\)</span>). Moreover, the scaling factor <span class="math inline">\(\sigma^{-2}_{\epsilon_1}\)</span> is also simple to interpret: the more we trust the regression output (because of a small <span class="math inline">\(\sigma^{2}_{\epsilon_1}\)</span>), the more we invest in the hedging portfolio of the asset.</p>
<p>This reasoning is easily generalized for any line of <span class="math inline">\(\mathbf{\Sigma}^{-1}\)</span>, which can be obtained by regressing the returns of asset <span class="math inline">\(i\)</span> on the returns of all other assets. If the allocation scheme has the form () for given values of <span class="math inline">\(\boldsymbol{\mu}\)</span>, then the pseudo-code for the sparse portfolio strategy is the following.</p>
At each date (which we omit for notational convenience),

<p>where we recall that the vectors <span class="math inline">\(\mathbf{\beta}_{i|}=[\mathbf{\beta}_{i|1},\dots,\mathbf{\beta}_{i|i-1},\mathbf{\beta}_{i|i+1},\dots,\mathbf{\beta}_{i|N}]\)</span> are the coefficients from regressing the returns of asset <span class="math inline">\(i\)</span> against the returns of all other assets.<br />
The introduction of the penalization norms is the new ingredient, compared to the original approach of <span class="citation">Stevens (<a href="#ref-stevens1998inverse">1998</a>)</span>. The benefits are twofold: first, introducing constraints yields weights that are more robust and less subject to errors in the estimates of <span class="math inline">\(\mathbf{\mu}\)</span>; second, because of sparsity, weights are more stable, less leveraged and thus the strategy is less impacted by transaction costs. Before we turn to numerical applications, we mention a more direct route to the estimation of a robust inverse covariance matrix: the Graphical LASSO. The GLASSO estimates the precision matrix (inverse covariance matrix) via maximum likelihood while imposing constraints/penalizations on the weights of the matrix. When the penalization is strong enough, this yields a sparse matrix, i.e., a matrix in which some and possibly many coefficients are zero. We refer to the original article <span class="citation">Friedman, Hastie, and Tibshirani (<a href="#ref-friedman2008sparse">2008</a>)</span> for more details on this subject.</p>
</div>
<div id="sparseex" class="section level3">
<h3><span class="header-section-number">6.2.2</span> Example</h3>
<p>The interest of sparse hedging portfolios is to propose a robust approach to the estimation of minimum variance policies. Indeed, since the vector of expected returns <span class="math inline">\(\boldsymbol{\mu}\)</span> is usually very noisy, a simple solution is to adopt an agnostic view by setting <span class="math inline">\(\boldsymbol{\mu}=\boldsymbol{1}\)</span>. In order to test the added value of the sparsity constraint, we must resort to a full backtest. In doing so, we anticipate the content of chapter <a href="backtest.html#backtest">13</a>.</p>
<p>We first prepare the variables. Sparse portfolios are based on returns only; we thus base our analysis on the dedicated variable in matrix/rectangular format (<em>returns</em>) which were created at the end of Chapter <a href="notdata.html#notdata">2</a>.</p>
<p>Then, we initialize the output variables: portfolio weights and portfolio returns. We want to compare three strategies: an equally weighted (EW) benchmark of all stocks, the classical global minimum variance portfolio (GMV) and the sparse-hedging approach to minimum variance.</p>

<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb34-1" data-line-number="1">t_oos &lt;-<span class="st"> </span>returns<span class="op">$</span>date[returns<span class="op">$</span>date <span class="op">&gt;</span><span class="st"> </span>separation_date] <span class="op">%&gt;%</span><span class="st">            </span><span class="co"># Out-of-sample dates </span></a>
<a class="sourceLine" id="cb34-2" data-line-number="2"><span class="st">    </span><span class="kw">unique</span>() <span class="op">%&gt;%</span><span class="st">                                                     </span><span class="co"># Remove duplicates</span></a>
<a class="sourceLine" id="cb34-3" data-line-number="3"><span class="st">    </span><span class="kw">as.Date</span>(<span class="dt">origin =</span> <span class="st">&quot;1970-01-01&quot;</span>)                                   <span class="co"># Transform in date format</span></a>
<a class="sourceLine" id="cb34-4" data-line-number="4">Tt &lt;-<span class="st"> </span><span class="kw">length</span>(t_oos)                                                  <span class="co"># Nb of dates, avoid T </span></a>
<a class="sourceLine" id="cb34-5" data-line-number="5">nb_port &lt;-<span class="st"> </span><span class="dv">3</span>                                                         <span class="co"># Nb of portfolios/strats.</span></a>
<a class="sourceLine" id="cb34-6" data-line-number="6">portf_weights &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="dt">dim =</span> <span class="kw">c</span>(Tt, nb_port, <span class="kw">ncol</span>(returns) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>))   <span class="co"># Initial portf. weights</span></a>
<a class="sourceLine" id="cb34-7" data-line-number="7">portf_returns &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow =</span> Tt, <span class="dt">ncol =</span> nb_port)                <span class="co"># Initial portf. returns </span></a></code></pre></div>
<p></p>
<p>Next, because it is the purpose of this section, we isolate the computation of the weights of sparse-hedging portfolios. In the case of minimum variance portfolios, when <span class="math inline">\(\boldsymbol{\mu}=\boldsymbol{1}\)</span>, the weight in asset 1 will simply be the sum of all terms in Equation <a href="lasso.html#eq:sparsehedgeeq2">(6.13)</a> and the other weights have similar forms.</p>

<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb35-1" data-line-number="1">weights_sparsehedge &lt;-<span class="st"> </span><span class="cf">function</span>(returns, alpha, lambda){  <span class="co"># The parameters are defined here</span></a>
<a class="sourceLine" id="cb35-2" data-line-number="2">    w &lt;-<span class="st"> </span><span class="dv">0</span>                                                <span class="co"># Initiate weights</span></a>
<a class="sourceLine" id="cb35-3" data-line-number="3">    <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(returns)){                            <span class="co"># Loop on the assets</span></a>
<a class="sourceLine" id="cb35-4" data-line-number="4">        y &lt;-<span class="st"> </span>returns[,i]                                  <span class="co"># Dependent variable</span></a>
<a class="sourceLine" id="cb35-5" data-line-number="5">        x &lt;-<span class="st"> </span>returns[,<span class="op">-</span>i]                                 <span class="co"># Independent variable</span></a>
<a class="sourceLine" id="cb35-6" data-line-number="6">        fit &lt;-<span class="st"> </span><span class="kw">glmnet</span>(x,y, <span class="dt">family =</span> <span class="st">&quot;gaussian&quot;</span>, <span class="dt">alpha =</span> alpha, <span class="dt">lambda =</span> lambda)</a>
<a class="sourceLine" id="cb35-7" data-line-number="7">        err &lt;-<span class="st"> </span>y<span class="op">-</span><span class="kw">predict</span>(fit, x)                          <span class="co"># Prediction errors</span></a>
<a class="sourceLine" id="cb35-8" data-line-number="8">        w[i] &lt;-<span class="st"> </span>(<span class="dv">1</span><span class="op">-</span><span class="kw">sum</span>(fit<span class="op">$</span>beta))<span class="op">/</span><span class="kw">var</span>(err)                <span class="co"># Output: weight of asset i</span></a>
<a class="sourceLine" id="cb35-9" data-line-number="9">    }</a>
<a class="sourceLine" id="cb35-10" data-line-number="10">    <span class="kw">return</span>(w <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(w))                                    <span class="co"># Normalisation of weights</span></a>
<a class="sourceLine" id="cb35-11" data-line-number="11">}</a></code></pre></div>
<p></p>
<p>In order to benchmark our strategy, we define a meta-weighting function that embeds three strategies: the EW benchmarks (1), the classical GMV (2) and the sparse-hedging minimum variance (3). For the GMV, since there are much more assets than dates, the covariance matrix is singular. Thus, we have a small heuristic shrinkage term. For a more rigorous treatment of this technique, ce refer to the original article <span class="citation">Ledoit and Wolf (<a href="#ref-ledoit2004well">2004</a>)</span> and to the recent improvements mentioned in <span class="citation">Ledoit and Wolf (<a href="#ref-ledoit2017nonlinear">2017</a>)</span>. In short, we use <span class="math inline">\(\hat{\boldsymbol{\Sigma}}=\boldsymbol{\Sigma}_S+\delta \boldsymbol{I}\)</span> for some small constant <span class="math inline">\(\delta\)</span> (equal to 0.01 in the code below).</p>

<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb36-1" data-line-number="1">weights_multi &lt;-<span class="st"> </span><span class="cf">function</span>(returns,j, alpha, lambda){</a>
<a class="sourceLine" id="cb36-2" data-line-number="2">    N &lt;-<span class="st"> </span><span class="kw">ncol</span>(returns)</a>
<a class="sourceLine" id="cb36-3" data-line-number="3">    <span class="cf">if</span>(j <span class="op">==</span><span class="st"> </span><span class="dv">1</span>){                                    <span class="co"># j = 1 =&gt; EW</span></a>
<a class="sourceLine" id="cb36-4" data-line-number="4">        <span class="kw">return</span>(<span class="kw">rep</span>(<span class="dv">1</span><span class="op">/</span>N,N))</a>
<a class="sourceLine" id="cb36-5" data-line-number="5">    }</a>
<a class="sourceLine" id="cb36-6" data-line-number="6">    <span class="cf">if</span>(j <span class="op">==</span><span class="st"> </span><span class="dv">2</span>){                                    <span class="co"># j = 2 =&gt; Minimum Variance</span></a>
<a class="sourceLine" id="cb36-7" data-line-number="7">        sigma &lt;-<span class="st"> </span><span class="kw">cov</span>(returns) <span class="op">+</span><span class="st"> </span><span class="fl">0.01</span> <span class="op">*</span><span class="st"> </span><span class="kw">diag</span>(N)     <span class="co"># Covariance matrix + regularizing term</span></a>
<a class="sourceLine" id="cb36-8" data-line-number="8">        w &lt;-<span class="st"> </span><span class="kw">solve</span>(sigma) <span class="op">%*%</span><span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>,N)             <span class="co"># Inverse &amp; multiply</span></a>
<a class="sourceLine" id="cb36-9" data-line-number="9">        <span class="kw">return</span>(w <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(w))                         <span class="co"># Normalize</span></a>
<a class="sourceLine" id="cb36-10" data-line-number="10">    }</a>
<a class="sourceLine" id="cb36-11" data-line-number="11">    <span class="cf">if</span>(j <span class="op">==</span><span class="st"> </span><span class="dv">3</span>){                                    <span class="co"># j = 3 =&gt; Penalised / elasticnet</span></a>
<a class="sourceLine" id="cb36-12" data-line-number="12">        w &lt;-<span class="st"> </span><span class="kw">weights_sparsehedge</span>(returns, alpha, lambda)</a>
<a class="sourceLine" id="cb36-13" data-line-number="13">    }</a>
<a class="sourceLine" id="cb36-14" data-line-number="14">}</a></code></pre></div>
<p></p>
<p>Finally, we proceed to the backtesting loop. Given the number of assets, the execution of the loop takes a few minutes. At the end of the loop, we compute the standard deviation of portfolio returns (monthly volatility). This is the key indicator as minimum variance seeks to minimize this particular metric.</p>

<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb37-1" data-line-number="1"><span class="cf">for</span>(t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(t_oos)){                                                 <span class="co"># Loop = rebal. dates</span></a>
<a class="sourceLine" id="cb37-2" data-line-number="2">    temp_data &lt;-<span class="st"> </span>returns <span class="op">%&gt;%</span><span class="st">                                               </span><span class="co"># Data for weights</span></a>
<a class="sourceLine" id="cb37-3" data-line-number="3"><span class="st">        </span><span class="kw">filter</span>(date <span class="op">&lt;</span><span class="st"> </span>t_oos[t]) <span class="op">%&gt;%</span><span class="st">                                        </span><span class="co"># Expand. window</span></a>
<a class="sourceLine" id="cb37-4" data-line-number="4"><span class="st">        </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="op">-</span>date) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb37-5" data-line-number="5"><span class="st">        </span><span class="kw">as.matrix</span>() </a>
<a class="sourceLine" id="cb37-6" data-line-number="6">    realised_returns &lt;-<span class="st"> </span>returns <span class="op">%&gt;%</span><span class="st">                                        </span><span class="co"># OOS returns</span></a>
<a class="sourceLine" id="cb37-7" data-line-number="7"><span class="st">        </span><span class="kw">filter</span>(date <span class="op">==</span><span class="st">  </span>t_oos[t]) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb37-8" data-line-number="8"><span class="st">        </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="op">-</span>date)</a>
<a class="sourceLine" id="cb37-9" data-line-number="9">    <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>nb_port){                                                   <span class="co"># Loop over strats</span></a>
<a class="sourceLine" id="cb37-10" data-line-number="10">        portf_weights[t,j,] &lt;-<span class="st"> </span><span class="kw">weights_multi</span>(temp_data, j, <span class="fl">0.1</span>, <span class="fl">0.1</span>)       <span class="co"># Hard-coded params!</span></a>
<a class="sourceLine" id="cb37-11" data-line-number="11">        portf_returns[t,j] &lt;-<span class="st"> </span><span class="kw">sum</span>(portf_weights[t,j,] <span class="op">*</span><span class="st"> </span>realised_returns)  <span class="co"># Portf. returns</span></a>
<a class="sourceLine" id="cb37-12" data-line-number="12">    }</a>
<a class="sourceLine" id="cb37-13" data-line-number="13">}</a>
<a class="sourceLine" id="cb37-14" data-line-number="14"><span class="kw">colnames</span>(portf_returns) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;EW&quot;</span>, <span class="st">&quot;MV&quot;</span>, <span class="st">&quot;Sparse&quot;</span>) <span class="co"># Colnames</span></a>
<a class="sourceLine" id="cb37-15" data-line-number="15"><span class="kw">apply</span>(portf_returns, <span class="dv">2</span>, sd)                        <span class="co"># Portfolio volatilities (monthly scale)</span></a></code></pre></div>
<pre><code>##         EW         MV     Sparse 
## 0.04180422 0.03350424 0.02672169</code></pre>
<p></p>
<p>The aim of the sparse hedging restrictions is to provide a better estimate of the covariance structure of assets so that the estimation of minimum variance portfolio weights is more accurate. From the above exercise, we see that the monthly volatility is indeed reduced when building covariance matrices based on sparse hedging relationships. This is not the case if we use the shrunk sample covariance matrix because there is probably too much noise in the estimates of correlations between assets. Working with daily returns would likely improve the quality of the estimates. But the above backtest shows that the penalized methodology performs well even when the number of observations (dates) is small compared to the number of assets.</p>
</div>
</div>
<div id="predictive-regressions" class="section level2">
<h2><span class="header-section-number">6.3</span> Predictive regressions</h2>
<div id="literature-review-and-principle" class="section level3">
<h3><span class="header-section-number">6.3.1</span> Literature review and principle</h3>
<p>The topic of predictive regressions sits on a collection of very interesting articles. One influential contribution is <span class="citation">Stambaugh (<a href="#ref-stambaugh1999predictive">1999</a>)</span>, where the author shows the perils of regressions in which the independent variables are autocorrelated. In this case, the usual OLS estimate is biased and must therefore be corrected. The results have since then been extended in numerous directions (see <span class="citation">Campbell and Yogo (<a href="#ref-campbell2006efficient">2006</a>)</span> and <span class="citation">Hjalmarsson (<a href="#ref-hjalmarsson2011new">2011</a>)</span>, the survey <span class="citation">Gonzalo and Pitarakis (<a href="#ref-gonzalo2018predictive">2018</a>)</span> and, more recently, the study of <span class="citation">Xu (<a href="#ref-xu2020testing">2020</a>)</span> on predicability over multiple horizons).</p>
<p>A second important topic pertains to the time-dependence of the coefficients in predictive regressions. One contribution in this direction is <span class="citation">Dangl and Halling (<a href="#ref-dangl2012predictive">2012</a>)</span>, where coefficients are estimated via a Bayesian procedure. More recently <span class="citation">Kelly, Pruitt, and Su (<a href="#ref-kelly2019characteristics">2019</a>)</span> use time-dependent factor loadings to model the cross-section of stock returns. The time-varying nature of coefficients of predictive regressions is further documented by <span class="citation">Henkel, Martin, and Nardari (<a href="#ref-henkel2011time">2011</a>)</span> for short term returns. Lastly, <span class="citation">Farmer, Schmidt, and Timmermann (<a href="#ref-farmer2019pockets">2019</a>)</span> introduce the concept of pockets of predictability: assets or markets experiences different phases; in some phases they are predictable and in some others, they arenâ€™t. Pockets are measured both by the number of days that a <em>t</em>-statistic is above a particular threshold and by the magnitude of the <span class="math inline">\(R^2\)</span> over the considered period. Forma statistical tests are developed by <span class="citation">Demetrescu et al. (<a href="#ref-demetrescu2020testing">2020</a>)</span>.</p>
<p>The introduction of penalization within predictive regressions goes back at least to <span class="citation">Rapach, Strauss, and Zhou (<a href="#ref-rapach2013international">2013</a>)</span>, where they are used to assess lead-lag relationships between US markets and other international stock exchanges. More recently, <span class="citation">Alexander Chinco, Clark-Joseph, and Ye (<a href="#ref-chinco2019sparse">2019</a>)</span> use LASSO regressions to forecast high frequency returns based on past returns (in the cross-section) at various horizons. They report statistically significant gains. <span class="citation">Han et al. (<a href="#ref-han2018firm">2019</a>)</span> and <span class="citation">Rapach and Zhou (<a href="#ref-rapach2019time">2019</a>)</span> use LASSO and elasticnet regressions (respectively) to improve forecast combinations and single out the characteristics that matter when explaining stock returns.</p>
<p>These contributions underline the relevance of the overlap between predictive regressions and penalized regressions. In simple machine-learning based asset pricing, we often seek to build models such as that of Equation <a href="factor.html#eq:genML">(4.6)</a>. If we stick to a linear relationship and add penalization terms, then the model becomes:
<span class="math display">\[r_{t+1,n} = \alpha_n + \sum_{k=1}^K\beta_n^kf^k_{t,n}+\epsilon_{t+1,n}, \quad \text{s.t.} \quad (1-\alpha)\sum_{j=1}^J |\beta_j| +\alpha\sum_{j=1}^J \beta_j^2&lt; \theta\]</span>
where we use <span class="math inline">\(f^k_{t,n}\)</span> or <span class="math inline">\(x_{t,n}^k\)</span> interchangeably and <span class="math inline">\(\theta\)</span> is some penalization intensity. Again, one of the aim of the regularization is to generate more robust estimates. If the patterns extracted hold out of sample, then
<span class="math display">\[\hat{r}_{t+1,n} = \hat{\alpha}_n + \sum_{k=1}^K\hat{\beta}_n^kf^k_{t,n},\]</span>
will be a relatively reliable proxy of future performance.</p>
</div>
<div id="code-and-results" class="section level3">
<h3><span class="header-section-number">6.3.2</span> Code and results</h3>
<p>Given the form of our dataset, implementing penalized predictive regressions is easy.</p>

<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb39-1" data-line-number="1">y_penalized_train &lt;-<span class="st"> </span>training_sample<span class="op">$</span>R1M_Usd                 <span class="co"># Dependent variable</span></a>
<a class="sourceLine" id="cb39-2" data-line-number="2">x_penalized_train &lt;-<span class="st"> </span>training_sample <span class="op">%&gt;%</span><span class="st">                     </span><span class="co"># Predictors</span></a>
<a class="sourceLine" id="cb39-3" data-line-number="3"><span class="st">    </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="kw">all_of</span>(features)) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.matrix</span>()                  </a>
<a class="sourceLine" id="cb39-4" data-line-number="4">fit_pen_pred &lt;-<span class="st"> </span><span class="kw">glmnet</span>(x_penalized_train, y_penalized_train, <span class="co"># Model</span></a>
<a class="sourceLine" id="cb39-5" data-line-number="5">                       <span class="dt">alpha =</span> <span class="fl">0.1</span>, <span class="dt">lambda =</span> <span class="fl">0.1</span>)</a></code></pre></div>
<p></p>
<p>We then report two key performance measures: the mean squared error and the hit ratio, which is the proportion of times that the prediction guesses the sign of the return correctly. A detailed account of metrics is given later in the book (Chapter <a href="backtest.html#backtest">13</a>).</p>

<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb40-1" data-line-number="1">x_penalized_test &lt;-<span class="st"> </span>testing_sample <span class="op">%&gt;%</span><span class="st">                                     </span><span class="co"># Predictors</span></a>
<a class="sourceLine" id="cb40-2" data-line-number="2"><span class="st">    </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="kw">all_of</span>(features)) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.matrix</span>()         </a>
<a class="sourceLine" id="cb40-3" data-line-number="3"><span class="kw">mean</span>((<span class="kw">predict</span>(fit_pen_pred, x_penalized_test) <span class="op">-</span><span class="st"> </span>testing_sample<span class="op">$</span>R1M_Usd)<span class="op">^</span><span class="dv">2</span>) <span class="co"># MSE</span></a></code></pre></div>
<pre><code>## [1] 0.03699696</code></pre>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb42-1" data-line-number="1"><span class="kw">mean</span>(<span class="kw">predict</span>(fit_pen_pred, x_penalized_test) <span class="op">*</span><span class="st"> </span>testing_sample<span class="op">$</span>R1M_Usd <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>) <span class="co"># Hit ratio</span></a></code></pre></div>
<pre><code>## [1] 0.5460346</code></pre>
<p></p>
<p>From an investorâ€™s standpoint, the MSE (or even the mean absolute error) are hard to interpret because it is complicated to map them mentally into some intuitive financial indicator. In this perspective, the hit ratio is more natural. It tells the proportion of correct signs achieved by the predictions. If the investor is long in positive signals and short in negative ones, the hit ratio indicates the proportion of â€˜correctâ€™ bets (the positions that go in the expected direction). A natural threshold is 50% but because of transaction costs, 51% of accurate forecasts probably wonâ€™t be profitable. The figure 0.5460346 can be deemed a relatively good hit ratio, though not a very impressive one.</p>
</div>
</div>
<div id="coding-exercise" class="section level2">
<h2><span class="header-section-number">6.4</span> Coding exercise</h2>
<p>On the test sample, evaluate the impact of the two elastic net parameters on out-of-sample accuracy.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-ban2016machine">
<p>Ban, Gah-Yi, Noureddine El Karoui, and Andrew EB Lim. 2016. â€œMachine Learning and Portfolio Optimization.â€ <em>Management Science</em> 64 (3): 1136â€“54.</p>
</div>
<div id="ref-brodie2009sparse">
<p>Brodie, Joshua, Ingrid Daubechies, Christine De Mol, Domenico Giannone, and Ignace Loris. 2009. â€œSparse and Stable Markowitz Portfolios.â€ <em>Proceedings of the National Academy of Sciences</em> 106 (30): 12267â€“72.</p>
</div>
<div id="ref-campbell2006efficient">
<p>Campbell, John Y, and Motohiro Yogo. 2006. â€œEfficient Tests of Stock Return Predictability.â€ <em>Journal of Financial Economics</em> 81 (1): 27â€“60.</p>
</div>
<div id="ref-chinco2019sparse">
<p>Chinco, Alexander, Adam D Clark-Joseph, and Mao Ye. 2019. â€œSparse Signals in the Cross-Section of Returns.â€ <em>Journal of Finance</em> 74 (1): 449â€“92.</p>
</div>
<div id="ref-coqueret2015diversified">
<p>Coqueret, Guillaume. 2015. â€œDiversified Minimum-Variance Portfolios.â€ <em>Annals of Finance</em> 11 (2): 221â€“41.</p>
</div>
<div id="ref-dangl2012predictive">
<p>Dangl, Thomas, and Michael Halling. 2012. â€œPredictive Regressions with Time-Varying Coefficients.â€ <em>Journal of Financial Economics</em> 106 (1): 157â€“81.</p>
</div>
<div id="ref-d2011identifying">
<p>dâ€™Aspremont, Alexandre. 2011. â€œIdentifying Small Mean-Reverting Portfolios.â€ <em>Quantitative Finance</em> 11 (3). Taylor &amp; Francis: 351â€“64.</p>
</div>
<div id="ref-demetrescu2020testing">
<p>Demetrescu, Matei, Iliyan Georgiev, Paulo MM Rodrigues, and AM Taylor. 2020. â€œTesting for Episodic Predictability in Stock Returnsâ€ Forthcoming. Journal of Econometrics.</p>
</div>
<div id="ref-farmer2019pockets">
<p>Farmer, Leland, Lawrence Schmidt, and Allan Timmermann. 2019. â€œPockets of Predictability.â€ <em>SSRN Working Paper</em> 3152386.</p>
</div>
<div id="ref-fastrich2015constructing">
<p>Fastrich, BjÃ¶rn, Sandra Paterlini, and Peter Winker. 2015. â€œConstructing Optimal Sparse Portfolios Using Regularization Methods.â€ <em>Computational Management Science</em> 12 (3): 417â€“34.</p>
</div>
<div id="ref-freyberger2020dissecting">
<p>Freyberger, Joachim, Andreas Neuhierl, and Michael Weber. 2020. â€œDissecting Characteristics Nonparametrically.â€ <em>Review of Financial Studies</em> Forthcoming.</p>
</div>
<div id="ref-friedman2008sparse">
<p>Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2008. â€œSparse Inverse Covariance Estimation with the Graphical Lasso.â€ <em>Biostatistics</em> 9 (3): 432â€“41.</p>
</div>
<div id="ref-gonzalo2018predictive">
<p>Gonzalo, JesÃºs, and Jean-Yves Pitarakis. 2018. â€œPredictive Regressions.â€ In <em>Oxford Research Encyclopedia of Economics and Finance</em>.</p>
</div>
<div id="ref-goto2015improving">
<p>Goto, Shingo, and Yan Xu. 2015. â€œImproving Mean Variance Optimization Through Sparse Hedging Restrictions.â€ <em>Journal of Financial and Quantitative Analysis</em> 50 (6): 1415â€“41.</p>
</div>
<div id="ref-greene2018econometric">
<p>Greene, William H. 2018. <em>Econometric Analysis, Eighth Edition</em>. Pearson Education.</p>
</div>
<div id="ref-han2018firm">
<p>Han, Yufeng, Ai He, D Rapach, and Guofu Zhou. 2019. â€œFirm Characteristics and Expected Stock Returns.â€ <em>SSRN Working Paper</em> 3185335.</p>
</div>
<div id="ref-henkel2011time">
<p>Henkel, Sam James, J Spencer Martin, and Federico Nardari. 2011. â€œTime-Varying Short-Horizon Predictability.â€ <em>Journal of Financial Economics</em> 99 (3): 560â€“80.</p>
</div>
<div id="ref-hjalmarsson2011new">
<p>Hjalmarsson, Erik. 2011. â€œNew Methods for Inference in Long-Horizon Regressions.â€ <em>Journal of Financial and Quantitative Analysis</em> 46 (3): 815â€“39.</p>
</div>
<div id="ref-kelly2019characteristics">
<p>Kelly, Bryan T, Seth Pruitt, and Yinan Su. 2019. â€œCharacteristics Are Covariances: A Unified Model of Risk and Return.â€ <em>Journal of Financial Economics</em> 134 (3): 501â€“24.</p>
</div>
<div id="ref-kremer2019sparse">
<p>Kremer, Philipp J, Sangkyun Lee, MaÅ‚gorzata Bogdan, and Sandra Paterlini. 2019. â€œSparse Portfolio Selection via the Sorted L1-Norm.â€ <em>Journal of Banking &amp; Finance</em>, 105687.</p>
</div>
<div id="ref-ledoit2004well">
<p>Ledoit, Olivier, and Michael Wolf. 2004. â€œA Well-Conditioned Estimator for Large-Dimensional Covariance Matrices.â€ <em>Journal of Multivariate Analysis</em> 88 (2): 365â€“411.</p>
</div>
<div id="ref-ledoit2017nonlinear">
<p>Ledoit, Olivier, and Michael Wolf. 2017. â€œNonlinear Shrinkage of the Covariance Matrix for Portfolio Selection: Markowitz Meets Goldilocks.â€ <em>Review of Financial Studies</em> 30 (12): 4349â€“88.</p>
</div>
<div id="ref-legendre1805nouvelles">
<p>Legendre, Adrien Marie. 1805. <em>Nouvelles MÃ©thodes Pour La dÃ©termination Des Orbites Des ComÃ¨tes</em>. F. Didot.</p>
</div>
<div id="ref-markowitz1952portfolio">
<p>Markowitz, Harry. 1952. â€œPortfolio Selection.â€ <em>Journal of Finance</em> 7 (1): 77â€“91.</p>
</div>
<div id="ref-rapach2013international">
<p>Rapach, David E, Jack K Strauss, and Guofu Zhou. 2013. â€œInternational Stock Return Predictability: What Is the Role of the United States?â€ <em>Journal of Finance</em> 68 (4): 1633â€“62.</p>
</div>
<div id="ref-rapach2019time">
<p>Rapach, David, and Guofu Zhou. 2019. â€œTime-Series and Cross-Sectional Stock Return Forecasting: New Machine Learning Methods.â€ <em>SSRN Working Paper</em> 3428095.</p>
</div>
<div id="ref-stambaugh1999predictive">
<p>Stambaugh, Robert F. 1999. â€œPredictive Regressions.â€ <em>Journal of Financial Economics</em> 54 (3): 375â€“421.</p>
</div>
<div id="ref-stevens1998inverse">
<p>Stevens, Guy VG. 1998. â€œOn the Inverse of the Covariance Matrix in Portfolio Analysis.â€ <em>Journal of Finance</em> 53 (5): 1821â€“7.</p>
</div>
<div id="ref-tibshirani1996regression">
<p>Tibshirani, Robert. 1996. â€œRegression Shrinkage and Selection via the Lasso.â€ <em>Journal of the Royal Statistical Society. Series B (Methodological)</em>, 267â€“88.</p>
</div>
<div id="ref-uematsu2019high">
<p>Uematsu, Yoshimasa, and Shinya Tanaka. 2019. â€œHigh-Dimensional Macroeconomic Forecasting and Variable Selection via Penalized Regression.â€ <em>Econometrics Journal</em> 22 (1): 34â€“56.</p>
</div>
<div id="ref-xu2020testing">
<p>Xu, Ke-Li. 2020. â€œTesting for Multiple-Horizon Predictability: Direct Regression Based Versus Implication Based.â€ <em>Review of Financial Studies</em> Forthcoming.</p>
</div>
<div id="ref-zou2005regularization">
<p>Zou, Hui, and Trevor Hastie. 2005. â€œRegularization and Variable Selection via the Elastic Net.â€ <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 67 (2): 301â€“20.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="Data.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="trees.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["ML_factor.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true,
"info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
