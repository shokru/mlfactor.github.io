<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 17 Reinforcement learning | Machine Learning for Factor Investing</title>
  <meta name="description" content="Chapter 17 Reinforcement learning | Machine Learning for Factor Investing" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 17 Reinforcement learning | Machine Learning for Factor Investing" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 17 Reinforcement learning | Machine Learning for Factor Investing" />
  
  
  

<meta name="author" content="Guillaume Coqueret and Tony Guida" />


<meta name="date" content="2020-03-28" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="unsup.html"/>
<link rel="next" href="data-description.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="part"><span><b>I Introduction</b></span></li>
<li class="chapter" data-level="1" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="preface.html"><a href="preface.html#what-this-book-is-not-about"><i class="fa fa-check"></i><b>1.1</b> What this book is not about</a></li>
<li class="chapter" data-level="1.2" data-path="preface.html"><a href="preface.html#the-targeted-audience"><i class="fa fa-check"></i><b>1.2</b> The targeted audience</a></li>
<li class="chapter" data-level="1.3" data-path="preface.html"><a href="preface.html#how-this-book-is-structured"><i class="fa fa-check"></i><b>1.3</b> How this book is structured</a></li>
<li class="chapter" data-level="1.4" data-path="preface.html"><a href="preface.html#companion-website"><i class="fa fa-check"></i><b>1.4</b> Companion website</a></li>
<li class="chapter" data-level="1.5" data-path="preface.html"><a href="preface.html#why-r"><i class="fa fa-check"></i><b>1.5</b> Why R?</a></li>
<li class="chapter" data-level="1.6" data-path="preface.html"><a href="preface.html#coding-instructions"><i class="fa fa-check"></i><b>1.6</b> Coding instructions</a></li>
<li class="chapter" data-level="1.7" data-path="preface.html"><a href="preface.html#acknowledgements"><i class="fa fa-check"></i><b>1.7</b> Acknowledgements</a></li>
<li class="chapter" data-level="1.8" data-path="preface.html"><a href="preface.html#future-developments"><i class="fa fa-check"></i><b>1.8</b> Future developments</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="notdata.html"><a href="notdata.html"><i class="fa fa-check"></i><b>2</b> Notations and data</a><ul>
<li class="chapter" data-level="2.1" data-path="notdata.html"><a href="notdata.html#notations"><i class="fa fa-check"></i><b>2.1</b> Notations</a></li>
<li class="chapter" data-level="2.2" data-path="notdata.html"><a href="notdata.html#dataset"><i class="fa fa-check"></i><b>2.2</b> Dataset</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>3</b> Introduction</a><ul>
<li class="chapter" data-level="3.1" data-path="intro.html"><a href="intro.html#context"><i class="fa fa-check"></i><b>3.1</b> Context</a></li>
<li class="chapter" data-level="3.2" data-path="intro.html"><a href="intro.html#portfolio-construction-the-workflow"><i class="fa fa-check"></i><b>3.2</b> Portfolio construction: the workflow</a></li>
<li class="chapter" data-level="3.3" data-path="intro.html"><a href="intro.html#machine-learning-is-no-magic-wand"><i class="fa fa-check"></i><b>3.3</b> Machine Learning is no Magic Wand</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="factor.html"><a href="factor.html"><i class="fa fa-check"></i><b>4</b> Factor investing and asset pricing anomalies</a><ul>
<li class="chapter" data-level="4.1" data-path="factor.html"><a href="factor.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="factor.html"><a href="factor.html#detecting-anomalies"><i class="fa fa-check"></i><b>4.2</b> Detecting anomalies</a><ul>
<li class="chapter" data-level="4.2.1" data-path="factor.html"><a href="factor.html#simple-portfolio-sorts"><i class="fa fa-check"></i><b>4.2.1</b> Simple portfolio sorts</a></li>
<li class="chapter" data-level="4.2.2" data-path="factor.html"><a href="factor.html#factors"><i class="fa fa-check"></i><b>4.2.2</b> Factors</a></li>
<li class="chapter" data-level="4.2.3" data-path="factor.html"><a href="factor.html#predictive-regressions-sorts-and-p-value-issues"><i class="fa fa-check"></i><b>4.2.3</b> Predictive regressions, sorts, and p-value issues</a></li>
<li class="chapter" data-level="4.2.4" data-path="factor.html"><a href="factor.html#fama-macbeth-regressions"><i class="fa fa-check"></i><b>4.2.4</b> Fama-Macbeth regressions</a></li>
<li class="chapter" data-level="4.2.5" data-path="factor.html"><a href="factor.html#factor-competition"><i class="fa fa-check"></i><b>4.2.5</b> Factor competition</a></li>
<li class="chapter" data-level="4.2.6" data-path="factor.html"><a href="factor.html#advanced-techniques"><i class="fa fa-check"></i><b>4.2.6</b> Advanced techniques</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="factor.html"><a href="factor.html#factors-or-characteristics"><i class="fa fa-check"></i><b>4.3</b> Factors or characteristics?</a></li>
<li class="chapter" data-level="4.4" data-path="factor.html"><a href="factor.html#hot-topics-momentum-timing-and-esg"><i class="fa fa-check"></i><b>4.4</b> Hot topics: momentum, timing and ESG</a><ul>
<li class="chapter" data-level="4.4.1" data-path="factor.html"><a href="factor.html#factor-momentum"><i class="fa fa-check"></i><b>4.4.1</b> Factor momentum</a></li>
<li class="chapter" data-level="4.4.2" data-path="factor.html"><a href="factor.html#factor-timing"><i class="fa fa-check"></i><b>4.4.2</b> Factor timing</a></li>
<li class="chapter" data-level="4.4.3" data-path="factor.html"><a href="factor.html#the-green-factors"><i class="fa fa-check"></i><b>4.4.3</b> The green factors</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="factor.html"><a href="factor.html#the-link-with-machine-learning"><i class="fa fa-check"></i><b>4.5</b> The link with machine learning</a><ul>
<li class="chapter" data-level="4.5.1" data-path="factor.html"><a href="factor.html#a-short-list-of-recent-references"><i class="fa fa-check"></i><b>4.5.1</b> A short list of recent references</a></li>
<li class="chapter" data-level="4.5.2" data-path="factor.html"><a href="factor.html#explicit-connections-with-asset-pricing-models"><i class="fa fa-check"></i><b>4.5.2</b> Explicit connections with asset pricing models</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="factor.html"><a href="factor.html#coding-exercises"><i class="fa fa-check"></i><b>4.6</b> Coding exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Data.html"><a href="Data.html"><i class="fa fa-check"></i><b>5</b> Data preprocessing</a><ul>
<li class="chapter" data-level="5.1" data-path="Data.html"><a href="Data.html#know-your-data"><i class="fa fa-check"></i><b>5.1</b> Know your data</a></li>
<li class="chapter" data-level="5.2" data-path="Data.html"><a href="Data.html#missing-data"><i class="fa fa-check"></i><b>5.2</b> Missing data</a></li>
<li class="chapter" data-level="5.3" data-path="Data.html"><a href="Data.html#outlier-detection"><i class="fa fa-check"></i><b>5.3</b> Outlier detection</a></li>
<li class="chapter" data-level="5.4" data-path="Data.html"><a href="Data.html#feateng"><i class="fa fa-check"></i><b>5.4</b> Feature engineering</a><ul>
<li class="chapter" data-level="5.4.1" data-path="Data.html"><a href="Data.html#feature-selection"><i class="fa fa-check"></i><b>5.4.1</b> Feature selection</a></li>
<li class="chapter" data-level="5.4.2" data-path="Data.html"><a href="Data.html#scaling"><i class="fa fa-check"></i><b>5.4.2</b> Scaling the predictors</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="Data.html"><a href="Data.html#labelling"><i class="fa fa-check"></i><b>5.5</b> Labelling</a><ul>
<li class="chapter" data-level="5.5.1" data-path="Data.html"><a href="Data.html#simple-labels"><i class="fa fa-check"></i><b>5.5.1</b> Simple labels</a></li>
<li class="chapter" data-level="5.5.2" data-path="Data.html"><a href="Data.html#categorical-labels"><i class="fa fa-check"></i><b>5.5.2</b> Categorical labels</a></li>
<li class="chapter" data-level="5.5.3" data-path="Data.html"><a href="Data.html#the-triple-barrier-method"><i class="fa fa-check"></i><b>5.5.3</b> The triple barrier method</a></li>
<li class="chapter" data-level="5.5.4" data-path="Data.html"><a href="Data.html#filtering-the-sample"><i class="fa fa-check"></i><b>5.5.4</b> Filtering the sample</a></li>
<li class="chapter" data-level="5.5.5" data-path="Data.html"><a href="Data.html#horizons"><i class="fa fa-check"></i><b>5.5.5</b> Return horizons</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="Data.html"><a href="Data.html#pers"><i class="fa fa-check"></i><b>5.6</b> Handling persistence</a></li>
<li class="chapter" data-level="5.7" data-path="Data.html"><a href="Data.html#extensions"><i class="fa fa-check"></i><b>5.7</b> Extensions</a><ul>
<li class="chapter" data-level="5.7.1" data-path="Data.html"><a href="Data.html#transforming-features"><i class="fa fa-check"></i><b>5.7.1</b> Transforming features</a></li>
<li class="chapter" data-level="5.7.2" data-path="Data.html"><a href="Data.html#macrovar"><i class="fa fa-check"></i><b>5.7.2</b> Macro-economic variables</a></li>
<li class="chapter" data-level="5.7.3" data-path="Data.html"><a href="Data.html#active-learning"><i class="fa fa-check"></i><b>5.7.3</b> Active learning</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="Data.html"><a href="Data.html#additional-code-and-results"><i class="fa fa-check"></i><b>5.8</b> Additional code and results</a><ul>
<li class="chapter" data-level="5.8.1" data-path="Data.html"><a href="Data.html#impact-of-rescaling-graphical-representation"><i class="fa fa-check"></i><b>5.8.1</b> Impact of rescaling: graphical representation</a></li>
<li class="chapter" data-level="5.8.2" data-path="Data.html"><a href="Data.html#impact-of-rescaling-toy-example"><i class="fa fa-check"></i><b>5.8.2</b> Impact of rescaling: toy example</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="Data.html"><a href="Data.html#coding-exercises-1"><i class="fa fa-check"></i><b>5.9</b> Coding exercises</a></li>
</ul></li>
<li class="part"><span><b>II Common supervised algorithms</b></span></li>
<li class="chapter" data-level="6" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>6</b> Penalized regressions and sparse hedging for minimum variance portfolios</a><ul>
<li class="chapter" data-level="6.1" data-path="lasso.html"><a href="lasso.html#penalised-regressions"><i class="fa fa-check"></i><b>6.1</b> Penalised regressions</a><ul>
<li class="chapter" data-level="6.1.1" data-path="lasso.html"><a href="lasso.html#penreg"><i class="fa fa-check"></i><b>6.1.1</b> Simple regressions</a></li>
<li class="chapter" data-level="6.1.2" data-path="lasso.html"><a href="lasso.html#forms-of-penalizations"><i class="fa fa-check"></i><b>6.1.2</b> Forms of penalizations</a></li>
<li class="chapter" data-level="6.1.3" data-path="lasso.html"><a href="lasso.html#illustrations"><i class="fa fa-check"></i><b>6.1.3</b> Illustrations</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="lasso.html"><a href="lasso.html#sparse-hedging-for-minimum-variance-portfolios"><i class="fa fa-check"></i><b>6.2</b> Sparse hedging for minimum variance portfolios</a><ul>
<li class="chapter" data-level="6.2.1" data-path="lasso.html"><a href="lasso.html#presentation-and-derivations"><i class="fa fa-check"></i><b>6.2.1</b> Presentation and derivations</a></li>
<li class="chapter" data-level="6.2.2" data-path="lasso.html"><a href="lasso.html#sparseex"><i class="fa fa-check"></i><b>6.2.2</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="lasso.html"><a href="lasso.html#predictive-regressions"><i class="fa fa-check"></i><b>6.3</b> Predictive regressions</a><ul>
<li class="chapter" data-level="6.3.1" data-path="lasso.html"><a href="lasso.html#literature-review-and-principle"><i class="fa fa-check"></i><b>6.3.1</b> Literature review and principle</a></li>
<li class="chapter" data-level="6.3.2" data-path="lasso.html"><a href="lasso.html#code-and-results"><i class="fa fa-check"></i><b>6.3.2</b> Code and results</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="lasso.html"><a href="lasso.html#coding-exercise"><i class="fa fa-check"></i><b>6.4</b> Coding exercise</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="trees.html"><a href="trees.html"><i class="fa fa-check"></i><b>7</b> Tree-based methods</a><ul>
<li class="chapter" data-level="7.1" data-path="trees.html"><a href="trees.html#simple-trees"><i class="fa fa-check"></i><b>7.1</b> Simple trees</a><ul>
<li class="chapter" data-level="7.1.1" data-path="trees.html"><a href="trees.html#principle"><i class="fa fa-check"></i><b>7.1.1</b> Principle</a></li>
<li class="chapter" data-level="7.1.2" data-path="trees.html"><a href="trees.html#treeclass"><i class="fa fa-check"></i><b>7.1.2</b> Further details on classification</a></li>
<li class="chapter" data-level="7.1.3" data-path="trees.html"><a href="trees.html#pruning-criteria"><i class="fa fa-check"></i><b>7.1.3</b> Pruning criteria</a></li>
<li class="chapter" data-level="7.1.4" data-path="trees.html"><a href="trees.html#code-and-interpretation"><i class="fa fa-check"></i><b>7.1.4</b> Code and interpretation</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="trees.html"><a href="trees.html#random-forests"><i class="fa fa-check"></i><b>7.2</b> Random forests</a><ul>
<li class="chapter" data-level="7.2.1" data-path="trees.html"><a href="trees.html#principle-1"><i class="fa fa-check"></i><b>7.2.1</b> Principle</a></li>
<li class="chapter" data-level="7.2.2" data-path="trees.html"><a href="trees.html#code-and-results-1"><i class="fa fa-check"></i><b>7.2.2</b> Code and results</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="trees.html"><a href="trees.html#adaboost"><i class="fa fa-check"></i><b>7.3</b> Boosted trees: Adaboost</a><ul>
<li class="chapter" data-level="7.3.1" data-path="trees.html"><a href="trees.html#methodology"><i class="fa fa-check"></i><b>7.3.1</b> Methodology</a></li>
<li class="chapter" data-level="7.3.2" data-path="trees.html"><a href="trees.html#illustration"><i class="fa fa-check"></i><b>7.3.2</b> Illustration</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="trees.html"><a href="trees.html#boosted-trees-extreme-gradient-boosting"><i class="fa fa-check"></i><b>7.4</b> Boosted trees: extreme gradient boosting</a><ul>
<li class="chapter" data-level="7.4.1" data-path="trees.html"><a href="trees.html#managing-loss"><i class="fa fa-check"></i><b>7.4.1</b> Managing Loss</a></li>
<li class="chapter" data-level="7.4.2" data-path="trees.html"><a href="trees.html#penalisation"><i class="fa fa-check"></i><b>7.4.2</b> Penalisation</a></li>
<li class="chapter" data-level="7.4.3" data-path="trees.html"><a href="trees.html#aggregation"><i class="fa fa-check"></i><b>7.4.3</b> Aggregation</a></li>
<li class="chapter" data-level="7.4.4" data-path="trees.html"><a href="trees.html#tree-structure"><i class="fa fa-check"></i><b>7.4.4</b> Tree structure</a></li>
<li class="chapter" data-level="7.4.5" data-path="trees.html"><a href="trees.html#boostext"><i class="fa fa-check"></i><b>7.4.5</b> Extensions</a></li>
<li class="chapter" data-level="7.4.6" data-path="trees.html"><a href="trees.html#boostcode"><i class="fa fa-check"></i><b>7.4.6</b> Code and results</a></li>
<li class="chapter" data-level="7.4.7" data-path="trees.html"><a href="trees.html#instweight"><i class="fa fa-check"></i><b>7.4.7</b> Instance weighting</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="trees.html"><a href="trees.html#discussion"><i class="fa fa-check"></i><b>7.5</b> Discussion</a></li>
<li class="chapter" data-level="7.6" data-path="trees.html"><a href="trees.html#coding-exercises-2"><i class="fa fa-check"></i><b>7.6</b> Coding exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="NN.html"><a href="NN.html"><i class="fa fa-check"></i><b>8</b> Neural networks</a><ul>
<li class="chapter" data-level="8.1" data-path="NN.html"><a href="NN.html#the-original-perceptron"><i class="fa fa-check"></i><b>8.1</b> The original perceptron</a></li>
<li class="chapter" data-level="8.2" data-path="NN.html"><a href="NN.html#multilayer-perceptron-mlp"><i class="fa fa-check"></i><b>8.2</b> Multilayer perceptron (MLP)</a><ul>
<li class="chapter" data-level="8.2.1" data-path="NN.html"><a href="NN.html#introduction-and-notations"><i class="fa fa-check"></i><b>8.2.1</b> Introduction and notations</a></li>
<li class="chapter" data-level="8.2.2" data-path="NN.html"><a href="NN.html#universal-approximation"><i class="fa fa-check"></i><b>8.2.2</b> Universal approximation</a></li>
<li class="chapter" data-level="8.2.3" data-path="NN.html"><a href="NN.html#backprop"><i class="fa fa-check"></i><b>8.2.3</b> Learning via back-propagation</a></li>
<li class="chapter" data-level="8.2.4" data-path="NN.html"><a href="NN.html#further-details-on-classification"><i class="fa fa-check"></i><b>8.2.4</b> Further details on classification</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="NN.html"><a href="NN.html#howdeep"><i class="fa fa-check"></i><b>8.3</b> How deep should we go? And other practical issues</a><ul>
<li class="chapter" data-level="8.3.1" data-path="NN.html"><a href="NN.html#architectural-choices"><i class="fa fa-check"></i><b>8.3.1</b> Architectural choices</a></li>
<li class="chapter" data-level="8.3.2" data-path="NN.html"><a href="NN.html#frequency-of-weight-updates-and-learning-duration"><i class="fa fa-check"></i><b>8.3.2</b> Frequency of weight updates and learning duration</a></li>
<li class="chapter" data-level="8.3.3" data-path="NN.html"><a href="NN.html#penalizations-and-dropout"><i class="fa fa-check"></i><b>8.3.3</b> Penalizations and dropout</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="NN.html"><a href="NN.html#code-samples-and-comments-for-vanilla-mlp"><i class="fa fa-check"></i><b>8.4</b> Code samples and comments for vanilla MLP</a><ul>
<li class="chapter" data-level="8.4.1" data-path="NN.html"><a href="NN.html#regression-example"><i class="fa fa-check"></i><b>8.4.1</b> Regression example</a></li>
<li class="chapter" data-level="8.4.2" data-path="NN.html"><a href="NN.html#classification-example"><i class="fa fa-check"></i><b>8.4.2</b> Classification example</a></li>
<li class="chapter" data-level="8.4.3" data-path="NN.html"><a href="NN.html#custloss"><i class="fa fa-check"></i><b>8.4.3</b> Custom losses</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="NN.html"><a href="NN.html#recurrent-networks"><i class="fa fa-check"></i><b>8.5</b> Recurrent networks</a><ul>
<li class="chapter" data-level="8.5.1" data-path="NN.html"><a href="NN.html#presentation"><i class="fa fa-check"></i><b>8.5.1</b> Presentation</a></li>
<li class="chapter" data-level="8.5.2" data-path="NN.html"><a href="NN.html#code-and-results-2"><i class="fa fa-check"></i><b>8.5.2</b> Code and results</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="NN.html"><a href="NN.html#other-common-architectures"><i class="fa fa-check"></i><b>8.6</b> Other common architectures</a><ul>
<li class="chapter" data-level="8.6.1" data-path="NN.html"><a href="NN.html#generative-aversarial-networks"><i class="fa fa-check"></i><b>8.6.1</b> Generative adversarial networks</a></li>
<li class="chapter" data-level="8.6.2" data-path="NN.html"><a href="NN.html#autoencoders"><i class="fa fa-check"></i><b>8.6.2</b> Auto-encoders</a></li>
<li class="chapter" data-level="8.6.3" data-path="NN.html"><a href="NN.html#a-word-on-convolutional-networks"><i class="fa fa-check"></i><b>8.6.3</b> A word on convolutional networks</a></li>
<li class="chapter" data-level="8.6.4" data-path="NN.html"><a href="NN.html#advanced-architectures"><i class="fa fa-check"></i><b>8.6.4</b> Advanced architectures</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="NN.html"><a href="NN.html#coding-exercise-1"><i class="fa fa-check"></i><b>8.7</b> Coding exercise</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="svm.html"><a href="svm.html"><i class="fa fa-check"></i><b>9</b> Support vector machines</a><ul>
<li class="chapter" data-level="9.1" data-path="svm.html"><a href="svm.html#svm-for-classification"><i class="fa fa-check"></i><b>9.1</b> SVM for classification</a></li>
<li class="chapter" data-level="9.2" data-path="svm.html"><a href="svm.html#svm-for-regression"><i class="fa fa-check"></i><b>9.2</b> SVM for regression</a></li>
<li class="chapter" data-level="9.3" data-path="svm.html"><a href="svm.html#practice"><i class="fa fa-check"></i><b>9.3</b> Practice</a></li>
<li class="chapter" data-level="9.4" data-path="svm.html"><a href="svm.html#coding-exercises-3"><i class="fa fa-check"></i><b>9.4</b> Coding exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bayes.html"><a href="bayes.html"><i class="fa fa-check"></i><b>10</b> Bayesian methods</a><ul>
<li class="chapter" data-level="10.1" data-path="bayes.html"><a href="bayes.html#the-bayesian-framework"><i class="fa fa-check"></i><b>10.1</b> The Bayesian framework</a></li>
<li class="chapter" data-level="10.2" data-path="bayes.html"><a href="bayes.html#bayesian-sampling"><i class="fa fa-check"></i><b>10.2</b> Bayesian sampling</a><ul>
<li class="chapter" data-level="10.2.1" data-path="bayes.html"><a href="bayes.html#gibbs-sampling"><i class="fa fa-check"></i><b>10.2.1</b> Gibbs sampling</a></li>
<li class="chapter" data-level="10.2.2" data-path="bayes.html"><a href="bayes.html#metropolis-hastings-sampling"><i class="fa fa-check"></i><b>10.2.2</b> Metropolis-Hastings sampling</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="bayes.html"><a href="bayes.html#bayesian-linear-regression"><i class="fa fa-check"></i><b>10.3</b> Bayesian linear regression</a></li>
<li class="chapter" data-level="10.4" data-path="bayes.html"><a href="bayes.html#naive-bayes-classifier"><i class="fa fa-check"></i><b>10.4</b> Naive Bayes classifier</a></li>
<li class="chapter" data-level="10.5" data-path="bayes.html"><a href="bayes.html#BART"><i class="fa fa-check"></i><b>10.5</b> Bayesian additive trees</a><ul>
<li class="chapter" data-level="10.5.1" data-path="bayes.html"><a href="bayes.html#general-formulation"><i class="fa fa-check"></i><b>10.5.1</b> General formulation</a></li>
<li class="chapter" data-level="10.5.2" data-path="bayes.html"><a href="bayes.html#priors"><i class="fa fa-check"></i><b>10.5.2</b> Priors</a></li>
<li class="chapter" data-level="10.5.3" data-path="bayes.html"><a href="bayes.html#sampling-and-predictions"><i class="fa fa-check"></i><b>10.5.3</b> Sampling and predictions</a></li>
<li class="chapter" data-level="10.5.4" data-path="bayes.html"><a href="bayes.html#code"><i class="fa fa-check"></i><b>10.5.4</b> Code</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III From predictions to portfolios</b></span></li>
<li class="chapter" data-level="11" data-path="valtune.html"><a href="valtune.html"><i class="fa fa-check"></i><b>11</b> Validating and tuning</a><ul>
<li class="chapter" data-level="11.1" data-path="valtune.html"><a href="valtune.html#mlmetrics"><i class="fa fa-check"></i><b>11.1</b> Learning metrics</a><ul>
<li class="chapter" data-level="11.1.1" data-path="valtune.html"><a href="valtune.html#regression-analysis"><i class="fa fa-check"></i><b>11.1.1</b> Regression analysis</a></li>
<li class="chapter" data-level="11.1.2" data-path="valtune.html"><a href="valtune.html#classification-analysis"><i class="fa fa-check"></i><b>11.1.2</b> Classification analysis</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="valtune.html"><a href="valtune.html#validation"><i class="fa fa-check"></i><b>11.2</b> Validation</a><ul>
<li class="chapter" data-level="11.2.1" data-path="valtune.html"><a href="valtune.html#the-variance-bias-tradeoff-theory"><i class="fa fa-check"></i><b>11.2.1</b> The variance-bias tradeoff: theory</a></li>
<li class="chapter" data-level="11.2.2" data-path="valtune.html"><a href="valtune.html#the-variance-bias-tradeoff-illustration"><i class="fa fa-check"></i><b>11.2.2</b> The variance-bias tradeoff: illustration</a></li>
<li class="chapter" data-level="11.2.3" data-path="valtune.html"><a href="valtune.html#the-risk-of-overfitting-principle"><i class="fa fa-check"></i><b>11.2.3</b> The risk of overfitting: principle</a></li>
<li class="chapter" data-level="11.2.4" data-path="valtune.html"><a href="valtune.html#the-risk-of-overfitting-some-solutions"><i class="fa fa-check"></i><b>11.2.4</b> The risk of overfitting: some solutions</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="valtune.html"><a href="valtune.html#the-search-for-good-hyperparameters"><i class="fa fa-check"></i><b>11.3</b> The search for good hyperparameters</a><ul>
<li class="chapter" data-level="11.3.1" data-path="valtune.html"><a href="valtune.html#methods"><i class="fa fa-check"></i><b>11.3.1</b> Methods</a></li>
<li class="chapter" data-level="11.3.2" data-path="valtune.html"><a href="valtune.html#example-grid-search"><i class="fa fa-check"></i><b>11.3.2</b> Example: grid search</a></li>
<li class="chapter" data-level="11.3.3" data-path="valtune.html"><a href="valtune.html#example-bayesian-optimization"><i class="fa fa-check"></i><b>11.3.3</b> Example: Bayesian optimization</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="valtune.html"><a href="valtune.html#short-discussion-on-validation-in-backtests"><i class="fa fa-check"></i><b>11.4</b> Short discussion on validation in backtests</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ensemble.html"><a href="ensemble.html"><i class="fa fa-check"></i><b>12</b> Ensemble models</a><ul>
<li class="chapter" data-level="12.1" data-path="ensemble.html"><a href="ensemble.html#linear-ensembles"><i class="fa fa-check"></i><b>12.1</b> Linear ensembles</a><ul>
<li class="chapter" data-level="12.1.1" data-path="ensemble.html"><a href="ensemble.html#principles"><i class="fa fa-check"></i><b>12.1.1</b> Principles</a></li>
<li class="chapter" data-level="12.1.2" data-path="ensemble.html"><a href="ensemble.html#example"><i class="fa fa-check"></i><b>12.1.2</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="ensemble.html"><a href="ensemble.html#stacked-ensembles"><i class="fa fa-check"></i><b>12.2</b> Stacked ensembles</a><ul>
<li class="chapter" data-level="12.2.1" data-path="ensemble.html"><a href="ensemble.html#two-stage-training"><i class="fa fa-check"></i><b>12.2.1</b> Two stage training</a></li>
<li class="chapter" data-level="12.2.2" data-path="ensemble.html"><a href="ensemble.html#code-and-results-3"><i class="fa fa-check"></i><b>12.2.2</b> Code and results</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="ensemble.html"><a href="ensemble.html#extensions-1"><i class="fa fa-check"></i><b>12.3</b> Extensions</a><ul>
<li class="chapter" data-level="12.3.1" data-path="ensemble.html"><a href="ensemble.html#exogenous-variables"><i class="fa fa-check"></i><b>12.3.1</b> Exogenous variables</a></li>
<li class="chapter" data-level="12.3.2" data-path="ensemble.html"><a href="ensemble.html#shrinking-inter-model-correlations"><i class="fa fa-check"></i><b>12.3.2</b> Shrinking inter-model correlations</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="ensemble.html"><a href="ensemble.html#exercise"><i class="fa fa-check"></i><b>12.4</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="backtest.html"><a href="backtest.html"><i class="fa fa-check"></i><b>13</b> Portfolio backtesting</a><ul>
<li class="chapter" data-level="13.1" data-path="backtest.html"><a href="backtest.html#protocol"><i class="fa fa-check"></i><b>13.1</b> Setting the protocol</a></li>
<li class="chapter" data-level="13.2" data-path="backtest.html"><a href="backtest.html#turning-signals-into-portfolio-weights"><i class="fa fa-check"></i><b>13.2</b> Turning signals into portfolio weights</a></li>
<li class="chapter" data-level="13.3" data-path="backtest.html"><a href="backtest.html#perfmet"><i class="fa fa-check"></i><b>13.3</b> Performance metrics</a><ul>
<li class="chapter" data-level="13.3.1" data-path="backtest.html"><a href="backtest.html#discussion-1"><i class="fa fa-check"></i><b>13.3.1</b> Discussion</a></li>
<li class="chapter" data-level="13.3.2" data-path="backtest.html"><a href="backtest.html#pure-performance-and-risk-indicators"><i class="fa fa-check"></i><b>13.3.2</b> Pure performance and risk indicators</a></li>
<li class="chapter" data-level="13.3.3" data-path="backtest.html"><a href="backtest.html#factor-based-evaluation"><i class="fa fa-check"></i><b>13.3.3</b> Factor-based evaluation</a></li>
<li class="chapter" data-level="13.3.4" data-path="backtest.html"><a href="backtest.html#risk-adjusted-measures"><i class="fa fa-check"></i><b>13.3.4</b> Risk-adjusted measures</a></li>
<li class="chapter" data-level="13.3.5" data-path="backtest.html"><a href="backtest.html#transaction-costs-and-turnover"><i class="fa fa-check"></i><b>13.3.5</b> Transaction costs and turnover</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="backtest.html"><a href="backtest.html#common-errors-and-issues"><i class="fa fa-check"></i><b>13.4</b> Common errors and issues</a><ul>
<li class="chapter" data-level="13.4.1" data-path="backtest.html"><a href="backtest.html#forward-looking-data"><i class="fa fa-check"></i><b>13.4.1</b> Forward looking data</a></li>
<li class="chapter" data-level="13.4.2" data-path="backtest.html"><a href="backtest.html#backtest-overfitting"><i class="fa fa-check"></i><b>13.4.2</b> Backtest overfitting</a></li>
<li class="chapter" data-level="13.4.3" data-path="backtest.html"><a href="backtest.html#simple-safeguards"><i class="fa fa-check"></i><b>13.4.3</b> Simple safeguards</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="backtest.html"><a href="backtest.html#implication-of-non-stationarity-forecasting-is-hard"><i class="fa fa-check"></i><b>13.5</b> Implication of non-stationarity: forecasting is hard</a><ul>
<li class="chapter" data-level="13.5.1" data-path="backtest.html"><a href="backtest.html#general-comments"><i class="fa fa-check"></i><b>13.5.1</b> General comments</a></li>
<li class="chapter" data-level="13.5.2" data-path="backtest.html"><a href="backtest.html#the-no-free-lunch-theorem"><i class="fa fa-check"></i><b>13.5.2</b> The no free lunch theorem</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="backtest.html"><a href="backtest.html#example-1"><i class="fa fa-check"></i><b>13.6</b> Example</a></li>
<li class="chapter" data-level="13.7" data-path="backtest.html"><a href="backtest.html#coding-exercises-4"><i class="fa fa-check"></i><b>13.7</b> Coding exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Further important topics</b></span></li>
<li class="chapter" data-level="14" data-path="interp.html"><a href="interp.html"><i class="fa fa-check"></i><b>14</b> Interpretability</a><ul>
<li class="chapter" data-level="14.1" data-path="interp.html"><a href="interp.html#global-interpretations"><i class="fa fa-check"></i><b>14.1</b> Global interpretations</a><ul>
<li class="chapter" data-level="14.1.1" data-path="interp.html"><a href="interp.html#surr"><i class="fa fa-check"></i><b>14.1.1</b> Simple models as surrogates</a></li>
<li class="chapter" data-level="14.1.2" data-path="interp.html"><a href="interp.html#variable-importance"><i class="fa fa-check"></i><b>14.1.2</b> Variable importance (tree-based)</a></li>
<li class="chapter" data-level="14.1.3" data-path="interp.html"><a href="interp.html#variable-importance-agnostic"><i class="fa fa-check"></i><b>14.1.3</b> Variable importance (agnostic)</a></li>
<li class="chapter" data-level="14.1.4" data-path="interp.html"><a href="interp.html#partial-dependence-plot"><i class="fa fa-check"></i><b>14.1.4</b> Partial dependence plot</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="interp.html"><a href="interp.html#local-interpretations"><i class="fa fa-check"></i><b>14.2</b> Local interpretations</a><ul>
<li class="chapter" data-level="14.2.1" data-path="interp.html"><a href="interp.html#lime"><i class="fa fa-check"></i><b>14.2.1</b> LIME</a></li>
<li class="chapter" data-level="14.2.2" data-path="interp.html"><a href="interp.html#shapley-values"><i class="fa fa-check"></i><b>14.2.2</b> Shapley values</a></li>
<li class="chapter" data-level="14.2.3" data-path="interp.html"><a href="interp.html#breakdown"><i class="fa fa-check"></i><b>14.2.3</b> Breakdown</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="causality.html"><a href="causality.html"><i class="fa fa-check"></i><b>15</b> Two key concepts: causality and non-stationarity</a><ul>
<li class="chapter" data-level="15.1" data-path="causality.html"><a href="causality.html#causality-1"><i class="fa fa-check"></i><b>15.1</b> Causality</a><ul>
<li class="chapter" data-level="15.1.1" data-path="causality.html"><a href="causality.html#granger"><i class="fa fa-check"></i><b>15.1.1</b> Granger causality</a></li>
<li class="chapter" data-level="15.1.2" data-path="causality.html"><a href="causality.html#causal-additive-models"><i class="fa fa-check"></i><b>15.1.2</b> Causal additive models</a></li>
<li class="chapter" data-level="15.1.3" data-path="causality.html"><a href="causality.html#structural-time-series-models"><i class="fa fa-check"></i><b>15.1.3</b> Structural time-series models</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="causality.html"><a href="causality.html#nonstat"><i class="fa fa-check"></i><b>15.2</b> Dealing with changing environments</a><ul>
<li class="chapter" data-level="15.2.1" data-path="causality.html"><a href="causality.html#non-stationarity-yet-another-illustration"><i class="fa fa-check"></i><b>15.2.1</b> Non-stationarity: yet another illustration</a></li>
<li class="chapter" data-level="15.2.2" data-path="causality.html"><a href="causality.html#online-learning"><i class="fa fa-check"></i><b>15.2.2</b> Online learning</a></li>
<li class="chapter" data-level="15.2.3" data-path="causality.html"><a href="causality.html#homogeneous-transfer-learning"><i class="fa fa-check"></i><b>15.2.3</b> Homogeneous transfer learning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="unsup.html"><a href="unsup.html"><i class="fa fa-check"></i><b>16</b> Unsupervised learning</a><ul>
<li class="chapter" data-level="16.1" data-path="unsup.html"><a href="unsup.html#corpred"><i class="fa fa-check"></i><b>16.1</b> The problem with correlated predictors</a></li>
<li class="chapter" data-level="16.2" data-path="unsup.html"><a href="unsup.html#principal-component-analysis-and-autoencoders"><i class="fa fa-check"></i><b>16.2</b> Principal component analysis and autoencoders</a><ul>
<li class="chapter" data-level="16.2.1" data-path="unsup.html"><a href="unsup.html#a-bit-of-algebra"><i class="fa fa-check"></i><b>16.2.1</b> A bit of algebra</a></li>
<li class="chapter" data-level="16.2.2" data-path="unsup.html"><a href="unsup.html#pca"><i class="fa fa-check"></i><b>16.2.2</b> PCA</a></li>
<li class="chapter" data-level="16.2.3" data-path="unsup.html"><a href="unsup.html#ae"><i class="fa fa-check"></i><b>16.2.3</b> Autoencoders</a></li>
<li class="chapter" data-level="16.2.4" data-path="unsup.html"><a href="unsup.html#application"><i class="fa fa-check"></i><b>16.2.4</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="unsup.html"><a href="unsup.html#clustering-via-k-means"><i class="fa fa-check"></i><b>16.3</b> Clustering via k-means</a></li>
<li class="chapter" data-level="16.4" data-path="unsup.html"><a href="unsup.html#nearest-neighbors"><i class="fa fa-check"></i><b>16.4</b> Nearest neighbors</a></li>
<li class="chapter" data-level="16.5" data-path="unsup.html"><a href="unsup.html#coding-exercise-2"><i class="fa fa-check"></i><b>16.5</b> Coding exercise</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="RL.html"><a href="RL.html"><i class="fa fa-check"></i><b>17</b> Reinforcement learning</a><ul>
<li class="chapter" data-level="17.1" data-path="RL.html"><a href="RL.html#theoretical-layout"><i class="fa fa-check"></i><b>17.1</b> Theoretical layout</a><ul>
<li class="chapter" data-level="17.1.1" data-path="RL.html"><a href="RL.html#general-framework"><i class="fa fa-check"></i><b>17.1.1</b> General framework</a></li>
<li class="chapter" data-level="17.1.2" data-path="RL.html"><a href="RL.html#q-learning"><i class="fa fa-check"></i><b>17.1.2</b> Q-learning</a></li>
<li class="chapter" data-level="17.1.3" data-path="RL.html"><a href="RL.html#sarsa"><i class="fa fa-check"></i><b>17.1.3</b> SARSA</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="RL.html"><a href="RL.html#the-curse-of-dimensionality"><i class="fa fa-check"></i><b>17.2</b> The curse of dimensionality</a></li>
<li class="chapter" data-level="17.3" data-path="RL.html"><a href="RL.html#policy-gradient"><i class="fa fa-check"></i><b>17.3</b> Policy gradient</a><ul>
<li class="chapter" data-level="17.3.1" data-path="RL.html"><a href="RL.html#principle-2"><i class="fa fa-check"></i><b>17.3.1</b> Principle</a></li>
<li class="chapter" data-level="17.3.2" data-path="RL.html"><a href="RL.html#extensions-2"><i class="fa fa-check"></i><b>17.3.2</b> Extensions</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="RL.html"><a href="RL.html#simple-examples"><i class="fa fa-check"></i><b>17.4</b> Simple examples</a><ul>
<li class="chapter" data-level="17.4.1" data-path="RL.html"><a href="RL.html#q-learning-with-simulations"><i class="fa fa-check"></i><b>17.4.1</b> Q-learning with simulations</a></li>
<li class="chapter" data-level="17.4.2" data-path="RL.html"><a href="RL.html#RLemp2"><i class="fa fa-check"></i><b>17.4.2</b> Q-learning with market data</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="RL.html"><a href="RL.html#concluding-remarks"><i class="fa fa-check"></i><b>17.5</b> Concluding remarks</a></li>
<li class="chapter" data-level="17.6" data-path="RL.html"><a href="RL.html#exercises"><i class="fa fa-check"></i><b>17.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Appendix</b></span></li>
<li class="chapter" data-level="18" data-path="data-description.html"><a href="data-description.html"><i class="fa fa-check"></i><b>18</b> Data Description</a></li>
<li class="chapter" data-level="19" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html"><i class="fa fa-check"></i><b>19</b> Solution to exercises</a><ul>
<li class="chapter" data-level="19.1" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-4"><i class="fa fa-check"></i><b>19.1</b> Chapter 4</a></li>
<li class="chapter" data-level="19.2" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-5"><i class="fa fa-check"></i><b>19.2</b> Chapter 5</a></li>
<li class="chapter" data-level="19.3" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-6"><i class="fa fa-check"></i><b>19.3</b> Chapter 6</a></li>
<li class="chapter" data-level="19.4" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-7"><i class="fa fa-check"></i><b>19.4</b> Chapter 7</a></li>
<li class="chapter" data-level="19.5" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-8-the-autoencoder-model"><i class="fa fa-check"></i><b>19.5</b> Chapter 8: the autoencoder model</a></li>
<li class="chapter" data-level="19.6" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-9"><i class="fa fa-check"></i><b>19.6</b> Chapter 9</a></li>
<li class="chapter" data-level="19.7" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-12-ensemble-neural-network"><i class="fa fa-check"></i><b>19.7</b> Chapter 12: ensemble neural network</a></li>
<li class="chapter" data-level="19.8" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-13"><i class="fa fa-check"></i><b>19.8</b> Chapter 13</a><ul>
<li class="chapter" data-level="19.8.1" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#ew-portfolios-with-the-tidyverse"><i class="fa fa-check"></i><b>19.8.1</b> EW portfolios with the tidyverse</a></li>
<li class="chapter" data-level="19.8.2" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#advanced-weighting-function"><i class="fa fa-check"></i><b>19.8.2</b> Advanced weighting function</a></li>
<li class="chapter" data-level="19.8.3" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#functional-programming-in-the-backtest"><i class="fa fa-check"></i><b>19.8.3</b> Functional programming in the backtest</a></li>
</ul></li>
<li class="chapter" data-level="19.9" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-16"><i class="fa fa-check"></i><b>19.9</b> Chapter 16</a></li>
<li class="chapter" data-level="19.10" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-17"><i class="fa fa-check"></i><b>19.10</b> Chapter 17</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning for Factor Investing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="RL" class="section level1">
<h1><span class="header-section-number">Chapter 17</span> Reinforcement learning</h1>
<p>Due to its increasing popularity within the Machine Learning community, we dedicate a chapter to reinforcement learning (RL). In 2019 only, more than 25 papers dedicated to RL have been submitted to (or updated on) arXiv under the <strong>q:fin</strong> (quantitative finance) classification. Moreover, an early survey of RL-based portfolios is compiled in <span class="citation">Sato (<a href="#ref-sato2019model">2019</a>)</span> and general financial applications are discussed in <span class="citation">Kolm and Ritter (<a href="#ref-kolm2019modern">2019</a><a href="#ref-kolm2019modern">b</a>)</span>, <span class="citation">Meng and Khushi (<a href="#ref-meng2019reinforcement">2019</a>)</span>, <span class="citation">Charpentier, Elie, and Remlinger (<a href="#ref-charpentier2020reinforcement">2020</a>)</span> and <span class="citation">Mosavi et al. (<a href="#ref-mosavi2020comprehensive">2020</a>)</span>. This shows that RL has recently gained traction among the quantitative finance community.<a href="#fn31" class="footnote-ref" id="fnref31"><sup>31</sup></a></p>
<p>While RL is a framework much more than a particular algorithm, its efficient application in portfolio management is not straightforward, as we will show.</p>
<div id="theoretical-layout" class="section level2">
<h2><span class="header-section-number">17.1</span> Theoretical layout</h2>
<div id="general-framework" class="section level3">
<h3><span class="header-section-number">17.1.1</span> General framework</h3>
<p>In this section, we introduce the core concepts of RL and follow relatively closely the notations (and layout) of
<span class="citation">Sutton and Barto (<a href="#ref-sutton2018reinforcement">2018</a>)</span>, which is widely considered as a solid reference in the field, along with <span class="citation">Bertsekas (<a href="#ref-bertsekas2017dynamic">2017</a>)</span>. One central tool in the field is called the <strong>Markov Decision Process</strong> (MDP, see Chapter 3 in <span class="citation">Sutton and Barto (<a href="#ref-sutton2018reinforcement">2018</a>)</span>).</p>
<p>MDPs, like all RL frameworks, involve the interaction between an <strong>agent</strong> (e.g., a trader or portfolio manager) and an <strong>environment</strong> (e.g., a financial market). The agent performs <strong>actions</strong> that may alter the state of environment and gets a reward (possibly negative) for each action. This short sequence can be repeated an arbitrary number of times, as is shown in Figure <a href="RL.html#fig:mdpscheme">17.1</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:mdpscheme"></span>
<img src="images/MDP_scheme.png" alt="Scheme of Markov Decision Process. R, S and A stand for reward, state and action, respectively." width="500px" />
<p class="caption">
FIGURE 17.1: Scheme of Markov Decision Process. R, S and A stand for reward, state and action, respectively.
</p>
</div>
<p>Given initialized values for the state of the environement (<span class="math inline">\(S_0\)</span>) and reward (usually <span class="math inline">\(R_0=0\)</span>), the agent performs an action (e.g., invests in some assets). This generates a reward <span class="math inline">\(R_1\)</span> (e.g., returns, profits, Sharpe ratio) and also a future state of the environment (<span class="math inline">\(S_1\)</span>). Based on that, the agent performs a new action and the sequence continues. When the sets of states, actions and rewards are finite, the MDP is logically called <em>finite</em>. In a financial framework, this is somewhat unrealistic and we discuss this issue later on. It nevertheless is not hard to think of simplified and discretized financial problems. For instance, the reward can be binary: win money versus lose money. In the case of only one asset, the action can also be dual: investing versus not investing. When the number of assets is sufficiently small, it is possible to set fixed proportions that lead to a reasonable number of combinations of portfolio choices, etc.</p>
<p>We pursue our expos with finite MDPs:they are the most common in the literature and their formal treatment is simpler. The relative simplicity of MDPs helps grasp the concept that are common to other RL techniques. As is often the case with markovian objects, the key notion is that of <strong>transition probability</strong>:</p>
<p><span class="math display" id="eq:transprob">\[\begin{equation}
\tag{17.1}
p(s&#39;,r|s,a)=\mathbb{P}\left[S_t=s&#39;,R_t=r | S_{t-1}=s,A_{t-1}=a \right],
\end{equation}\]</span></p>
<p>which is the probability of reaching state <span class="math inline">\(s&#39;\)</span> and reward <span class="math inline">\(r\)</span> at time <span class="math inline">\(t\)</span>, conditionally on being in state <span class="math inline">\(s\)</span> and performing action <span class="math inline">\(a\)</span> at time <span class="math inline">\(t-1\)</span>. The finite sets of states and actions will be denoted with <span class="math inline">\(\mathcal{S}\)</span> and <span class="math inline">\(\mathcal{A}\)</span> henceforth.
Sometimes, this probability is averaged over the set of rewards which gives the following decomposition:
<span class="math display" id="eq:transprob2\tag{17.2}}  (#eq:transprob2)
\sum_r rp(s&#39;,r|s,a)&amp;=\mathcal{P}_{ss&#39;}^a \mathcal{R}_{ss&#39;}^a, \quad \text{ where } \\
\mathcal{P}_{ss&#39;}^a &amp;=\mathbb{P}\left[S_t=s&#39; | S_{t-1}=s,A_{t-1}=a \right],  \quad \text{ and } \nonumber \\
 \mathcal{R}_{ss&#39;}^a &amp;= \mathbb{E}\left[R_t | S_{t-1}=s,S_t=s&#39;, A_{t-1}=a \right]. \nonumber
\end{align}\]</span></p>
<p>The goal of the agent is to maximize some function of the stream of rewards. This gain is usually defined as
<span class="math display" id="eq:gain6">\[\begin{align}
G_t&amp;=\sum_{k=0}^T\gamma^kR_{t+k+1} \nonumber \\   \tag{17.3}
&amp;=R_{t+1} +\gamma G_{t+1},
\end{align}\]</span></p>
<p>i.e., it is a discounted version of the reward, where the discount factor is <span class="math inline">\(\gamma \in (0,1]\)</span>. The horizon <span class="math inline">\(T\)</span> may be infinite, which is why <span class="math inline">\(\gamma\)</span> was originally introduced. Assuming the rewards are bounded, the infinite sum may diverge for <span class="math inline">\(\gamma=1\)</span>. That is the case if rewards dont decrease with time and there is no reason why they should.
When <span class="math inline">\(\gamma &lt;1\)</span> and rewards are bounded, convergence is assured. When <span class="math inline">\(T\)</span> is finite, the task is called <em>episodic</em> and otherwise, it is said to be <em>continuous</em>.</p>
<p>In RL, the focal unknown to be optimized or learned is the <strong>policy</strong> <span class="math inline">\(\pi\)</span>, which drives the actions of the agent. More precisely, <span class="math inline">\(\pi(a,s)=\mathbb{P}[A_t=a|S_t=s]\)</span>, that is, <span class="math inline">\(\pi\)</span> equals the probability of taking action <span class="math inline">\(a\)</span> if the state of the environment is <span class="math inline">\(s\)</span>. This means that actions are subject to randomness, just like for mixed strategies in game theory. While this may seem disappointing because an investor would want to be sure to take <em>the</em> best action, but it is also a good reminder that the best way to face random outcomes may well be to randomize actions as well.</p>
<p>Finally, in order to try to determine the <em>best</em> policy, one key indicator is the so-called value function:
<span class="math display" id="eq:RLvalue">\[\begin{equation}
\tag{17.4}
v_\pi(s)=\mathbb{E}_\pi\left[ G_t | S_t=s \right],
\end{equation}\]</span></p>
<p>where the time index <span class="math inline">\(t\)</span> is not very relevant and omitted in the notation of the function. The index <span class="math inline">\(\pi\)</span> under the expectation operator <span class="math inline">\(\mathbb{E}[\cdot]\)</span> simply indicates that the average is taken when the policy <span class="math inline">\(\pi\)</span> is enforced. The value function is simply equal to the average gain conditionally on the state being equal to <span class="math inline">\(s\)</span>. In financial terms, this is equivalent to the average profit if the agents takes actions driven by <span class="math inline">\(\pi\)</span> when the market environment is <span class="math inline">\(s\)</span>. More generally, it is also possible to condition not only on the state, but also on the action taken. We thus introduce the <span class="math inline">\(q_\pi\)</span> action-value function:
<span class="math display" id="eq:RLQ">\[\begin{equation}
\tag{17.5}
q_\pi(s,a)=\mathbb{E}_\pi\left[ G_t | S_t=s, \ A_t=a \right].
\end{equation}\]</span></p>
<p>The <span class="math inline">\(q_\pi\)</span> function is highly important because it gives the average gain when the state and action are fixed. Hence, if the current state is known, then one obvious choice is to select the action for which <span class="math inline">\(q_\pi(s,\cdot)\)</span> is the highest. Of course, this is the best solution if the optimal value of <span class="math inline">\(q_\pi\)</span> is known, which is not always the case in practice. The value function can easily be accessed via <span class="math inline">\(q_\pi\)</span>: <span class="math inline">\(v_\pi(s)=\sum_a \pi(a,s)q_\pi(s,a)\)</span>.</p>
<p>The optimal <span class="math inline">\(v_\pi\)</span> and <span class="math inline">\(q_\pi\)</span> are straightforwardly defined as
<span class="math display">\[v_*(s)=\underset{\pi}{\max} \, v_\pi(s), \ \forall s\in \mathcal{S}, \quad \text{ and } \quad q_*(s,a) =\underset{\pi}{\max} \, q_\pi(s,a), \ \forall (s,a)\in \mathcal{S}\times \mathcal{A}.\]</span></p>
<p>If only <span class="math inline">\(v_*(s)\)</span> is known, then the agent must span the set of actions and find those that yield the maximum value for any given state <span class="math inline">\(s\)</span>.</p>
<p>Finding these optimal values is a very complicated task and many articles are dedicated to solving this challenge. One reason why finding the best <span class="math inline">\(q_\pi(s,a)\)</span> is difficult is because it depends on two elements (<span class="math inline">\(s\)</span> and <span class="math inline">\(a\)</span>) on one side and <span class="math inline">\(\pi\)</span> on the other. Usually, for a fixed policy <span class="math inline">\(\pi\)</span>, it can be time consuming to evaluate <span class="math inline">\(q_\pi(s,a)\)</span> for a given stream of actions, states and rewards. Once <span class="math inline">\(q_\pi(s,a)\)</span> is estimated, then a new policy <span class="math inline">\(\pi&#39;\)</span> must be tested and evaluated to determine if it is better than the original one.
Thus, this iterative search for a good policy can take long. For more details on policy improvement and value function updating, we recommend Chapter 4 of <span class="citation">Sutton and Barto (<a href="#ref-sutton2018reinforcement">2018</a>)</span> which is dedicated to dynamic programming.</p>
</div>
<div id="q-learning" class="section level3">
<h3><span class="header-section-number">17.1.2</span> Q-learning</h3>
<p>An interesting shortcut to the problem of finding <span class="math inline">\(v_*(s)\)</span> and <span class="math inline">\(q_*(s,a)\)</span> is to remove the dependence on the policy. Consequently, there is then of course no need to iteratively improve it. The central relationship that is required to do this is the so-called Bellman equation that is satisfies by <span class="math inline">\(q_\pi(s,a)\)</span>. We detail its derivation below. First of all, we recall that
<span class="math display">\[\begin{align*}
q_\pi(s,a) &amp;= \mathbb{E}_\pi[G_t|S_t=s,A_t=a] \\
&amp;= \mathbb{E}_\pi[R_{t+1}+ \gamma G_{t+1}|S_t=s,A_t=a],
\end{align*}\]</span>
where the second equality stems from <a href="RL.html#eq:gain6">(17.3)</a>. The expression <span class="math inline">\(\mathbb{E}_\pi[R_{t+1}|S_t=s,A_t=a]\)</span> can be further decomposed. Since the expectation runs over <span class="math inline">\(\pi\)</span>, we need to sum over all possible actions <span class="math inline">\(a&#39;\)</span> and states <span class="math inline">\(s&#39;\)</span> and resort to <span class="math inline">\(\pi(a&#39;,s&#39;)\)</span>. In addition, the sum on the <span class="math inline">\(s&#39;\)</span> and <span class="math inline">\(r\)</span> arguments of the probability <span class="math inline">\(p(s&#39;,r|s,a)=\mathbb{P}\left[S_{t+1}=s&#39;,R_{t+1}=r | S_t=s,A_t=a \right]\)</span> gives access to the distribution of the random couple <span class="math inline">\((S_{t+1},R_{t+1})\)</span> so that in the end <span class="math inline">\(\mathbb{E}_\pi[R_{t+1}|S_t=s,A_t=a]=\sum_{a&#39;, r,s&#39;}\pi(a&#39;,s&#39;)p(s&#39;,r|s,a) r\)</span>. A similar reasoning applies to the second portion of <span class="math inline">\(q_\pi\)</span> and:
<span class="math display" id="eq:bellman">\[\begin{align}
q_\pi(s,a) &amp;=\sum_{a&#39;,r, s&#39;}\pi(a&#39;,s&#39;)p(s&#39;,r|s,a) \left[ r+\gamma \mathbb{E}_\pi[ G_{t+1}|S_t=s&#39;,A_t=a&#39;]\right] \nonumber \\  \tag{17.6}
&amp;=\sum_{a&#39;,r,s&#39;}\pi(a&#39;,s&#39;)p(s&#39;,r|s,a) \left[ r+\gamma q_\pi(s&#39;,a&#39;)\right].
\end{align}\]</span></p>
<p>This equation links <span class="math inline">\(q_\pi(s,a)\)</span> to the future <span class="math inline">\(q_\pi(s&#39;,a&#39;)\)</span> from the states and actions <span class="math inline">\((s&#39;,a&#39;)\)</span> that are accessible from <span class="math inline">\((s,a)\)</span>.</p>
<p>Notably, Equation <a href="RL.html#eq:bellman">(17.6)</a> is also true for the optimal action-value function <span class="math inline">\(q_*=\underset{\pi}{\max} \, q_\pi(s,a)\)</span>:</p>
<p><span class="math display" id="eq:bellmanq">\[\begin{align}
q_*(s,a) &amp;=\underset{a&#39;}{\max} \sum_{r,s&#39;}p(s&#39;,r|s,a) \left[ r+\gamma q_*(s&#39;,a&#39;)\right], \nonumber \\ \tag{17.7}
&amp;= \mathbb{E}_{\pi^*}[r|s,a]+ \gamma \, \sum_{r,s&#39;}p(s&#39;,r|s,a)\left(  \underset{a&#39;}{\max} q_*(s&#39;,a&#39;) \right) 
\end{align}\]</span></p>
<p>because one optimal policy is one that maximizes <span class="math inline">\(q_\pi(s,a)\)</span>, for a given state <span class="math inline">\(s\)</span> and over all possible actions <span class="math inline">\(a\)</span>. This expression is central to a cornerstone algorithm in reinforcement learning called <span class="math inline">\(Q\)</span>-learning (the formal proof of convergence is outlined in <span class="citation">Watkins and Dayan (<a href="#ref-watkins1992q">1992</a>)</span>). In <span class="math inline">\(Q\)</span>-learning, the state-action function does no longer depend on policy and is written with capital <span class="math inline">\(Q\)</span>. The process is the following:</p>
<p>Initialize values <span class="math inline">\(Q(s,a)\)</span> for all states <span class="math inline">\(s\)</span> and actions <span class="math inline">\(a\)</span>. For each episode:<br />
<span class="math display">\[ (\textbf{QL}) \quad \left\{
\begin{array}{l}
\text{0. Initialize state } S_0 \text{ and for each iteration } i \text{ until the end of the episode:}   \\
\text{1. observe state } s_i;    \\
\text{2. perform action } a_i \text{(depending on } Q);   \\
\text{3. receive reward }r_{i+1} \text{ and observe state } s_{i+1};  \\
\text{4. Update } Q \text{ as follows: }
\end{array} \right.\]</span></p>
<p><span class="math display" id="eq:QLupdate">\[\begin{equation}
\tag{17.8}
Q_{i+1}(s_i,a_i) \longleftarrow Q_i(s_i,a_i) + \eta  \left(\underbrace{r_{i+1}+\gamma \, \underset{a}{\max} \, Q_i(s_{i+1},a)}_{\text{echo of } (\ref{eq:bellmanq})}-Q_i(s_i,a_i) \right)
\end{equation}\]</span></p>
<p>The underlying reason why this update rule works can be linked to fixed point theorems of contraction mappings. If a function <span class="math inline">\(f\)</span> satisfies <span class="math inline">\(|f(x)-f(y)|&lt; \delta |x-y|\)</span> (Lipshitz continuity), then a fixed point <span class="math inline">\(z\)</span> satisfying <span class="math inline">\(f(z)=z\)</span> can be iteratively obtained via <span class="math inline">\(z \leftarrow f(z)\)</span>. This updating rule converges to the fixed point. Equation <a href="RL.html#eq:bellmanq">(17.7)</a> can be solved using a similar principle, except that a learning rate <span class="math inline">\(\eta\)</span> slows the learning process but also technically ensures convergence under technical assumptions.</p>
<p>More generally, <a href="RL.html#eq:QLupdate">(17.8)</a> has a form that is widespread in reinforcement learning that is summarized in Equation (2.4) of <span class="citation">Sutton and Barto (<a href="#ref-sutton2018reinforcement">2018</a>)</span>:
<span class="math display" id="eq:RLeq">\[\begin{equation}
\tag{17.9}
\text{New estimate} \leftarrow \text{Old estimate + Step size (}i.e., \text{ learning rate)} \times (\text{Target - Old estimate}),
\end{equation}\]</span></p>
<p>where the last part can be viewed as an error term. Starting from the old estimate, the new estimate therefore goes in the right (or sought) direction, modulo a discount term that makes sure that the magnitude of this direction is not too large. The update rule in <a href="RL.html#eq:QLupdate">(17.8)</a> is often referred to as <em>temporal difference</em> learning because it is driven by the improvement yielded by estimates that are known at time <span class="math inline">\(t+1\)</span> (target) versus those known at time <span class="math inline">\(t\)</span>.</p>
<p>One important step of the <em>Q</em>-learning sequence (<strong>QL</strong>) is the second one where the action <span class="math inline">\(a_i\)</span> is picked. In RL, the best algorithms combine two features: <strong>exploitation</strong> and <strong>exploration</strong>. Exploitation is when the machine uses the currrent information at its disposal to choose the next action. In this case, for a given state <span class="math inline">\(s_i\)</span>, it chooses the action <span class="math inline">\(a_i\)</span> that maximizes the expected reward <span class="math inline">\(Q_i(s_i,a_i)\)</span>. While obvious, this choice is not optimal if the current function <span class="math inline">\(Q_i\)</span> is relatively far from the <em>true</em> <span class="math inline">\(Q\)</span>. Repeating the locally optimal strategy is likely to favor a limited number of actions: which will narrowly improve the accuracy of the <span class="math inline">\(Q\)</span> function</p>
<p>In order to gather new information stemming from actions that have not been tested much (but that can potentially generate higher rewards), exploration is needed. This is when an action <span class="math inline">\(a_i\)</span> is chosen randomly. The most common way to combine these two concepts is called <span class="math inline">\(\epsilon\)</span>-greedy exploration. The action <span class="math inline">\(a_i\)</span> is assigned according to:</p>
<p><span class="math display" id="eq:egreedy">\[\begin{equation}
\tag{17.10}
a_i=\left\{ \begin{array}{c l}
\underset{a}{\text{argmax}} \ Q_i(s_i,a) &amp; \text{ with probability } 1-\epsilon \\
\text{randomly (uniformly) over } \mathcal{A} &amp; \text{ with probability } \epsilon
\end{array}\right. .
\end{equation}\]</span></p>
<p>Thus, with probability <span class="math inline">\(\epsilon\)</span>, the algorithm explores and with probability <span class="math inline">\(1-\epsilon\)</span>, it exploits the current knowledge of the expected reward and picks the best action. Because all actions have a non-zero probability of being chosen, the policy is called soft. Indeed, then best action has a probability of selection equal to <span class="math inline">\(1-\epsilon(1-\text{card}(\mathcal{A})^{-1})\)</span> while all other actions are picked with probability <span class="math inline">\(\epsilon/\text{card}(\mathcal{A})\)</span>.</p>
</div>
<div id="sarsa" class="section level3">
<h3><span class="header-section-number">17.1.3</span> SARSA</h3>
<p>In <span class="math inline">\(Q\)</span>-learning, the algorithm seeks to find the action-value function of the optimal policy. Thus, the policy that is followed to pick actions is different from the one that is learned (via <span class="math inline">\(Q\)</span>). Such algorithms are called <em>off-policy</em>. <em>On-policy</em> algorithms seek to improve the estmation of the action-value function <span class="math inline">\(q_\pi\)</span> by continuously acting according to the policy <span class="math inline">\(\pi\)</span>. One canonical example of on-policy learning is the SARSA method which requires two consecutive states and actions <strong>SA</strong>R<strong>SA</strong>. The way the quintuple <span class="math inline">\((S_t,A_t,R_{t+1}, S_{t+1}, A_{t+1})\)</span> is processed is presented below.</p>
<p>The main difference between <span class="math inline">\(Q\)</span> learning and SARSA is the update rule. In SARSA, it is given by
<span class="math display" id="eq:SARSAupdate">\[\begin{equation}
\tag{17.11}
Q_{i+1}(s_i,a_i) \longleftarrow Q_i(s_i,a_i) + \eta  \left(r_{i+1}+\gamma \, Q_i(s_{i+1},a_{i+1})-Q_i(s_i,a_i) \right)
\end{equation}\]</span></p>
<p>The improvement comes only from the <strong>local</strong> point <span class="math inline">\(Q_i(s_{i+1},a_{i+1})\)</span> that is based on the new states and actions (<span class="math inline">\(s_{i+1},a_{i+1}\)</span>), whereas in <span class="math inline">\(Q\)</span>-learning, it comes from all possible actions of which only the best is retained <span class="math inline">\(\underset{a}{\max} \, Q_i(s_{i+1},a)\)</span>.</p>
<p>A more robust but also more computationally demanding version of SARSA is <em>expected</em> SARSA in which the target <span class="math inline">\(Q\)</span> function is averaged over all actions:
<span class="math display" id="eq:exSARSAupdate">\[\begin{equation}
\tag{17.12}
Q_{i+1}(s_i,a_i) \longleftarrow Q_i(s_i,a_i) + \eta  \left(r_{i+1}+\gamma \, \sum_a \pi(a,s_{i+1}) Q_i(s_{i+1},a) -Q_i(s_i,a_i) \right)
\end{equation}\]</span></p>
<p>Expected SARSA is less volatile than SARSA because the latter is strongly impacted by the random choice of <span class="math inline">\(a_{i+1}\)</span>. In expected SARSA, the average smoothes the learning process.</p>
</div>
</div>
<div id="the-curse-of-dimensionality" class="section level2">
<h2><span class="header-section-number">17.2</span> The curse of dimensionality</h2>
<p>Let us first recall that reinforcement learning is a framework that is not linked to a particular algorithm. In fact, different tools can very well co-exist in a RL task (AlphaGo combined both tree methods and neural networks, see <span class="citation">Silver et al. (<a href="#ref-silver2016mastering">2016</a>)</span>). Nonetheless, any RL attempt will always rely on the three key concepts that are: the states, the actions and the rewards. In factor investing, they fairly easy to identify, though there is always room for interpretation. Actions are evidently defined by portfolio compositions. The states can be viewed as the current values that describe the economy: as a first order approximation, it can be assumed that the feature levels fulfill this role (possibly conditioned or complemented with macro-economic data). The rewards are even more straightforward. Returns or any relevant performance metric<a href="#fn32" class="footnote-ref" id="fnref32"><sup>32</sup></a> can account for rewards.</p>
<p>A major problem lies in the dimensionality of both states and actions. Assuming an absence of leverage (no negative weights), the actions take values on the simplex
<span class="math display" id="eq:simplex">\[\begin{equation}
\tag{17.13}
\mathbb{S}_N=\left\{ \mathbf{x} \in \mathbb{R}^N\left|\sum_{n=1}^Nx_n=1, \ x_n\ge 0, \ \forall n=1,\dots,N \right.\right\}
\end{equation}\]</span>
and assuming that all features have been uniformized, their space is <span class="math inline">\([0,1]^{NK}\)</span>. Needless to say, the dimensions of both spaces are numerically impractical.</p>
<p>A simple solution to this problem is discretization: each space is divided into a small number of categories. Some authors do take this route. In <span class="citation">Yang, Yu, and Almahdi (<a href="#ref-yang2018investor">2018</a>)</span>, the state space is discretized into three values depending on volatility, and actions are also split into three categories. <span class="citation">Bertoluzzo and Corazza (<a href="#ref-bertoluzzo2012testing">2012</a>)</span> and <span class="citation">Xiong et al. (<a href="#ref-xiong2018practical">2018</a>)</span> also choose three possible actions (buy, hold, sell). In <span class="citation">Almahdi and Yang (<a href="#ref-almahdi2019constrained">2019</a>)</span>, the learner is expected to yield binary signals for buying or shorting. <span class="citation">Garc'a-Galicia, Carsteanu, and Clempner (<a href="#ref-garcia2019continuous">2019</a>)</span> consider a larger state space (8 elements) but restrict the action set to 3 options.<a href="#fn33" class="footnote-ref" id="fnref33"><sup>33</sup></a> In terms of the state space, all articles assume that the state of the economy is determined by prices (or returns).</p>
<p>One strong limitation of these approaches is the marked simplification they imply. Realistic discretizations are numerically intractable when investing in multiple assets. Indeed, splitting the unit interval in <span class="math inline">\(h\)</span> points yields <span class="math inline">\(h^{NK}\)</span> possibilities for feature values. The number of options for weight combinations is exponentially increasing <span class="math inline">\(N\)</span>. As an example: just 10 possible values for 10 features of 10 stocks yield <span class="math inline">\(10^{100}\)</span> permutations.</p>
<p>The problems mentioned above are of course not restricted to portfolio construction. Many solutions have been proposed to solve Markov Decision Processes in continuous spaces. We refer for instance to Section 4 in <span class="citation">Powell and Ma (<a href="#ref-powell2011review">2011</a>)</span> for a review of early methods (outside finance).</p>
<p>This curse of dimensionality is accompanied by fundamental question of training data. Two options are conceivable: market data versus simulations. Under a given controlled generator of samples, it is hard to imagine that the algorithm will beat the solution that maximizes a given utility function. If anything, it should converge towards the static optimal solution under a stationary data generating process (see, e.g. <span class="citation">Chaouki et al. (<a href="#ref-chaouki2020deep">2020</a>)</span> for trading tasks), which is by the way a very strong modelling assumption.</p>
<p>This leaves market data as a preferred solution but even with large datasets, there is little chance to cover all the (actions, states) combinations mentioned above. Characteristics-based datasets have depths that run through a few decades of monthly data, which means several hundreds of time-stamps at most. This is by far too limited to allow for a reliable learning process. It is always possible to generate synthetic data (as in <span class="citation">Yu et al. (<a href="#ref-yu2019model">2019</a>)</span>), but it is unclear that this will solidly improve the performance of the algorithm.</p>
</div>
<div id="policy-gradient" class="section level2">
<h2><span class="header-section-number">17.3</span> Policy gradient</h2>
<div id="principle-2" class="section level3">
<h3><span class="header-section-number">17.3.1</span> Principle</h3>
<p>Beyond the discretization of action and state spaces, a powerful trick is <strong>parametrization</strong>. When <span class="math inline">\(a\)</span> and <span class="math inline">\(s\)</span> can take discrete values, action-value functions must be computed for all pairs <span class="math inline">\((a,s)\)</span>, which can be prohibitively cumbersome. An elegant way to circumvent this problem is to assume that the policy is driven by a relatively modest number of parameters. The learning process is then focused on optimizing this set of parameters <span class="math inline">\(\boldsymbol{\theta}\)</span>. We then write <span class="math inline">\(\pi_{\boldsymbol{\theta}}(a,s)\)</span> for the probability of choosing action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span>. One intuitive way to define <span class="math inline">\(\pi_{\boldsymbol{\theta}}(a,s)\)</span> is to resort to a soft-max form:
<span class="math display" id="eq:policyex">\[\begin{equation}
\tag{17.14}
\pi_{\boldsymbol{\theta}}(a,s) = \frac{e^{\boldsymbol{\theta}&#39;\textbf{h}(a,s)}}{\sum_{b}e^{\boldsymbol{\theta}&#39;\textbf{h}(b,s)}},
\end{equation}\]</span>
where the output of function <span class="math inline">\(\textbf{h}(a,s)\)</span>, which has the same dimension as <span class="math inline">\(\boldsymbol{\theta}\)</span> is called a feature vector representing the pair <span class="math inline">\((a,s)\)</span>. Typically, <span class="math inline">\(\textbf{h}\)</span> can very well be a simple neural network with two input units and an output dimension equal to the length of <span class="math inline">\(\boldsymbol{\theta}\)</span>.</p>
<p>One desired property for <span class="math inline">\(\pi_{\boldsymbol{\theta}}\)</span> is that it be differentiable with respect to <span class="math inline">\(\boldsymbol{\theta}\)</span> so that <span class="math inline">\(\boldsymbol{\theta}\)</span> can be improved via some gradient method. The most simple and intuitive results about policy gradients are known in the case of episodic tasks (finite horizon) for which it is sought to maximize the average gain <span class="math inline">\(\mathbb{E}_{\boldsymbol{\theta}}[G_t]\)</span> where the gain is defined in Equation <a href="RL.html#eq:gain6">(17.3)</a>. The expectation is computed according to a particular policy that depends on <span class="math inline">\(\boldsymbol{\theta}\)</span>, this is why we use a simple subscript. One central result is the so-called policy gradient theorem which states that</p>
<p><span class="math display" id="eq:PGT">\[\begin{equation}
\tag{17.15}
\nabla \mathbb{E}_{\boldsymbol{\theta}}[G_t]=\mathbb{E}_{\boldsymbol{\theta}} \left[G_t\frac{\nabla \pi_{\boldsymbol{\theta}}}{\pi_{\boldsymbol{\theta}}} \right].
\end{equation}\]</span></p>
<p>This result can then be used for <strong>gradient ascent</strong>: when seeking to maximize a quantity, the parameter change must go in the upward direction:</p>
<p><span class="math display" id="eq:ascent">\[\begin{equation}
\tag{17.16}
\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} + \eta \nabla \mathbb{E}_{\boldsymbol{\theta}}[G_t].
\end{equation}\]</span></p>
<p>This simple update rule is known as the <strong>REINFORCE</strong> algorithm. One improvement of this simple idea is to add a baseline and we refer to Section 13.4 of <span class="citation">Sutton and Barto (<a href="#ref-sutton2018reinforcement">2018</a>)</span> for a detailed account on this topic.</p>
</div>
<div id="extensions-2" class="section level3">
<h3><span class="header-section-number">17.3.2</span> Extensions</h3>
<p>A popular extension of REINFORCE is the so-called <strong>actor-critic</strong> (AC) method which combines policy gradient with <span class="math inline">\(Q\)</span>- or <span class="math inline">\(v\)</span>- learning. The AC algorithm can be viewed as some kind of mix between policy gradient and SARSA. A central requirement is that the state-value function <span class="math inline">\(v(\cdot)\)</span> be a differentiable function of some parameter vector <span class="math inline">\(\textbf{w}\)</span> (it is often taken to be a neural network). The update rule is then</p>
<p><span class="math display" id="eq:ascentAC">\[\begin{equation}
\tag{17.17}
\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} + \eta \left(R_{t+1}+\gamma v(S_{t+1},\textbf{w})-v(S_t,\textbf{w}) \right)\frac{\nabla \pi_{\boldsymbol{\theta}}}{\pi_{\boldsymbol{\theta}}},
\end{equation}\]</span>
but the trick is that the vector <span class="math inline">\(\textbf{w}\)</span> must also be updated. The actor is the policy side which is what drives decision making. The critic side is the value function that evaluates the actors performance. As learning progresses (each time both sets of parameters are updated), both sides improve. The exact algorithmic formulation is a bit long and we refer to Section 13.5 in <span class="citation">Sutton and Barto (<a href="#ref-sutton2018reinforcement">2018</a>)</span> for the precise sequence of steps of AC.</p>
<p>Another interesting application of parametric policies is outlined in <span class="citation">Aboussalah and Lee (<a href="#ref-aboussalah2020continuous">2020</a>)</span>. In their article, the authors define a trading policy that is based on a recurrent neural network. Thus, the parameter <span class="math inline">\(\boldsymbol{\theta}\)</span> in this case encompasses all weights and biases in the network.</p>
<p>Another favorable feature of parametric policies is that they are compatible with continuous sets of actions. Beyond the form <a href="RL.html#eq:policyex">(17.14)</a>, there are other ways to shape <span class="math inline">\(\pi_{\boldsymbol{\theta}}\)</span>. If <span class="math inline">\(\mathcal{A}\)</span> is a subset of <span class="math inline">\(\mathbb{R}\)</span>, and <span class="math inline">\(f_{\boldsymbol{\Omega}}\)</span> is a density function with parameters <span class="math inline">\(\boldsymbol{\Omega}\)</span>, then a candidate form for <span class="math inline">\(\pi_{\boldsymbol{\theta}}\)</span> is</p>
<p><span class="math display" id="eq:parpol">\[\begin{equation}
\tag{17.18}
\pi_{\boldsymbol{\theta}} = f_{\boldsymbol{\Omega}(s,\boldsymbol{\theta})}(a),
\end{equation}\]</span>
in which the parameters <span class="math inline">\(\boldsymbol{\Omega}\)</span> are in turn functions of the states and of the underlying (second order) parameters <span class="math inline">\(\boldsymbol{\theta}\)</span>.</p>
<p>While the Gaussian distribution (see Section 13.7 in <span class="citation">Sutton and Barto (<a href="#ref-sutton2018reinforcement">2018</a>)</span>) are often a preferred choice, they would require some processing to lie inside the unit interval. One easy way to obtain such values is to apply the normal cumulative distribution function to the output. In <span class="citation">Wang and Zhou (<a href="#ref-wang2019continuous">2019</a>)</span>, the multivariate Gaussian policy is theoretically explored, but it assumes no constraint on weights.</p>
<p>Some natural parametric distributions emerge as alternatives. If only one asset is traded, then the Bernoulli distribution can be used to determine whether or not to buy the asset. If a riskless asset is available, the beta distribution offers more flexibility because the values for the proportion invested in the risky asset span the whole interval; the remainder can be invested into the safe asset. When many asset are traded, things become more complicated because of the budget constraint. One ideal candidate is the Dirichlet distribution because it is defined on a simplex (see Equation <a href="RL.html#eq:simplex">(17.13)</a>):
<span class="math display">\[f_{\boldsymbol{\alpha}}(w_1,\dots,w_n)=\frac{1}{B(\boldsymbol{\alpha})}\prod_{n=1}^Nw_n^{\alpha_n-1},\]</span>
where <span class="math inline">\(B(\boldsymbol{\alpha})\)</span> is the multinomial beta function:
<span class="math display">\[B(\boldsymbol{\alpha})=\frac{\prod_{n=1}^N\Gamma(\alpha_n)}{\Gamma\left(\sum_{n=1}^N\alpha_n \right)}.\]</span></p>
<p>If we set <span class="math inline">\(\pi=\pi_{\boldsymbol{\alpha}}=f_{\boldsymbol{\alpha}}\)</span>, the link with factors or characteristics can be coded through <span class="math inline">\({\boldsymbol{\alpha}}\)</span> via a linear form:
<span class="math display">\[\begin{equation}
(\textbf{F1}) \quad  \alpha_{n,t}=\theta_{0,t} + \sum_{k=1}^K \theta_{t}^{(k)}x_{t,n}^{(k)},
\end{equation}\]</span>
which is highly tractable, but may violate the condition that <span class="math inline">\(\alpha_{n,t}&gt;0\)</span> for some values of <span class="math inline">\(\theta_{k,t}\)</span>. Indeed, during the learning process, an update in <span class="math inline">\(\boldsymbol{\theta}\)</span> might yield values that are out of the feasible set of <span class="math inline">\(\boldsymbol{\alpha}_t\)</span>. In this case, it is possible to resort to a trick that is widely used in online learning (see, e.g., Section 2.3.1 in ). The idea is simply to find the acceptable solution that is closest to the suggestion from the algorithm. If we call <span class="math inline">\(\boldsymbol{\theta}^*\)</span> the result of an update rule from a given algorithm, then the closest feasible vector is
<span class="math display">\[\begin{equation}
\boldsymbol{\theta}= \underset{\textbf{z} \in \Theta(\textbf{x}_t)}{\min} ||\boldsymbol{\theta}^*-\textbf{z}||^2,
\end{equation}\]</span>
where <span class="math inline">\(||\cdot||\)</span> is the Euclidean norm and <span class="math inline">\(\Theta(\textbf{x}_t)\)</span> is the feasible set, that is, the set of vectors <span class="math inline">\(\boldsymbol{\theta}\)</span> such that the <span class="math inline">\(\alpha_{n,t}=\theta_{0,t} + \sum_{k=1}^K \theta_{t}^{(k)}x_{t,n}^{(k)}\)</span> are all nonnegative.</p>
<p>A second option for the form of the policy, <span class="math inline">\(\pi^2_{\boldsymbol{\theta}_t}\)</span>, is slightly more complex but remains always valid (i.e., has positive <span class="math inline">\(\alpha_{n,t}\)</span> values):
<span class="math display">\[\begin{equation}
(\textbf{F2}) \quad  \alpha_{n,t}=\exp \left(\theta_{0,t} + \sum_{k=1}^K \theta_{t}^{(k)}x_{t,n}^{(k)}\right),
\end{equation}\]</span>
which is simply the exponential of the first version. With some algebra, it is possible to derive the policy gradients. The policies <span class="math inline">\(\pi^j_{\boldsymbol{\theta}_t}\)</span> are defined by the Equations <span class="math inline">\((\textbf{Fj})\)</span> above. Let <span class="math inline">\(\digamma\)</span> denote the digamma function. Let <span class="math inline">\(\textbf{1}\)</span> denote the <span class="math inline">\(\mathbb{R}^N\)</span> vector of all ones. We have
<span class="math display">\[\begin{align*}
\frac{\nabla_{\boldsymbol{\theta}_t} \pi^1_{\boldsymbol{\theta}_t}}{\pi^1_{\boldsymbol{\theta}_t}}&amp;= \sum_{n=1}^N \left( \digamma \left( \textbf{1}&#39;\textbf{X}_t\boldsymbol{\theta}_t \right) - \digamma(\textbf{x}_{t,n}\boldsymbol{\theta}_t) + \ln w_n \right) \textbf{x}_{t,n}&#39; \\
\frac{\nabla_{\boldsymbol{\theta}_t} \pi^2_{\boldsymbol{\theta}_t}}{\pi^2_{\boldsymbol{\theta}_t}}&amp;= \sum_{n=1}^N \left( \digamma \left( \textbf{1}&#39;e^{\textbf{X}_{t}\boldsymbol{\theta}_t} \right) - \digamma(e^{\textbf{x}_{t,n}\boldsymbol{\theta}_t}) + \ln w_n \right) e^{\textbf{x}_{t,n}\boldsymbol{\theta}_t} \textbf{x}_{t,n}&#39; 
\end{align*}\]</span>
where <span class="math inline">\(e^{\textbf{X}}\)</span> is the element-wise exponential of a matrix <span class="math inline">\(\textbf{X}\)</span>.</p>
<p>The allocation can then either be made by direct sampling, or using the mean of the distribution <span class="math inline">\((\textbf{1}&#39;\boldsymbol{\alpha})^{-1}\boldsymbol{\alpha}\)</span>. Lastly, a techincal note: Dirichlet distributions can only be used for small portfolios because the scaling constant in the density becomes numerically intractable for large values of <span class="math inline">\(N\)</span> (e.g., above 50).</p>
</div>
</div>
<div id="simple-examples" class="section level2">
<h2><span class="header-section-number">17.4</span> Simple examples</h2>
<div id="q-learning-with-simulations" class="section level3">
<h3><span class="header-section-number">17.4.1</span> Q-learning with simulations</h3>
<p>To illustrate the gist of the problems mentioned above, we propose two implementations of <span class="math inline">\(Q\)</span>-learning. For simplicity, the first one is based on simulations. This helps understand the learning process in a simplified framework. We consider two assets: one risky and one riskless, with return equal to zero. The returns for the risky process follow an autoregressive model of order one (AR(1)): <span class="math inline">\(r_{t+1}=a+\rho r_t+\epsilon_{t+1}\)</span> with <span class="math inline">\(|\rho|&lt;1\)</span> and <span class="math inline">\(\epsilon\)</span> following a standard white noise with variance <span class="math inline">\(\sigma^2\)</span>. In practivc, individual (monthly) returns are seldom autocorrelated, but adjusting the autocorrelation helps understand if the algorithm learns correctly (see exercise below).</p>
<p>The environment consists only in observing the past return <span class="math inline">\(r_t\)</span>. Since we seek to estimate the <span class="math inline">\(Q\)</span> function, we need to discretize this state variable. The simplest choice is to resort to a binary variable: equal to -1 (negative) if <span class="math inline">\(r_t&lt;0\)</span> and to +1 (positive) if <span class="math inline">\(r_t\ge 0\)</span>. The actions are summarized by the quantity invested in the risky asset. It can take 5 values: 0 (risk-free portfolio), 0.25, 0.5, 0.75 and 1 (fully invested in the risky asset). This is for instance the same choice as in <span class="citation">Pendharkar and Cusatis (<a href="#ref-pendharkar2018trading">2018</a>)</span>.</p>
<p>The landscape of R libraries for RL is surprisingly sparse. We resort to the package <em>ReinforcementLearning</em> which has an intuitive implementation of <span class="math inline">\(Q\)</span>-learning (another option would be the <em>reinforcelearn</em> package). It requires a dataset with the usual inputs: state, action, reward and subsequent state. We start by simulating the returns: they drive the states and the rewards (portfolio returns). The actions are sampled randomly. Technically, the main function of the package requires that states and actions be of character type. The data is built in the chunk below.</p>

<div class="sourceCode" id="cb226"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb226-1" data-line-number="1"><span class="kw">library</span>(ReinforcementLearning)                              <span class="co"># Package for RL</span></a>
<a class="sourceLine" id="cb226-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">42</span>)                                                <span class="co"># Fixing the random seed</span></a>
<a class="sourceLine" id="cb226-3" data-line-number="3">n_sample &lt;-<span class="st"> </span><span class="dv">10</span><span class="op">^</span><span class="dv">5</span>                                            <span class="co"># Number of samples to be generated</span></a>
<a class="sourceLine" id="cb226-4" data-line-number="4">rho &lt;-<span class="st"> </span><span class="fl">0.8</span>                                                  <span class="co"># Autoregressive parameter</span></a>
<a class="sourceLine" id="cb226-5" data-line-number="5">sd &lt;-<span class="st"> </span><span class="fl">0.4</span>                                                   <span class="co"># Std. dev. of noise</span></a>
<a class="sourceLine" id="cb226-6" data-line-number="6">a &lt;-<span class="st"> </span><span class="fl">0.06</span> <span class="op">*</span><span class="st"> </span>rho                                             <span class="co"># Scaled mean of returns</span></a>
<a class="sourceLine" id="cb226-7" data-line-number="7">data_RL &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">returns =</span> a<span class="op">/</span>rho <span class="op">+</span><span class="st"> </span><span class="kw">arima.sim</span>(<span class="dt">n =</span> n_sample, <span class="co"># Returns via AR(1) simulation</span></a>
<a class="sourceLine" id="cb226-8" data-line-number="8">                                      <span class="kw">list</span>(<span class="dt">ar =</span> rho),       </a>
<a class="sourceLine" id="cb226-9" data-line-number="9">                                      <span class="dt">sd =</span> sd),</a>
<a class="sourceLine" id="cb226-10" data-line-number="10">                  <span class="dt">action =</span> <span class="kw">round</span>(<span class="kw">runif</span>(n_sample)<span class="op">*</span><span class="dv">4</span>)<span class="op">/</span><span class="dv">4</span>) <span class="op">%&gt;%</span><span class="st">  </span><span class="co"># Random action (portfolio)</span></a>
<a class="sourceLine" id="cb226-11" data-line-number="11"><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">new_state =</span> <span class="kw">if_else</span>(returns <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span>, <span class="st">&quot;neg&quot;</span>, <span class="st">&quot;pos&quot;</span>),  <span class="co"># Coding of state</span></a>
<a class="sourceLine" id="cb226-12" data-line-number="12">           <span class="dt">reward =</span> returns <span class="op">*</span><span class="st"> </span>action,                       <span class="co"># Reward = portfolio return</span></a>
<a class="sourceLine" id="cb226-13" data-line-number="13">           <span class="dt">state =</span> <span class="kw">lag</span>(new_state),                          <span class="co"># Next state</span></a>
<a class="sourceLine" id="cb226-14" data-line-number="14">           <span class="dt">action =</span> <span class="kw">as.character</span>(action)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb226-15" data-line-number="15"><span class="st">    </span><span class="kw">na.omit</span>()                                               <span class="co"># Remove one missing state</span></a>
<a class="sourceLine" id="cb226-16" data-line-number="16">data_RL <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">head</span>()                                          <span class="co"># Show first lines</span></a></code></pre></div>
<pre><code>## # A tibble: 6 x 5
##   returns action new_state  reward state
##     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;
## 1  -0.474 0.5    neg       -0.237  neg  
## 2  -0.185 0.25   neg       -0.0463 neg  
## 3   0.146 0.25   pos        0.0364 neg  
## 4   0.543 0.75   pos        0.407  pos  
## 5   0.202 0.75   pos        0.152  pos  
## 6   0.376 0.25   pos        0.0940 pos</code></pre>
<p></p>
<p>There are 3 parameters in the implementation of the <em>Q</em>-learning algorithm:</p>
<ul>
<li><span class="math inline">\(\eta\)</span>, which is the learning rate in the updating Equation <a href="RL.html#eq:QLupdate">(17.8)</a>. In <em>ReinforcementLearning</em>, this is coded as <em>alpha</em>;<br />
</li>
<li><span class="math inline">\(\gamma\)</span>, the discounting rate for the rewards (also shown in Equation <a href="RL.html#eq:QLupdate">(17.8)</a>);<br />
</li>
<li>and <span class="math inline">\(\epsilon\)</span>, which controls the rate of exploration versus exploitation (see Equation <a href="RL.html#eq:egreedy">(17.10)</a>).</li>
</ul>

<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb228-1" data-line-number="1">control &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">alpha =</span> <span class="fl">0.1</span>,                       <span class="co"># Learning rate</span></a>
<a class="sourceLine" id="cb228-2" data-line-number="2">                <span class="dt">gamma =</span> <span class="fl">0.7</span>,                       <span class="co"># Discount factor for rewards</span></a>
<a class="sourceLine" id="cb228-3" data-line-number="3">                <span class="dt">epsilon =</span> <span class="fl">0.1</span>)                     <span class="co"># Exploration rate</span></a>
<a class="sourceLine" id="cb228-4" data-line-number="4"></a>
<a class="sourceLine" id="cb228-5" data-line-number="5">fit_RL &lt;-<span class="st"> </span><span class="kw">ReinforcementLearning</span>(data_RL,           <span class="co"># Main RL function</span></a>
<a class="sourceLine" id="cb228-6" data-line-number="6">                               <span class="dt">s =</span> <span class="st">&quot;state&quot;</span>, </a>
<a class="sourceLine" id="cb228-7" data-line-number="7">                               <span class="dt">a =</span> <span class="st">&quot;action&quot;</span>, </a>
<a class="sourceLine" id="cb228-8" data-line-number="8">                               <span class="dt">r =</span> <span class="st">&quot;reward&quot;</span>, </a>
<a class="sourceLine" id="cb228-9" data-line-number="9">                               <span class="dt">s_new =</span> <span class="st">&quot;new_state&quot;</span>, </a>
<a class="sourceLine" id="cb228-10" data-line-number="10">                               <span class="dt">control =</span> control)</a>
<a class="sourceLine" id="cb228-11" data-line-number="11"><span class="kw">print</span>(fit_RL)   <span class="co"># Show the output</span></a></code></pre></div>
<pre><code>## State-Action function Q
##          0.25         0         1      0.75      0.5
## neg 0.2473169 0.4216894 0.1509653 0.1734538 0.229004
## pos 1.0721669 0.7561417 1.4739050 1.1214795 1.045047
## 
## Policy
## neg pos 
## &quot;0&quot; &quot;1&quot; 
## 
## Reward (last iteration)
## [1] 2588.659</code></pre>
<p></p>
<p>The output shows the <em>Q</em> function, which depends naturally both on states and actions. When the state is negative, large risky positions (action equal to 0.75 or 1.00) are associated with the smallest average rewards whereas small positions yield the highest average rewards. When the state is positive, the average rewards are the highest for the largest allocations. The rewards in both cases are almost a monotonic function of the proportion invested in the risky asset. Thus, the recommendation of the algorithm (i.e., the policy) is to be fully invested in a postive state and to refrain from investing in a negative state. Given the positive autocorrelation of the underlying process, this does make sense.</p>
<p>Basically, the algorithm has simply learned that positive (<em>resp.</em> negative) returns are more likely to follow positive (<em>resp</em>. negative) returns. While this is somewhat reassuring, it is by no means impressive and much simpler tools would yield similar conclusions and guidance.</p>
</div>
<div id="RLemp2" class="section level3">
<h3><span class="header-section-number">17.4.2</span> Q-learning with market data</h3>
<p>The second application is based on the financial dataset. To reduce the dimensionality of the problem, we will assume:<br />
- that only one feature (price-to-book ratio) captures the state of the environment. This feature is processed so that is has only a limited number of possible values;<br />
- that actions take values over a discrete set consisting of three positions: +1 (buy the market), -1 (sell the market) and 0 (hold no risky positions);<br />
- that only two assets are traded: those with stock_id equal to 3 and 4 - they both have 245 days of trading data.</p>
<p>The construction of the dataset is unelegantly coded below.</p>

<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb230-1" data-line-number="1">return_<span class="dv">3</span> &lt;-<span class="st"> </span>data_ml <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(stock_id <span class="op">==</span><span class="st"> </span><span class="dv">3</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(R1M_Usd)  <span class="co"># Return of asset 3</span></a>
<a class="sourceLine" id="cb230-2" data-line-number="2">return_<span class="dv">4</span> &lt;-<span class="st"> </span>data_ml <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(stock_id <span class="op">==</span><span class="st"> </span><span class="dv">4</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(R1M_Usd)  <span class="co"># Return of asset 4</span></a>
<a class="sourceLine" id="cb230-3" data-line-number="3">pb_<span class="dv">3</span> &lt;-<span class="st"> </span>data_ml <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(stock_id <span class="op">==</span><span class="st"> </span><span class="dv">3</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(Pb)           <span class="co"># P/B ratio of asset 3</span></a>
<a class="sourceLine" id="cb230-4" data-line-number="4">pb_<span class="dv">4</span> &lt;-<span class="st"> </span>data_ml <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(stock_id <span class="op">==</span><span class="st"> </span><span class="dv">4</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(Pb)           <span class="co"># P/B ratio of asset 4</span></a>
<a class="sourceLine" id="cb230-5" data-line-number="5">action_<span class="dv">3</span> &lt;-<span class="st"> </span><span class="kw">floor</span>(<span class="kw">runif</span>(<span class="kw">length</span>(pb_<span class="dv">3</span>))<span class="op">*</span><span class="dv">3</span>) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>                     <span class="co"># Action for asset 3 (random)</span></a>
<a class="sourceLine" id="cb230-6" data-line-number="6">action_<span class="dv">4</span> &lt;-<span class="st"> </span><span class="kw">floor</span>(<span class="kw">runif</span>(<span class="kw">length</span>(pb_<span class="dv">4</span>))<span class="op">*</span><span class="dv">3</span>) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>                     <span class="co"># Action for asset 4 (random)</span></a>
<a class="sourceLine" id="cb230-7" data-line-number="7"></a>
<a class="sourceLine" id="cb230-8" data-line-number="8">RL_data &lt;-<span class="st"> </span><span class="kw">tibble</span>(return_<span class="dv">3</span>, return_<span class="dv">4</span>,                            <span class="co"># Building the dataset</span></a>
<a class="sourceLine" id="cb230-9" data-line-number="9">                  pb_<span class="dv">3</span>, pb_<span class="dv">4</span>,</a>
<a class="sourceLine" id="cb230-10" data-line-number="10">                  action_<span class="dv">3</span>, action_<span class="dv">4</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb230-11" data-line-number="11"><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">action =</span> <span class="kw">paste</span>(action_<span class="dv">3</span>, action_<span class="dv">4</span>),                   <span class="co"># Uniting actions</span></a>
<a class="sourceLine" id="cb230-12" data-line-number="12">           <span class="dt">pb_3 =</span> <span class="kw">round</span>(<span class="dv">5</span> <span class="op">*</span><span class="st"> </span>pb_<span class="dv">3</span>),                               <span class="co"># Simplifying states (P/B)</span></a>
<a class="sourceLine" id="cb230-13" data-line-number="13">           <span class="dt">pb_4 =</span> <span class="kw">round</span>(<span class="dv">5</span> <span class="op">*</span><span class="st"> </span>pb_<span class="dv">4</span>),                               <span class="co"># Simplifying states (P/B)</span></a>
<a class="sourceLine" id="cb230-14" data-line-number="14">           <span class="dt">state =</span> <span class="kw">paste</span>(pb_<span class="dv">3</span>, pb_<span class="dv">4</span>),                            <span class="co"># Uniting states</span></a>
<a class="sourceLine" id="cb230-15" data-line-number="15">           <span class="dt">reward =</span> action_<span class="dv">3</span><span class="op">*</span>return_<span class="dv">3</span> <span class="op">+</span><span class="st"> </span>action_<span class="dv">4</span><span class="op">*</span>return_<span class="dv">4</span>,       <span class="co"># Computing rewards</span></a>
<a class="sourceLine" id="cb230-16" data-line-number="16">           <span class="dt">new_state =</span> <span class="kw">lead</span>(state)) <span class="op">%&gt;%</span><span class="st">                          </span><span class="co"># Infer new state</span></a>
<a class="sourceLine" id="cb230-17" data-line-number="17"><span class="st">    </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="op">-</span>pb_<span class="dv">3</span>, <span class="op">-</span>pb_<span class="dv">4</span>, <span class="op">-</span>action_<span class="dv">3</span>,                       <span class="co"># Remove superfluous vars.</span></a>
<a class="sourceLine" id="cb230-18" data-line-number="18">                  <span class="op">-</span>action_<span class="dv">4</span>, <span class="op">-</span>return_<span class="dv">3</span>, <span class="op">-</span>return_<span class="dv">4</span>) </a>
<a class="sourceLine" id="cb230-19" data-line-number="19"><span class="kw">head</span>(RL_data)                                                    <span class="co"># Showing the result</span></a></code></pre></div>
<pre><code>## # A tibble: 6 x 4
##   action state reward new_state
##   &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;    
## 1 -1 -1  1 1   -0.061 1 1      
## 2 0 1    1 1    0     1 1      
## 3 -1 0   1 1   -0.018 1 1      
## 4 0 -1   1 1    0.011 1 1      
## 5 -1 1   1 1   -0.036 1 1      
## 6 -1 -1  1 1   -0.056 1 1</code></pre>
<p></p>
<p>Actions and states have to be merged to yield all possible combinations. To simplify the states, we round 5 times the price-to-book ratios.</p>
<p>We keep the same hyperparameters as in the previous example. Columns below stand for actions: the first (<span class="math inline">\(resp.\)</span> second) number notes the position in the first (<span class="math inline">\(resp.\)</span> second) asset. The rows correspond to states. The scaled P/B ratios are separated by a point (e.g., X2.3 means that the first (<span class="math inline">\(resp.\)</span> second) asset has a scaled P/B of 2 (<span class="math inline">\(resp.\)</span> 3).</p>

<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb232-1" data-line-number="1">fit_RL2 &lt;-<span class="st"> </span><span class="kw">ReinforcementLearning</span>(RL_data,           <span class="co"># Main RL function</span></a>
<a class="sourceLine" id="cb232-2" data-line-number="2">                               <span class="dt">s =</span> <span class="st">&quot;state&quot;</span>, </a>
<a class="sourceLine" id="cb232-3" data-line-number="3">                               <span class="dt">a =</span> <span class="st">&quot;action&quot;</span>, </a>
<a class="sourceLine" id="cb232-4" data-line-number="4">                               <span class="dt">r =</span> <span class="st">&quot;reward&quot;</span>, </a>
<a class="sourceLine" id="cb232-5" data-line-number="5">                               <span class="dt">s_new =</span> <span class="st">&quot;new_state&quot;</span>, </a>
<a class="sourceLine" id="cb232-6" data-line-number="6">                               <span class="dt">control =</span> control)</a>
<a class="sourceLine" id="cb232-7" data-line-number="7">fit_RL2<span class="op">$</span>Q &lt;-<span class="st"> </span><span class="kw">round</span>(fit_RL2<span class="op">$</span>Q, <span class="dv">3</span>) <span class="co"># Round the Q-matrix</span></a>
<a class="sourceLine" id="cb232-8" data-line-number="8"><span class="kw">print</span>(fit_RL2)                   <span class="co"># Show the output </span></a></code></pre></div>
<pre><code>## State-Action function Q
##        0 0    0 1   0 -1  -1 -1   -1 0   -1 1   1 -1    1 0    1 1
## X0.2 0.000  0.000  0.000 -0.017  0.000  0.000  0.000  0.002  0.000
## X0.3 0.000  0.000  0.003  0.000  0.000  0.000  0.030  0.000  0.000
## X3.1 0.002  0.000  0.005  0.000 -0.002  0.000  0.000  0.000  0.000
## X2.1 0.005  0.018  0.009 -0.028  0.010 -0.003  0.021  0.008 -0.004
## X2.2 0.000  0.010  0.000  0.014  0.000  0.000 -0.013  0.006  0.000
## X2.3 0.000  0.000  0.000  0.000  0.000  0.020  0.000 -0.034  0.000
## X1.1 0.002 -0.005 -0.022 -0.011 -0.002 -0.009 -0.020 -0.014 -0.023
## X1.2 0.006  0.016  0.006  0.028 -0.001  0.001  0.020  0.020 -0.001
## X1.3 0.001  0.004  0.004 -0.011  0.000  0.003  0.005  0.003  0.010
## 
## Policy
##    X0.2    X0.3    X3.1    X2.1    X2.2    X2.3    X1.1    X1.2    X1.3 
##   &quot;1 0&quot;  &quot;1 -1&quot;  &quot;0 -1&quot;  &quot;1 -1&quot; &quot;-1 -1&quot;  &quot;-1 1&quot;   &quot;0 0&quot; &quot;-1 -1&quot;   &quot;1 1&quot; 
## 
## Reward (last iteration)
## [1] -1.296</code></pre>
<p></p>
<p>The output shows that there are many combinations of states and actions that are not spanned by the data: basically, the <span class="math inline">\(Q\)</span> function has a zero and it is likely that the combination has not been explored. Some states seem to be more often represented (X1.1, X1.2 and X2.1), others, less (X3.1&quot; and X3.2). It is hard to make any sense of the recommendations. Some states close X0.1 and X1.1 but the outcomes related to them are very different (buy and short versus hold and buy). Moreover, there is no coherence and no monotonicity in actions with respect to individual state values: low values of states can be associated to very different actions.</p>
<p>One reason why these conclusion do not appear trustworthy pertains to the data size. With only 200+ time points and 99 state-action pairs (11 times 9), this yields on average only two data points to compute the <span class="math inline">\(Q\)</span> function. This could be improved by testing more random actions, but the limits of the sample size would eventually (rapidly) be reached anyway. This is left as an exercise (see below).</p>
</div>
</div>
<div id="concluding-remarks" class="section level2">
<h2><span class="header-section-number">17.5</span> Concluding remarks</h2>
<p>Reinforcement learning has been applied to financial problems for a long time. Early contributions in the late 1990s include <span class="citation">Neuneier (<a href="#ref-neuneier1996optimal">1996</a>)</span>, <span class="citation">Moody and Wu (<a href="#ref-moody1997optimization">1997</a>)</span>, <span class="citation">Moody et al. (<a href="#ref-moody1998performance">1998</a>)</span> and <span class="citation">Neuneier (<a href="#ref-neuneier1998enhancing">1998</a>)</span>. Since then, many researchers in the computer science field have sought to apply RL techniques to portfolio problems. The advent of massive datasets and the increase in dimensionality make it hard for RL tools to adapt well to very rich environments that are encountered in factor investing.</p>
<p>Recently, some approaches seek to adapt RL to continuous action spaces (<span class="citation">Wang and Zhou (<a href="#ref-wang2019continuous">2019</a>)</span>, <span class="citation">Aboussalah and Lee (<a href="#ref-aboussalah2020continuous">2020</a>)</span>) but not to high-dimensional state spaces. These spaces are those required in factor investing because all firms yields hundreds of data points characterizing their economic situation. In addition, applications of RL in financial frameworks have a particularity compared to many typical RL tasks: in financial markets, actions of agents have <strong>no impact on the environment</strong> (unless the agent is able to perform massive trades, which is rare and ill-advised because it pushes prices in the wrong direction). This lack of impact of actions may possibly mitigate the efficiency of traditional RL approaches.</p>
<p>Those are challenges that will need to be solved in order for RL to become competitive with alternative (supervised) methods. Nevertheless, the progressive (online-like) way RL works seems suitable for non-stationary environments: the algorithm slowly shifts paradigms as new data arrives. In stationary environments, it has been shown that RL manages to converge to optimal solutions (<span class="citation">Kong et al. (<a href="#ref-kong2019new">2019</a>)</span>, <span class="citation">Chaouki et al. (<a href="#ref-chaouki2020deep">2020</a>)</span>). Therefore, in non-stationary markets, RL could be a recourse to build dynamic predictions that adapt to changing macroeconomic conditions. More research needs to be carried out in this field on large dimensional datasets.</p>
<p>We end this chapter by underlining that reinforcement learning has also been used to estimate complex theoretical models (<span class="citation">Halperin and Feldshteyn (<a href="#ref-halperin2018market">2018</a>)</span>, <span class="citation">Garc'a-Galicia, Carsteanu, and Clempner (<a href="#ref-garcia2019continuous">2019</a>)</span>). The research in the field is incredibly diversified and is orientated towards many directions. It is likely that captivating work will be published in the near future.</p>
</div>
<div id="exercises" class="section level2">
<h2><span class="header-section-number">17.6</span> Exercises</h2>
<ol style="list-style-type: decimal">
<li><p>Test what happens if the process for generating returns has a negative autocorrelation. What is the impact on the <span class="math inline">\(Q\)</span> function and the policy?</p></li>
<li><p>Keeping the same 2 assets as in Section <a href="RL.html#RLemp2">17.4.2</a>, increase the size of RL_data by testing <strong>all possible action combination</strong> for each original data point. Re-run the <span class="math inline">\(Q\)</span>-learning function and see what happens.</p></li>
</ol>

</div>
</div>



<h3>References</h3>
<div id="refs" class="references">
<div id="ref-aboussalah2020continuous">
<p>Aboussalah, Amine Mohamed, and Chi-Guhn Lee. 2020. Continuous Control with Stacked Deep Dynamic Recurrent Reinforcement Learning for Portfolio Optimization. <em>Expert Systems with Applications</em> 140: 112891.</p>
</div>
<div id="ref-almahdi2019constrained">
<p>Almahdi, Saud, and Steve Y Yang. 2019. A Constrained Portfolio Trading System Using Particle Swarm Algorithm and Recurrent Reinforcement Learning. <em>Expert Systems with Applications</em> 130: 14556.</p>
</div>
<div id="ref-bertoluzzo2012testing">
<p>Bertoluzzo, Francesco, and Marco Corazza. 2012. Testing Different Reinforcement Learning Configurations for Financial Trading: Introduction and Applications. <em>Procedia Economics and Finance</em> 3: 6877.</p>
</div>
<div id="ref-bertsekas2017dynamic">
<p>Bertsekas, Dimitri P. 2017. <em>Dynamic Programming and Optimal Control - Volume Ii, Fourth Edition</em>. Athena Scientific.</p>
</div>
<div id="ref-chaouki2020deep">
<p>Chaouki, Ayman, Stephen Hardiman, Christian Schmidt, Joachim de Lataillade, and others. 2020. Deep Deterministic Portfolio Optimization. <em>arXiv Preprint</em>, no. 2003.06497.</p>
</div>
<div id="ref-charpentier2020reinforcement">
<p>Charpentier, Arthur, Romuald Elie, and Carl Remlinger. 2020. Reinforcement Learning in Economics and Finance. <em>arXiv Preprint</em>, no. 2003.10014.</p>
</div>
<div id="ref-garcia2019continuous">
<p>Garc'a-Galicia, Mauricio, Alin A Carsteanu, and Julio B Clempner. 2019. Continuous-Time Reinforcement Learning Approach for Portfolio Management with Time Penalization. <em>Expert Systems with Applications</em> 129: 2736.</p>
</div>
<div id="ref-halperin2018market">
<p>Halperin, Igor, and Ilya Feldshteyn. 2018. Market Self-Learning of Signals, Impact and Optimal Trading: Invisible Hand Inference with Free Energy. <em>arXiv Preprint</em>, no. 1805.06126.</p>
</div>
<div id="ref-kolm2019modern">
<p>Kolm, Petter N, and Gordon Ritter. 2019b. Modern Perspectives on Reinforcement Learning in Finance. <em>Journal of Machine Learning in Finance</em> 1 (1).</p>
</div>
<div id="ref-kong2019new">
<p>Kong, Weiwei, Christopher Liaw, Aranyak Mehta, and D Sivakumar. 2019. A New Dog Learns Old Tricks: RL Finds Classic Optimization Algorithms. <em>Proceedings of the ICLR Conference</em>, 125.</p>
</div>
<div id="ref-meng2019reinforcement">
<p>Meng, Terry Lingze, and Matloob Khushi. 2019. Reinforcement Learning in Financial Markets. <em>Data</em> 4 (3): 110.</p>
</div>
<div id="ref-moody1997optimization">
<p>Moody, John, and Lizhong Wu. 1997. Optimization of Trading Systems and Portfolios. In <em>Proceedings of the Ieee/Iafe 1997 Computational Intelligence for Financial Engineering (Cifer)</em>, 300307. IEEE.</p>
</div>
<div id="ref-moody1998performance">
<p>Moody, John, Lizhong Wu, Yuansong Liao, and Matthew Saffell. 1998. Performance Functions and Reinforcement Learning for Trading Systems and Portfolios. <em>Journal of Forecasting</em> 17 (5-6): 44170.</p>
</div>
<div id="ref-mosavi2020comprehensive">
<p>Mosavi, Amir, Pedram Ghamisi, Yaser Faghan, Puhong Duan, and Shahab Shamshirband. 2020. Comprehensive Review of Deep Reinforcement Learning Methods and Applications in Economics. <em>Preprints.org</em>. Preprints.</p>
</div>
<div id="ref-neuneier1996optimal">
<p>Neuneier, Ralph. 1996. Optimal Asset Allocation Using Adaptive Dynamic Programming. In <em>Advances in Neural Information Processing Systems</em>, 95258.</p>
</div>
<div id="ref-neuneier1998enhancing">
<p>Neuneier, Ralph. 1998. Enhancing Q-Learning for Optimal Asset Allocation. In <em>Advances in Neural Information Processing Systems</em>, 93642.</p>
</div>
<div id="ref-pendharkar2018trading">
<p>Pendharkar, Parag C, and Patrick Cusatis. 2018. Trading Financial Indices with Reinforcement Learning Agents. <em>Expert Systems with Applications</em> 103: 113.</p>
</div>
<div id="ref-powell2011review">
<p>Powell, Warren B, and Jun Ma. 2011. A Review of Stochastic Algorithms with Continuous Value Function Approximation and Some New Approximate Policy Iteration Algorithms for Multidimensional Continuous Applications. <em>Journal of Control Theory and Applications</em> 9 (3): 33652.</p>
</div>
<div id="ref-sato2019model">
<p>Sato, Yoshiharu. 2019. Model-Free Reinforcement Learning for Financial Portfolios: A Brief Survey. <em>arXiv Preprint</em>, no. 1904.04973.</p>
</div>
<div id="ref-silver2016mastering">
<p>Silver, David, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, and Marc Lanctot. 2016. Mastering the Game of Go with Deep Neural Networks and Tree Search. <em>Nature</em> 529: 48489.</p>
</div>
<div id="ref-sutton2018reinforcement">
<p>Sutton, Richard S, and Andrew G Barto. 2018. <em>Reinforcement Learning: An Introduction (2nd Edition)</em>. MIT press.</p>
</div>
<div id="ref-wang2019continuous">
<p>Wang, Haoran, and Xun Yu Zhou. 2019. Continuous-Time Mean-Variance Portfolio Selection: A Reinforcement Learning Framework. <em>SSRN Working Paper</em> 3382932.</p>
</div>
<div id="ref-watkins1992q">
<p>Watkins, Christopher JCH, and Peter Dayan. 1992. Q-Learning. <em>Machine Learning</em> 8 (3-4): 27992.</p>
</div>
<div id="ref-xiong2018practical">
<p>Xiong, Zhuoran, Xiao-Yang Liu, Shan Zhong, Hongyang Yang, and Anwar Walid. 2018. Practical Deep Reinforcement Learning Approach for Stock Trading. <em>arXiv Preprint</em>, no. 1811.07522.</p>
</div>
<div id="ref-yang2018investor">
<p>Yang, Steve Y, Yangyang Yu, and Saud Almahdi. 2018. An Investor Sentiment Reward-Based Trading System Using Gaussian Inverse Reinforcement Learning Algorithm. <em>Expert Systems with Applications</em> 114: 388401.</p>
</div>
<div id="ref-yu2019model">
<p>Yu, Pengqian, Joon Sern Lee, Ilya Kulyatin, Zekun Shi, and Sakyasingha Dasgupta. 2019. Model-Based Deep Reinforcement Learning for Dynamic Portfolio Optimization. <em>arXiv Preprint</em>, no. 1901.08740.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="31">
<li id="fn31"><p>Like neural networks, reinforcement learning methods have also been recently developed for derivatives pricing and hedging, see for instance <span class="citation">Kolm and Ritter (<a href="#ref-kolm2019dynamic">2019</a><a href="#ref-kolm2019dynamic">a</a>)</span>.<a href="RL.html#fnref31" class="footnote-back"></a></p></li>
<li id="fn32"><p>e.g., Sharpe ratio which is for instance used in <span class="citation">Moody et al. (<a href="#ref-moody1998performance">1998</a>)</span>, <span class="citation">Bertoluzzo and Corazza (<a href="#ref-bertoluzzo2012testing">2012</a>)</span> and <span class="citation">Aboussalah and Lee (<a href="#ref-aboussalah2020continuous">2020</a>)</span> or drawdown-based ratios, as in <span class="citation">Almahdi and Yang (<a href="#ref-almahdi2017adaptive">2017</a>)</span>.<a href="RL.html#fnref32" class="footnote-back"></a></p></li>
<li id="fn33"><p>Some recent papers consider arbitrary weights (e.g., <span class="citation">Jiang, Xu, and Liang (<a href="#ref-jiang2017deep">2017</a>)</span> and <span class="citation">Yu et al. (<a href="#ref-yu2019model">2019</a>)</span>) for a limited number of assets.<a href="RL.html#fnref33" class="footnote-back"></a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="unsup.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="data-description.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["ML_factor.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true,
"info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
