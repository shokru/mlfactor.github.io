<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 19 Solution to exercises | Machine Learning for Factor Investing</title>
  <meta name="description" content="Chapter 19 Solution to exercises | Machine Learning for Factor Investing" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 19 Solution to exercises | Machine Learning for Factor Investing" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 19 Solution to exercises | Machine Learning for Factor Investing" />
  
  
  

<meta name="author" content="Guillaume Coqueret and Tony Guida" />


<meta name="date" content="2020-04-13" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="data-description.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="part"><span><b>I Introduction</b></span></li>
<li class="chapter" data-level="1" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="preface.html"><a href="preface.html#what-this-book-is-not-about"><i class="fa fa-check"></i><b>1.1</b> What this book is not about</a></li>
<li class="chapter" data-level="1.2" data-path="preface.html"><a href="preface.html#the-targeted-audience"><i class="fa fa-check"></i><b>1.2</b> The targeted audience</a></li>
<li class="chapter" data-level="1.3" data-path="preface.html"><a href="preface.html#how-this-book-is-structured"><i class="fa fa-check"></i><b>1.3</b> How this book is structured</a></li>
<li class="chapter" data-level="1.4" data-path="preface.html"><a href="preface.html#companion-website"><i class="fa fa-check"></i><b>1.4</b> Companion website</a></li>
<li class="chapter" data-level="1.5" data-path="preface.html"><a href="preface.html#why-r"><i class="fa fa-check"></i><b>1.5</b> Why R?</a></li>
<li class="chapter" data-level="1.6" data-path="preface.html"><a href="preface.html#coding-instructions"><i class="fa fa-check"></i><b>1.6</b> Coding instructions</a></li>
<li class="chapter" data-level="1.7" data-path="preface.html"><a href="preface.html#acknowledgements"><i class="fa fa-check"></i><b>1.7</b> Acknowledgements</a></li>
<li class="chapter" data-level="1.8" data-path="preface.html"><a href="preface.html#future-developments"><i class="fa fa-check"></i><b>1.8</b> Future developments</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="notdata.html"><a href="notdata.html"><i class="fa fa-check"></i><b>2</b> Notations and data</a><ul>
<li class="chapter" data-level="2.1" data-path="notdata.html"><a href="notdata.html#notations"><i class="fa fa-check"></i><b>2.1</b> Notations</a></li>
<li class="chapter" data-level="2.2" data-path="notdata.html"><a href="notdata.html#dataset"><i class="fa fa-check"></i><b>2.2</b> Dataset</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>3</b> Introduction</a><ul>
<li class="chapter" data-level="3.1" data-path="intro.html"><a href="intro.html#context"><i class="fa fa-check"></i><b>3.1</b> Context</a></li>
<li class="chapter" data-level="3.2" data-path="intro.html"><a href="intro.html#portfolio-construction-the-workflow"><i class="fa fa-check"></i><b>3.2</b> Portfolio construction: the workflow</a></li>
<li class="chapter" data-level="3.3" data-path="intro.html"><a href="intro.html#machine-learning-is-no-magic-wand"><i class="fa fa-check"></i><b>3.3</b> Machine Learning is no Magic Wand</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="factor.html"><a href="factor.html"><i class="fa fa-check"></i><b>4</b> Factor investing and asset pricing anomalies</a><ul>
<li class="chapter" data-level="4.1" data-path="factor.html"><a href="factor.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="factor.html"><a href="factor.html#detecting-anomalies"><i class="fa fa-check"></i><b>4.2</b> Detecting anomalies</a><ul>
<li class="chapter" data-level="4.2.1" data-path="factor.html"><a href="factor.html#simple-portfolio-sorts"><i class="fa fa-check"></i><b>4.2.1</b> Simple portfolio sorts</a></li>
<li class="chapter" data-level="4.2.2" data-path="factor.html"><a href="factor.html#factors"><i class="fa fa-check"></i><b>4.2.2</b> Factors</a></li>
<li class="chapter" data-level="4.2.3" data-path="factor.html"><a href="factor.html#predictive-regressions-sorts-and-p-value-issues"><i class="fa fa-check"></i><b>4.2.3</b> Predictive regressions, sorts, and p-value issues</a></li>
<li class="chapter" data-level="4.2.4" data-path="factor.html"><a href="factor.html#fama-macbeth-regressions"><i class="fa fa-check"></i><b>4.2.4</b> Fama-Macbeth regressions</a></li>
<li class="chapter" data-level="4.2.5" data-path="factor.html"><a href="factor.html#factor-competition"><i class="fa fa-check"></i><b>4.2.5</b> Factor competition</a></li>
<li class="chapter" data-level="4.2.6" data-path="factor.html"><a href="factor.html#advanced-techniques"><i class="fa fa-check"></i><b>4.2.6</b> Advanced techniques</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="factor.html"><a href="factor.html#factors-or-characteristics"><i class="fa fa-check"></i><b>4.3</b> Factors or characteristics?</a></li>
<li class="chapter" data-level="4.4" data-path="factor.html"><a href="factor.html#hot-topics-momentum-timing-and-esg"><i class="fa fa-check"></i><b>4.4</b> Hot topics: momentum, timing and ESG</a><ul>
<li class="chapter" data-level="4.4.1" data-path="factor.html"><a href="factor.html#factor-momentum"><i class="fa fa-check"></i><b>4.4.1</b> Factor momentum</a></li>
<li class="chapter" data-level="4.4.2" data-path="factor.html"><a href="factor.html#factor-timing"><i class="fa fa-check"></i><b>4.4.2</b> Factor timing</a></li>
<li class="chapter" data-level="4.4.3" data-path="factor.html"><a href="factor.html#the-green-factors"><i class="fa fa-check"></i><b>4.4.3</b> The green factors</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="factor.html"><a href="factor.html#the-link-with-machine-learning"><i class="fa fa-check"></i><b>4.5</b> The link with machine learning</a><ul>
<li class="chapter" data-level="4.5.1" data-path="factor.html"><a href="factor.html#a-short-list-of-recent-references"><i class="fa fa-check"></i><b>4.5.1</b> A short list of recent references</a></li>
<li class="chapter" data-level="4.5.2" data-path="factor.html"><a href="factor.html#explicit-connections-with-asset-pricing-models"><i class="fa fa-check"></i><b>4.5.2</b> Explicit connections with asset pricing models</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="factor.html"><a href="factor.html#coding-exercises"><i class="fa fa-check"></i><b>4.6</b> Coding exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Data.html"><a href="Data.html"><i class="fa fa-check"></i><b>5</b> Data preprocessing</a><ul>
<li class="chapter" data-level="5.1" data-path="Data.html"><a href="Data.html#know-your-data"><i class="fa fa-check"></i><b>5.1</b> Know your data</a></li>
<li class="chapter" data-level="5.2" data-path="Data.html"><a href="Data.html#missing-data"><i class="fa fa-check"></i><b>5.2</b> Missing data</a></li>
<li class="chapter" data-level="5.3" data-path="Data.html"><a href="Data.html#outlier-detection"><i class="fa fa-check"></i><b>5.3</b> Outlier detection</a></li>
<li class="chapter" data-level="5.4" data-path="Data.html"><a href="Data.html#feateng"><i class="fa fa-check"></i><b>5.4</b> Feature engineering</a><ul>
<li class="chapter" data-level="5.4.1" data-path="Data.html"><a href="Data.html#feature-selection"><i class="fa fa-check"></i><b>5.4.1</b> Feature selection</a></li>
<li class="chapter" data-level="5.4.2" data-path="Data.html"><a href="Data.html#scaling"><i class="fa fa-check"></i><b>5.4.2</b> Scaling the predictors</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="Data.html"><a href="Data.html#labelling"><i class="fa fa-check"></i><b>5.5</b> Labelling</a><ul>
<li class="chapter" data-level="5.5.1" data-path="Data.html"><a href="Data.html#simple-labels"><i class="fa fa-check"></i><b>5.5.1</b> Simple labels</a></li>
<li class="chapter" data-level="5.5.2" data-path="Data.html"><a href="Data.html#categorical-labels"><i class="fa fa-check"></i><b>5.5.2</b> Categorical labels</a></li>
<li class="chapter" data-level="5.5.3" data-path="Data.html"><a href="Data.html#the-triple-barrier-method"><i class="fa fa-check"></i><b>5.5.3</b> The triple barrier method</a></li>
<li class="chapter" data-level="5.5.4" data-path="Data.html"><a href="Data.html#filtering-the-sample"><i class="fa fa-check"></i><b>5.5.4</b> Filtering the sample</a></li>
<li class="chapter" data-level="5.5.5" data-path="Data.html"><a href="Data.html#horizons"><i class="fa fa-check"></i><b>5.5.5</b> Return horizons</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="Data.html"><a href="Data.html#pers"><i class="fa fa-check"></i><b>5.6</b> Handling persistence</a></li>
<li class="chapter" data-level="5.7" data-path="Data.html"><a href="Data.html#extensions"><i class="fa fa-check"></i><b>5.7</b> Extensions</a><ul>
<li class="chapter" data-level="5.7.1" data-path="Data.html"><a href="Data.html#transforming-features"><i class="fa fa-check"></i><b>5.7.1</b> Transforming features</a></li>
<li class="chapter" data-level="5.7.2" data-path="Data.html"><a href="Data.html#macrovar"><i class="fa fa-check"></i><b>5.7.2</b> Macro-economic variables</a></li>
<li class="chapter" data-level="5.7.3" data-path="Data.html"><a href="Data.html#active-learning"><i class="fa fa-check"></i><b>5.7.3</b> Active learning</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="Data.html"><a href="Data.html#additional-code-and-results"><i class="fa fa-check"></i><b>5.8</b> Additional code and results</a><ul>
<li class="chapter" data-level="5.8.1" data-path="Data.html"><a href="Data.html#impact-of-rescaling-graphical-representation"><i class="fa fa-check"></i><b>5.8.1</b> Impact of rescaling: graphical representation</a></li>
<li class="chapter" data-level="5.8.2" data-path="Data.html"><a href="Data.html#impact-of-rescaling-toy-example"><i class="fa fa-check"></i><b>5.8.2</b> Impact of rescaling: toy example</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="Data.html"><a href="Data.html#coding-exercises-1"><i class="fa fa-check"></i><b>5.9</b> Coding exercises</a></li>
</ul></li>
<li class="part"><span><b>II Common supervised algorithms</b></span></li>
<li class="chapter" data-level="6" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>6</b> Penalized regressions and sparse hedging for minimum variance portfolios</a><ul>
<li class="chapter" data-level="6.1" data-path="lasso.html"><a href="lasso.html#penalised-regressions"><i class="fa fa-check"></i><b>6.1</b> Penalised regressions</a><ul>
<li class="chapter" data-level="6.1.1" data-path="lasso.html"><a href="lasso.html#penreg"><i class="fa fa-check"></i><b>6.1.1</b> Simple regressions</a></li>
<li class="chapter" data-level="6.1.2" data-path="lasso.html"><a href="lasso.html#forms-of-penalizations"><i class="fa fa-check"></i><b>6.1.2</b> Forms of penalizations</a></li>
<li class="chapter" data-level="6.1.3" data-path="lasso.html"><a href="lasso.html#illustrations"><i class="fa fa-check"></i><b>6.1.3</b> Illustrations</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="lasso.html"><a href="lasso.html#sparse-hedging-for-minimum-variance-portfolios"><i class="fa fa-check"></i><b>6.2</b> Sparse hedging for minimum variance portfolios</a><ul>
<li class="chapter" data-level="6.2.1" data-path="lasso.html"><a href="lasso.html#presentation-and-derivations"><i class="fa fa-check"></i><b>6.2.1</b> Presentation and derivations</a></li>
<li class="chapter" data-level="6.2.2" data-path="lasso.html"><a href="lasso.html#sparseex"><i class="fa fa-check"></i><b>6.2.2</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="lasso.html"><a href="lasso.html#predictive-regressions"><i class="fa fa-check"></i><b>6.3</b> Predictive regressions</a><ul>
<li class="chapter" data-level="6.3.1" data-path="lasso.html"><a href="lasso.html#literature-review-and-principle"><i class="fa fa-check"></i><b>6.3.1</b> Literature review and principle</a></li>
<li class="chapter" data-level="6.3.2" data-path="lasso.html"><a href="lasso.html#code-and-results"><i class="fa fa-check"></i><b>6.3.2</b> Code and results</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="lasso.html"><a href="lasso.html#coding-exercise"><i class="fa fa-check"></i><b>6.4</b> Coding exercise</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="trees.html"><a href="trees.html"><i class="fa fa-check"></i><b>7</b> Tree-based methods</a><ul>
<li class="chapter" data-level="7.1" data-path="trees.html"><a href="trees.html#simple-trees"><i class="fa fa-check"></i><b>7.1</b> Simple trees</a><ul>
<li class="chapter" data-level="7.1.1" data-path="trees.html"><a href="trees.html#principle"><i class="fa fa-check"></i><b>7.1.1</b> Principle</a></li>
<li class="chapter" data-level="7.1.2" data-path="trees.html"><a href="trees.html#treeclass"><i class="fa fa-check"></i><b>7.1.2</b> Further details on classification</a></li>
<li class="chapter" data-level="7.1.3" data-path="trees.html"><a href="trees.html#pruning-criteria"><i class="fa fa-check"></i><b>7.1.3</b> Pruning criteria</a></li>
<li class="chapter" data-level="7.1.4" data-path="trees.html"><a href="trees.html#code-and-interpretation"><i class="fa fa-check"></i><b>7.1.4</b> Code and interpretation</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="trees.html"><a href="trees.html#random-forests"><i class="fa fa-check"></i><b>7.2</b> Random forests</a><ul>
<li class="chapter" data-level="7.2.1" data-path="trees.html"><a href="trees.html#principle-1"><i class="fa fa-check"></i><b>7.2.1</b> Principle</a></li>
<li class="chapter" data-level="7.2.2" data-path="trees.html"><a href="trees.html#code-and-results-1"><i class="fa fa-check"></i><b>7.2.2</b> Code and results</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="trees.html"><a href="trees.html#adaboost"><i class="fa fa-check"></i><b>7.3</b> Boosted trees: Adaboost</a><ul>
<li class="chapter" data-level="7.3.1" data-path="trees.html"><a href="trees.html#methodology"><i class="fa fa-check"></i><b>7.3.1</b> Methodology</a></li>
<li class="chapter" data-level="7.3.2" data-path="trees.html"><a href="trees.html#illustration"><i class="fa fa-check"></i><b>7.3.2</b> Illustration</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="trees.html"><a href="trees.html#boosted-trees-extreme-gradient-boosting"><i class="fa fa-check"></i><b>7.4</b> Boosted trees: extreme gradient boosting</a><ul>
<li class="chapter" data-level="7.4.1" data-path="trees.html"><a href="trees.html#managing-loss"><i class="fa fa-check"></i><b>7.4.1</b> Managing Loss</a></li>
<li class="chapter" data-level="7.4.2" data-path="trees.html"><a href="trees.html#penalisation"><i class="fa fa-check"></i><b>7.4.2</b> Penalisation</a></li>
<li class="chapter" data-level="7.4.3" data-path="trees.html"><a href="trees.html#aggregation"><i class="fa fa-check"></i><b>7.4.3</b> Aggregation</a></li>
<li class="chapter" data-level="7.4.4" data-path="trees.html"><a href="trees.html#tree-structure"><i class="fa fa-check"></i><b>7.4.4</b> Tree structure</a></li>
<li class="chapter" data-level="7.4.5" data-path="trees.html"><a href="trees.html#boostext"><i class="fa fa-check"></i><b>7.4.5</b> Extensions</a></li>
<li class="chapter" data-level="7.4.6" data-path="trees.html"><a href="trees.html#boostcode"><i class="fa fa-check"></i><b>7.4.6</b> Code and results</a></li>
<li class="chapter" data-level="7.4.7" data-path="trees.html"><a href="trees.html#instweight"><i class="fa fa-check"></i><b>7.4.7</b> Instance weighting</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="trees.html"><a href="trees.html#discussion"><i class="fa fa-check"></i><b>7.5</b> Discussion</a></li>
<li class="chapter" data-level="7.6" data-path="trees.html"><a href="trees.html#coding-exercises-2"><i class="fa fa-check"></i><b>7.6</b> Coding exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="NN.html"><a href="NN.html"><i class="fa fa-check"></i><b>8</b> Neural networks</a><ul>
<li class="chapter" data-level="8.1" data-path="NN.html"><a href="NN.html#the-original-perceptron"><i class="fa fa-check"></i><b>8.1</b> The original perceptron</a></li>
<li class="chapter" data-level="8.2" data-path="NN.html"><a href="NN.html#multilayer-perceptron-mlp"><i class="fa fa-check"></i><b>8.2</b> Multilayer perceptron (MLP)</a><ul>
<li class="chapter" data-level="8.2.1" data-path="NN.html"><a href="NN.html#introduction-and-notations"><i class="fa fa-check"></i><b>8.2.1</b> Introduction and notations</a></li>
<li class="chapter" data-level="8.2.2" data-path="NN.html"><a href="NN.html#universal-approximation"><i class="fa fa-check"></i><b>8.2.2</b> Universal approximation</a></li>
<li class="chapter" data-level="8.2.3" data-path="NN.html"><a href="NN.html#backprop"><i class="fa fa-check"></i><b>8.2.3</b> Learning via back-propagation</a></li>
<li class="chapter" data-level="8.2.4" data-path="NN.html"><a href="NN.html#further-details-on-classification"><i class="fa fa-check"></i><b>8.2.4</b> Further details on classification</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="NN.html"><a href="NN.html#howdeep"><i class="fa fa-check"></i><b>8.3</b> How deep should we go? And other practical issues</a><ul>
<li class="chapter" data-level="8.3.1" data-path="NN.html"><a href="NN.html#architectural-choices"><i class="fa fa-check"></i><b>8.3.1</b> Architectural choices</a></li>
<li class="chapter" data-level="8.3.2" data-path="NN.html"><a href="NN.html#frequency-of-weight-updates-and-learning-duration"><i class="fa fa-check"></i><b>8.3.2</b> Frequency of weight updates and learning duration</a></li>
<li class="chapter" data-level="8.3.3" data-path="NN.html"><a href="NN.html#penalizations-and-dropout"><i class="fa fa-check"></i><b>8.3.3</b> Penalizations and dropout</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="NN.html"><a href="NN.html#code-samples-and-comments-for-vanilla-mlp"><i class="fa fa-check"></i><b>8.4</b> Code samples and comments for vanilla MLP</a><ul>
<li class="chapter" data-level="8.4.1" data-path="NN.html"><a href="NN.html#regression-example"><i class="fa fa-check"></i><b>8.4.1</b> Regression example</a></li>
<li class="chapter" data-level="8.4.2" data-path="NN.html"><a href="NN.html#classification-example"><i class="fa fa-check"></i><b>8.4.2</b> Classification example</a></li>
<li class="chapter" data-level="8.4.3" data-path="NN.html"><a href="NN.html#custloss"><i class="fa fa-check"></i><b>8.4.3</b> Custom losses</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="NN.html"><a href="NN.html#recurrent-networks"><i class="fa fa-check"></i><b>8.5</b> Recurrent networks</a><ul>
<li class="chapter" data-level="8.5.1" data-path="NN.html"><a href="NN.html#presentation"><i class="fa fa-check"></i><b>8.5.1</b> Presentation</a></li>
<li class="chapter" data-level="8.5.2" data-path="NN.html"><a href="NN.html#code-and-results-2"><i class="fa fa-check"></i><b>8.5.2</b> Code and results</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="NN.html"><a href="NN.html#other-common-architectures"><i class="fa fa-check"></i><b>8.6</b> Other common architectures</a><ul>
<li class="chapter" data-level="8.6.1" data-path="NN.html"><a href="NN.html#generative-aversarial-networks"><i class="fa fa-check"></i><b>8.6.1</b> Generative adversarial networks</a></li>
<li class="chapter" data-level="8.6.2" data-path="NN.html"><a href="NN.html#autoencoders"><i class="fa fa-check"></i><b>8.6.2</b> Auto-encoders</a></li>
<li class="chapter" data-level="8.6.3" data-path="NN.html"><a href="NN.html#a-word-on-convolutional-networks"><i class="fa fa-check"></i><b>8.6.3</b> A word on convolutional networks</a></li>
<li class="chapter" data-level="8.6.4" data-path="NN.html"><a href="NN.html#advanced-architectures"><i class="fa fa-check"></i><b>8.6.4</b> Advanced architectures</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="NN.html"><a href="NN.html#coding-exercise-1"><i class="fa fa-check"></i><b>8.7</b> Coding exercise</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="svm.html"><a href="svm.html"><i class="fa fa-check"></i><b>9</b> Support vector machines</a><ul>
<li class="chapter" data-level="9.1" data-path="svm.html"><a href="svm.html#svm-for-classification"><i class="fa fa-check"></i><b>9.1</b> SVM for classification</a></li>
<li class="chapter" data-level="9.2" data-path="svm.html"><a href="svm.html#svm-for-regression"><i class="fa fa-check"></i><b>9.2</b> SVM for regression</a></li>
<li class="chapter" data-level="9.3" data-path="svm.html"><a href="svm.html#practice"><i class="fa fa-check"></i><b>9.3</b> Practice</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bayes.html"><a href="bayes.html"><i class="fa fa-check"></i><b>10</b> Bayesian methods</a><ul>
<li class="chapter" data-level="10.1" data-path="bayes.html"><a href="bayes.html#the-bayesian-framework"><i class="fa fa-check"></i><b>10.1</b> The Bayesian framework</a></li>
<li class="chapter" data-level="10.2" data-path="bayes.html"><a href="bayes.html#bayesian-sampling"><i class="fa fa-check"></i><b>10.2</b> Bayesian sampling</a><ul>
<li class="chapter" data-level="10.2.1" data-path="bayes.html"><a href="bayes.html#gibbs-sampling"><i class="fa fa-check"></i><b>10.2.1</b> Gibbs sampling</a></li>
<li class="chapter" data-level="10.2.2" data-path="bayes.html"><a href="bayes.html#metropolis-hastings-sampling"><i class="fa fa-check"></i><b>10.2.2</b> Metropolis-Hastings sampling</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="bayes.html"><a href="bayes.html#bayesian-linear-regression"><i class="fa fa-check"></i><b>10.3</b> Bayesian linear regression</a></li>
<li class="chapter" data-level="10.4" data-path="bayes.html"><a href="bayes.html#naive-bayes-classifier"><i class="fa fa-check"></i><b>10.4</b> Naive Bayes classifier</a></li>
<li class="chapter" data-level="10.5" data-path="bayes.html"><a href="bayes.html#BART"><i class="fa fa-check"></i><b>10.5</b> Bayesian additive trees</a><ul>
<li class="chapter" data-level="10.5.1" data-path="bayes.html"><a href="bayes.html#general-formulation"><i class="fa fa-check"></i><b>10.5.1</b> General formulation</a></li>
<li class="chapter" data-level="10.5.2" data-path="bayes.html"><a href="bayes.html#priors"><i class="fa fa-check"></i><b>10.5.2</b> Priors</a></li>
<li class="chapter" data-level="10.5.3" data-path="bayes.html"><a href="bayes.html#sampling-and-predictions"><i class="fa fa-check"></i><b>10.5.3</b> Sampling and predictions</a></li>
<li class="chapter" data-level="10.5.4" data-path="bayes.html"><a href="bayes.html#code"><i class="fa fa-check"></i><b>10.5.4</b> Code</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III From predictions to portfolios</b></span></li>
<li class="chapter" data-level="11" data-path="valtune.html"><a href="valtune.html"><i class="fa fa-check"></i><b>11</b> Validating and tuning</a><ul>
<li class="chapter" data-level="11.1" data-path="valtune.html"><a href="valtune.html#mlmetrics"><i class="fa fa-check"></i><b>11.1</b> Learning metrics</a><ul>
<li class="chapter" data-level="11.1.1" data-path="valtune.html"><a href="valtune.html#regression-analysis"><i class="fa fa-check"></i><b>11.1.1</b> Regression analysis</a></li>
<li class="chapter" data-level="11.1.2" data-path="valtune.html"><a href="valtune.html#classification-analysis"><i class="fa fa-check"></i><b>11.1.2</b> Classification analysis</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="valtune.html"><a href="valtune.html#validation"><i class="fa fa-check"></i><b>11.2</b> Validation</a><ul>
<li class="chapter" data-level="11.2.1" data-path="valtune.html"><a href="valtune.html#the-variance-bias-tradeoff-theory"><i class="fa fa-check"></i><b>11.2.1</b> The variance-bias tradeoff: theory</a></li>
<li class="chapter" data-level="11.2.2" data-path="valtune.html"><a href="valtune.html#the-variance-bias-tradeoff-illustration"><i class="fa fa-check"></i><b>11.2.2</b> The variance-bias tradeoff: illustration</a></li>
<li class="chapter" data-level="11.2.3" data-path="valtune.html"><a href="valtune.html#the-risk-of-overfitting-principle"><i class="fa fa-check"></i><b>11.2.3</b> The risk of overfitting: principle</a></li>
<li class="chapter" data-level="11.2.4" data-path="valtune.html"><a href="valtune.html#the-risk-of-overfitting-some-solutions"><i class="fa fa-check"></i><b>11.2.4</b> The risk of overfitting: some solutions</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="valtune.html"><a href="valtune.html#the-search-for-good-hyperparameters"><i class="fa fa-check"></i><b>11.3</b> The search for good hyperparameters</a><ul>
<li class="chapter" data-level="11.3.1" data-path="valtune.html"><a href="valtune.html#methods"><i class="fa fa-check"></i><b>11.3.1</b> Methods</a></li>
<li class="chapter" data-level="11.3.2" data-path="valtune.html"><a href="valtune.html#example-grid-search"><i class="fa fa-check"></i><b>11.3.2</b> Example: grid search</a></li>
<li class="chapter" data-level="11.3.3" data-path="valtune.html"><a href="valtune.html#example-bayesian-optimization"><i class="fa fa-check"></i><b>11.3.3</b> Example: Bayesian optimization</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="valtune.html"><a href="valtune.html#short-discussion-on-validation-in-backtests"><i class="fa fa-check"></i><b>11.4</b> Short discussion on validation in backtests</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ensemble.html"><a href="ensemble.html"><i class="fa fa-check"></i><b>12</b> Ensemble models</a><ul>
<li class="chapter" data-level="12.1" data-path="ensemble.html"><a href="ensemble.html#linear-ensembles"><i class="fa fa-check"></i><b>12.1</b> Linear ensembles</a><ul>
<li class="chapter" data-level="12.1.1" data-path="ensemble.html"><a href="ensemble.html#principles"><i class="fa fa-check"></i><b>12.1.1</b> Principles</a></li>
<li class="chapter" data-level="12.1.2" data-path="ensemble.html"><a href="ensemble.html#example"><i class="fa fa-check"></i><b>12.1.2</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="ensemble.html"><a href="ensemble.html#stacked-ensembles"><i class="fa fa-check"></i><b>12.2</b> Stacked ensembles</a><ul>
<li class="chapter" data-level="12.2.1" data-path="ensemble.html"><a href="ensemble.html#two-stage-training"><i class="fa fa-check"></i><b>12.2.1</b> Two stage training</a></li>
<li class="chapter" data-level="12.2.2" data-path="ensemble.html"><a href="ensemble.html#code-and-results-3"><i class="fa fa-check"></i><b>12.2.2</b> Code and results</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="ensemble.html"><a href="ensemble.html#extensions-1"><i class="fa fa-check"></i><b>12.3</b> Extensions</a><ul>
<li class="chapter" data-level="12.3.1" data-path="ensemble.html"><a href="ensemble.html#exogenous-variables"><i class="fa fa-check"></i><b>12.3.1</b> Exogenous variables</a></li>
<li class="chapter" data-level="12.3.2" data-path="ensemble.html"><a href="ensemble.html#shrinking-inter-model-correlations"><i class="fa fa-check"></i><b>12.3.2</b> Shrinking inter-model correlations</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="ensemble.html"><a href="ensemble.html#exercise"><i class="fa fa-check"></i><b>12.4</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="backtest.html"><a href="backtest.html"><i class="fa fa-check"></i><b>13</b> Portfolio backtesting</a><ul>
<li class="chapter" data-level="13.1" data-path="backtest.html"><a href="backtest.html#protocol"><i class="fa fa-check"></i><b>13.1</b> Setting the protocol</a></li>
<li class="chapter" data-level="13.2" data-path="backtest.html"><a href="backtest.html#turning-signals-into-portfolio-weights"><i class="fa fa-check"></i><b>13.2</b> Turning signals into portfolio weights</a></li>
<li class="chapter" data-level="13.3" data-path="backtest.html"><a href="backtest.html#perfmet"><i class="fa fa-check"></i><b>13.3</b> Performance metrics</a><ul>
<li class="chapter" data-level="13.3.1" data-path="backtest.html"><a href="backtest.html#discussion-1"><i class="fa fa-check"></i><b>13.3.1</b> Discussion</a></li>
<li class="chapter" data-level="13.3.2" data-path="backtest.html"><a href="backtest.html#pure-performance-and-risk-indicators"><i class="fa fa-check"></i><b>13.3.2</b> Pure performance and risk indicators</a></li>
<li class="chapter" data-level="13.3.3" data-path="backtest.html"><a href="backtest.html#factor-based-evaluation"><i class="fa fa-check"></i><b>13.3.3</b> Factor-based evaluation</a></li>
<li class="chapter" data-level="13.3.4" data-path="backtest.html"><a href="backtest.html#risk-adjusted-measures"><i class="fa fa-check"></i><b>13.3.4</b> Risk-adjusted measures</a></li>
<li class="chapter" data-level="13.3.5" data-path="backtest.html"><a href="backtest.html#transaction-costs-and-turnover"><i class="fa fa-check"></i><b>13.3.5</b> Transaction costs and turnover</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="backtest.html"><a href="backtest.html#common-errors-and-issues"><i class="fa fa-check"></i><b>13.4</b> Common errors and issues</a><ul>
<li class="chapter" data-level="13.4.1" data-path="backtest.html"><a href="backtest.html#forward-looking-data"><i class="fa fa-check"></i><b>13.4.1</b> Forward looking data</a></li>
<li class="chapter" data-level="13.4.2" data-path="backtest.html"><a href="backtest.html#backov"><i class="fa fa-check"></i><b>13.4.2</b> Backtest overfitting</a></li>
<li class="chapter" data-level="13.4.3" data-path="backtest.html"><a href="backtest.html#simple-safeguards"><i class="fa fa-check"></i><b>13.4.3</b> Simple safeguards</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="backtest.html"><a href="backtest.html#implication-of-non-stationarity-forecasting-is-hard"><i class="fa fa-check"></i><b>13.5</b> Implication of non-stationarity: forecasting is hard</a><ul>
<li class="chapter" data-level="13.5.1" data-path="backtest.html"><a href="backtest.html#general-comments"><i class="fa fa-check"></i><b>13.5.1</b> General comments</a></li>
<li class="chapter" data-level="13.5.2" data-path="backtest.html"><a href="backtest.html#the-no-free-lunch-theorem"><i class="fa fa-check"></i><b>13.5.2</b> The no free lunch theorem</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="backtest.html"><a href="backtest.html#first-example-a-complete-backtest"><i class="fa fa-check"></i><b>13.6</b> First example: a complete backtest</a></li>
<li class="chapter" data-level="13.7" data-path="backtest.html"><a href="backtest.html#second-example-backtest-overfitting"><i class="fa fa-check"></i><b>13.7</b> Second example: backtest overfitting</a></li>
<li class="chapter" data-level="13.8" data-path="backtest.html"><a href="backtest.html#coding-exercises-3"><i class="fa fa-check"></i><b>13.8</b> Coding exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Further important topics</b></span></li>
<li class="chapter" data-level="14" data-path="interp.html"><a href="interp.html"><i class="fa fa-check"></i><b>14</b> Interpretability</a><ul>
<li class="chapter" data-level="14.1" data-path="interp.html"><a href="interp.html#global-interpretations"><i class="fa fa-check"></i><b>14.1</b> Global interpretations</a><ul>
<li class="chapter" data-level="14.1.1" data-path="interp.html"><a href="interp.html#surr"><i class="fa fa-check"></i><b>14.1.1</b> Simple models as surrogates</a></li>
<li class="chapter" data-level="14.1.2" data-path="interp.html"><a href="interp.html#variable-importance"><i class="fa fa-check"></i><b>14.1.2</b> Variable importance (tree-based)</a></li>
<li class="chapter" data-level="14.1.3" data-path="interp.html"><a href="interp.html#variable-importance-agnostic"><i class="fa fa-check"></i><b>14.1.3</b> Variable importance (agnostic)</a></li>
<li class="chapter" data-level="14.1.4" data-path="interp.html"><a href="interp.html#partial-dependence-plot"><i class="fa fa-check"></i><b>14.1.4</b> Partial dependence plot</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="interp.html"><a href="interp.html#local-interpretations"><i class="fa fa-check"></i><b>14.2</b> Local interpretations</a><ul>
<li class="chapter" data-level="14.2.1" data-path="interp.html"><a href="interp.html#lime"><i class="fa fa-check"></i><b>14.2.1</b> LIME</a></li>
<li class="chapter" data-level="14.2.2" data-path="interp.html"><a href="interp.html#shapley-values"><i class="fa fa-check"></i><b>14.2.2</b> Shapley values</a></li>
<li class="chapter" data-level="14.2.3" data-path="interp.html"><a href="interp.html#breakdown"><i class="fa fa-check"></i><b>14.2.3</b> Breakdown</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="causality.html"><a href="causality.html"><i class="fa fa-check"></i><b>15</b> Two key concepts: causality and non-stationarity</a><ul>
<li class="chapter" data-level="15.1" data-path="causality.html"><a href="causality.html#causality-1"><i class="fa fa-check"></i><b>15.1</b> Causality</a><ul>
<li class="chapter" data-level="15.1.1" data-path="causality.html"><a href="causality.html#granger"><i class="fa fa-check"></i><b>15.1.1</b> Granger causality</a></li>
<li class="chapter" data-level="15.1.2" data-path="causality.html"><a href="causality.html#causal-additive-models"><i class="fa fa-check"></i><b>15.1.2</b> Causal additive models</a></li>
<li class="chapter" data-level="15.1.3" data-path="causality.html"><a href="causality.html#structural-time-series-models"><i class="fa fa-check"></i><b>15.1.3</b> Structural time-series models</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="causality.html"><a href="causality.html#nonstat"><i class="fa fa-check"></i><b>15.2</b> Dealing with changing environments</a><ul>
<li class="chapter" data-level="15.2.1" data-path="causality.html"><a href="causality.html#non-stationarity-yet-another-illustration"><i class="fa fa-check"></i><b>15.2.1</b> Non-stationarity: yet another illustration</a></li>
<li class="chapter" data-level="15.2.2" data-path="causality.html"><a href="causality.html#online-learning"><i class="fa fa-check"></i><b>15.2.2</b> Online learning</a></li>
<li class="chapter" data-level="15.2.3" data-path="causality.html"><a href="causality.html#homogeneous-transfer-learning"><i class="fa fa-check"></i><b>15.2.3</b> Homogeneous transfer learning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="unsup.html"><a href="unsup.html"><i class="fa fa-check"></i><b>16</b> Unsupervised learning</a><ul>
<li class="chapter" data-level="16.1" data-path="unsup.html"><a href="unsup.html#corpred"><i class="fa fa-check"></i><b>16.1</b> The problem with correlated predictors</a></li>
<li class="chapter" data-level="16.2" data-path="unsup.html"><a href="unsup.html#principal-component-analysis-and-autoencoders"><i class="fa fa-check"></i><b>16.2</b> Principal component analysis and autoencoders</a><ul>
<li class="chapter" data-level="16.2.1" data-path="unsup.html"><a href="unsup.html#a-bit-of-algebra"><i class="fa fa-check"></i><b>16.2.1</b> A bit of algebra</a></li>
<li class="chapter" data-level="16.2.2" data-path="unsup.html"><a href="unsup.html#pca"><i class="fa fa-check"></i><b>16.2.2</b> PCA</a></li>
<li class="chapter" data-level="16.2.3" data-path="unsup.html"><a href="unsup.html#ae"><i class="fa fa-check"></i><b>16.2.3</b> Autoencoders</a></li>
<li class="chapter" data-level="16.2.4" data-path="unsup.html"><a href="unsup.html#application"><i class="fa fa-check"></i><b>16.2.4</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="unsup.html"><a href="unsup.html#clustering-via-k-means"><i class="fa fa-check"></i><b>16.3</b> Clustering via k-means</a></li>
<li class="chapter" data-level="16.4" data-path="unsup.html"><a href="unsup.html#nearest-neighbors"><i class="fa fa-check"></i><b>16.4</b> Nearest neighbors</a></li>
<li class="chapter" data-level="16.5" data-path="unsup.html"><a href="unsup.html#coding-exercise-2"><i class="fa fa-check"></i><b>16.5</b> Coding exercise</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="RL.html"><a href="RL.html"><i class="fa fa-check"></i><b>17</b> Reinforcement learning</a><ul>
<li class="chapter" data-level="17.1" data-path="RL.html"><a href="RL.html#theoretical-layout"><i class="fa fa-check"></i><b>17.1</b> Theoretical layout</a><ul>
<li class="chapter" data-level="17.1.1" data-path="RL.html"><a href="RL.html#general-framework"><i class="fa fa-check"></i><b>17.1.1</b> General framework</a></li>
<li class="chapter" data-level="17.1.2" data-path="RL.html"><a href="RL.html#q-learning"><i class="fa fa-check"></i><b>17.1.2</b> Q-learning</a></li>
<li class="chapter" data-level="17.1.3" data-path="RL.html"><a href="RL.html#sarsa"><i class="fa fa-check"></i><b>17.1.3</b> SARSA</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="RL.html"><a href="RL.html#the-curse-of-dimensionality"><i class="fa fa-check"></i><b>17.2</b> The curse of dimensionality</a></li>
<li class="chapter" data-level="17.3" data-path="RL.html"><a href="RL.html#policy-gradient"><i class="fa fa-check"></i><b>17.3</b> Policy gradient</a><ul>
<li class="chapter" data-level="17.3.1" data-path="RL.html"><a href="RL.html#principle-2"><i class="fa fa-check"></i><b>17.3.1</b> Principle</a></li>
<li class="chapter" data-level="17.3.2" data-path="RL.html"><a href="RL.html#extensions-2"><i class="fa fa-check"></i><b>17.3.2</b> Extensions</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="RL.html"><a href="RL.html#simple-examples"><i class="fa fa-check"></i><b>17.4</b> Simple examples</a><ul>
<li class="chapter" data-level="17.4.1" data-path="RL.html"><a href="RL.html#q-learning-with-simulations"><i class="fa fa-check"></i><b>17.4.1</b> Q-learning with simulations</a></li>
<li class="chapter" data-level="17.4.2" data-path="RL.html"><a href="RL.html#RLemp2"><i class="fa fa-check"></i><b>17.4.2</b> Q-learning with market data</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="RL.html"><a href="RL.html#concluding-remarks"><i class="fa fa-check"></i><b>17.5</b> Concluding remarks</a></li>
<li class="chapter" data-level="17.6" data-path="RL.html"><a href="RL.html#exercises"><i class="fa fa-check"></i><b>17.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Appendix</b></span></li>
<li class="chapter" data-level="18" data-path="data-description.html"><a href="data-description.html"><i class="fa fa-check"></i><b>18</b> Data Description</a></li>
<li class="chapter" data-level="19" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html"><i class="fa fa-check"></i><b>19</b> Solution to exercises</a><ul>
<li class="chapter" data-level="19.1" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-4"><i class="fa fa-check"></i><b>19.1</b> Chapter 4</a></li>
<li class="chapter" data-level="19.2" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-5"><i class="fa fa-check"></i><b>19.2</b> Chapter 5</a></li>
<li class="chapter" data-level="19.3" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-6"><i class="fa fa-check"></i><b>19.3</b> Chapter 6</a></li>
<li class="chapter" data-level="19.4" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-7"><i class="fa fa-check"></i><b>19.4</b> Chapter 7</a></li>
<li class="chapter" data-level="19.5" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-8-the-autoencoder-model"><i class="fa fa-check"></i><b>19.5</b> Chapter 8: the autoencoder model</a></li>
<li class="chapter" data-level="19.6" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-9"><i class="fa fa-check"></i><b>19.6</b> Chapter 9</a></li>
<li class="chapter" data-level="19.7" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-12-ensemble-neural-network"><i class="fa fa-check"></i><b>19.7</b> Chapter 12: ensemble neural network</a></li>
<li class="chapter" data-level="19.8" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-13"><i class="fa fa-check"></i><b>19.8</b> Chapter 13</a><ul>
<li class="chapter" data-level="19.8.1" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#ew-portfolios-with-the-tidyverse"><i class="fa fa-check"></i><b>19.8.1</b> EW portfolios with the tidyverse</a></li>
<li class="chapter" data-level="19.8.2" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#advanced-weighting-function"><i class="fa fa-check"></i><b>19.8.2</b> Advanced weighting function</a></li>
<li class="chapter" data-level="19.8.3" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#functional-programming-in-the-backtest"><i class="fa fa-check"></i><b>19.8.3</b> Functional programming in the backtest</a></li>
</ul></li>
<li class="chapter" data-level="19.9" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-16"><i class="fa fa-check"></i><b>19.9</b> Chapter 16</a></li>
<li class="chapter" data-level="19.10" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-17"><i class="fa fa-check"></i><b>19.10</b> Chapter 17</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning for Factor Investing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="solution-to-exercises" class="section level1">
<h1><span class="header-section-number">Chapter 19</span> Solution to exercises</h1>
<div id="chapter-4" class="section level2">
<h2><span class="header-section-number">19.1</span> Chapter 4</h2>
<p>For annual values:
</p>
<div class="sourceCode" id="cb243"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb243-1" data-line-number="1">data_ml <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb243-2" data-line-number="2"><span class="st">    </span><span class="kw">group_by</span>(date) <span class="op">%&gt;%</span><span class="st">                                            </span></a>
<a class="sourceLine" id="cb243-3" data-line-number="3"><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">growth =</span> Pb <span class="op">&gt;</span><span class="st"> </span><span class="kw">median</span>(Pb)) <span class="op">%&gt;%</span><span class="st">            </span><span class="co"># Creates the sort</span></a>
<a class="sourceLine" id="cb243-4" data-line-number="4"><span class="st">    </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span><span class="st">                                   </span><span class="co"># Ungroup</span></a>
<a class="sourceLine" id="cb243-5" data-line-number="5"><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">year =</span> lubridate<span class="op">::</span><span class="kw">year</span>(date)) <span class="op">%&gt;%</span><span class="st">        </span><span class="co"># Creates a year variable</span></a>
<a class="sourceLine" id="cb243-6" data-line-number="6"><span class="st">    </span><span class="kw">group_by</span>(year, growth) <span class="op">%&gt;%</span><span class="st">                      </span><span class="co"># Analyze by year &amp; sort</span></a>
<a class="sourceLine" id="cb243-7" data-line-number="7"><span class="st">    </span><span class="kw">summarize</span>(<span class="dt">ret =</span> <span class="kw">mean</span>(R1M_Usd)) <span class="op">%&gt;%</span><span class="st">              </span><span class="co"># Compute average return</span></a>
<a class="sourceLine" id="cb243-8" data-line-number="8"><span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> year, <span class="dt">y =</span> ret, <span class="dt">fill =</span> growth)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_col</span>(<span class="dt">position =</span> <span class="st">&quot;dodge&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="co"># Plot!</span></a>
<a class="sourceLine" id="cb243-9" data-line-number="9"><span class="st">    </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="kw">c</span>(<span class="fl">0.7</span>, <span class="fl">0.8</span>))</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:ex41b"></span>
<img src="ML_factor_files/figure-html/ex41b-1.png" alt="The value factor: annual returns." width="400px" height="150px" />
<p class="caption">
FIGURE 19.1: The value factor: annual returns.
</p>
</div>
<p></p>
<p>For monthly values:
</p>
<div class="sourceCode" id="cb244"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb244-1" data-line-number="1">returns_m &lt;-<span class="st"> </span>data_ml <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb244-2" data-line-number="2"><span class="st">    </span><span class="kw">group_by</span>(date) <span class="op">%&gt;%</span><span class="st">                                            </span></a>
<a class="sourceLine" id="cb244-3" data-line-number="3"><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">growth =</span> Pb <span class="op">&gt;</span><span class="st"> </span><span class="kw">median</span>(Pb)) <span class="op">%&gt;%</span><span class="st">                         </span><span class="co"># Creates the sort</span></a>
<a class="sourceLine" id="cb244-4" data-line-number="4"><span class="st">    </span><span class="kw">group_by</span>(date, growth) <span class="op">%&gt;%</span><span class="st">                                   </span><span class="co"># Analyze by date &amp; sort</span></a>
<a class="sourceLine" id="cb244-5" data-line-number="5"><span class="st">    </span><span class="kw">summarize</span>(<span class="dt">ret =</span> <span class="kw">mean</span>(R1M_Usd)) <span class="op">%&gt;%</span><span class="st">                           </span><span class="co"># Compute average return</span></a>
<a class="sourceLine" id="cb244-6" data-line-number="6"><span class="st">    </span><span class="kw">spread</span>(<span class="dt">key =</span> growth, <span class="dt">value =</span> ret) <span class="op">%&gt;%</span><span class="st">                        </span><span class="co"># Pivot to wide matrix format</span></a>
<a class="sourceLine" id="cb244-7" data-line-number="7"><span class="st">    </span><span class="kw">ungroup</span>()</a>
<a class="sourceLine" id="cb244-8" data-line-number="8"><span class="kw">colnames</span>(returns_m)[<span class="dv">2</span><span class="op">:</span><span class="dv">3</span>] &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;value&quot;</span>, <span class="st">&quot;growth&quot;</span>)                 <span class="co"># Changing column names</span></a>
<a class="sourceLine" id="cb244-9" data-line-number="9">returns_m <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb244-10" data-line-number="10"><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">value =</span> <span class="kw">cumprod</span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>value),                           <span class="co"># From returns to portf. values</span></a>
<a class="sourceLine" id="cb244-11" data-line-number="11">           <span class="dt">growth =</span> <span class="kw">cumprod</span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>growth)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb244-12" data-line-number="12"><span class="st">    </span><span class="kw">gather</span>(<span class="dt">key =</span> portfolio, <span class="dt">value =</span> value, <span class="op">-</span>date) <span class="op">%&gt;%</span><span class="st">            </span><span class="co"># Back in tidy format</span></a>
<a class="sourceLine" id="cb244-13" data-line-number="13"><span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> date, <span class="dt">y =</span> value, <span class="dt">color =</span> portfolio)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st">     </span><span class="co"># Plot!  </span></a>
<a class="sourceLine" id="cb244-14" data-line-number="14"><span class="st">    </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="kw">c</span>(<span class="fl">0.7</span>, <span class="fl">0.8</span>))</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:ex41"></span>
<img src="ML_factor_files/figure-html/ex41-1.png" alt="The value factor: portfolio values." width="400px" height="150px" />
<p class="caption">
FIGURE 19.2: The value factor: portfolio values.
</p>
</div>
<p></p>
<p>Portfolios based on quartiles, using the tidyverse only. We rely heavily on the fact that features are uniforimized, i.e., that their distribution is uniform for each given date. Overall, small firms outperform heavily.</p>

<div class="sourceCode" id="cb245"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb245-1" data-line-number="1">data_ml <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb245-2" data-line-number="2"><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">small =</span> Mkt_Cap_6M_Usd <span class="op">&lt;=</span><span class="st"> </span><span class="fl">0.25</span>,                        <span class="co"># Small firms...</span></a>
<a class="sourceLine" id="cb245-3" data-line-number="3">           <span class="dt">medium =</span> Mkt_Cap_6M_Usd <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.25</span> <span class="op">&amp;</span><span class="st"> </span>Mkt_Cap_6M_Usd <span class="op">&lt;=</span><span class="st"> </span><span class="fl">0.5</span>, </a>
<a class="sourceLine" id="cb245-4" data-line-number="4">           <span class="dt">large =</span> Mkt_Cap_6M_Usd <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span> <span class="op">&amp;</span><span class="st"> </span>Mkt_Cap_6M_Usd <span class="op">&lt;=</span><span class="st"> </span><span class="fl">0.75</span>,</a>
<a class="sourceLine" id="cb245-5" data-line-number="5">           <span class="dt">xl =</span> Mkt_Cap_6M_Usd <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.75</span>,                            <span class="co"># ...Xlarge firms</span></a>
<a class="sourceLine" id="cb245-6" data-line-number="6">           <span class="dt">year =</span> <span class="kw">year</span>(date)) <span class="op">%&gt;%</span><span class="st">                        </span></a>
<a class="sourceLine" id="cb245-7" data-line-number="7"><span class="st">    </span><span class="kw">group_by</span>(year) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb245-8" data-line-number="8"><span class="st">    </span><span class="kw">summarize</span>(<span class="dt">small =</span> <span class="kw">mean</span>(small <span class="op">*</span><span class="st"> </span>R1M_Usd),                      <span class="co"># Compute avg returns</span></a>
<a class="sourceLine" id="cb245-9" data-line-number="9">              <span class="dt">medium =</span> <span class="kw">mean</span>(medium <span class="op">*</span><span class="st"> </span>R1M_Usd),</a>
<a class="sourceLine" id="cb245-10" data-line-number="10">              <span class="dt">large =</span> <span class="kw">mean</span>(large <span class="op">*</span><span class="st"> </span>R1M_Usd),</a>
<a class="sourceLine" id="cb245-11" data-line-number="11">              <span class="dt">xl =</span> <span class="kw">mean</span>(xl <span class="op">*</span><span class="st"> </span>R1M_Usd)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb245-12" data-line-number="12"><span class="st">    </span><span class="kw">gather</span>(<span class="dt">key =</span> size, <span class="dt">value =</span> return, <span class="op">-</span>year) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb245-13" data-line-number="13"><span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> year, <span class="dt">y =</span> return, <span class="dt">fill =</span> size)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_col</span>(<span class="dt">position =</span> <span class="st">&quot;dodge&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:ex43"></span>
<img src="ML_factor_files/figure-html/ex43-1.png" alt="The value factor: portfolio values." width="432" />
<p class="caption">
FIGURE 19.3: The value factor: portfolio values.
</p>
</div>
<p></p>
</div>
<div id="chapter-5" class="section level2">
<h2><span class="header-section-number">19.2</span> Chapter 5</h2>
<p>Below, we import a credit spread supplied by Bank of America. Its symbol/ticker is BAMLC0A0CM. We apply the data expansion on the small number of predictors to save memory space. One important trick that should not be overlooked is the uniformization step after the product <a href="Data.html#eq:macrocond">(5.3)</a> is computed. Indeed, we want the new features to have the same properties as the old ones. If we skip this step, distributions will be altered, as we show in one example below.</p>
<p>We start with the data extraction and joining. Its important to join early so as to keep the highest data frequency (daily) in order to replace missing points with <strong>close values</strong>. Joining with monthly data before replacing creates unnecessary lags.</p>

<div class="sourceCode" id="cb246"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb246-1" data-line-number="1"><span class="kw">getSymbols.FRED</span>(<span class="st">&quot;BAMLC0A0CM&quot;</span>,                                    <span class="co"># Extract data</span></a>
<a class="sourceLine" id="cb246-2" data-line-number="2">                <span class="dt">env =</span> <span class="st">&quot;.GlobalEnv&quot;</span>, </a>
<a class="sourceLine" id="cb246-3" data-line-number="3">                <span class="dt">return.class =</span> <span class="st">&quot;xts&quot;</span>)</a></code></pre></div>
<pre><code>## [1] &quot;BAMLC0A0CM&quot;</code></pre>
<div class="sourceCode" id="cb248"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb248-1" data-line-number="1">cred_spread &lt;-<span class="st"> </span><span class="kw">fortify</span>(BAMLC0A0CM)                               <span class="co"># Transform to dataframe</span></a>
<a class="sourceLine" id="cb248-2" data-line-number="2"><span class="kw">colnames</span>(cred_spread) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;date&quot;</span>, <span class="st">&quot;spread&quot;</span>)                     <span class="co"># Change column name</span></a>
<a class="sourceLine" id="cb248-3" data-line-number="3">cred_spread &lt;-<span class="st"> </span>cred_spread <span class="op">%&gt;%</span><span class="st">                                   </span><span class="co"># Take extraction and...</span></a>
<a class="sourceLine" id="cb248-4" data-line-number="4"><span class="st">    </span><span class="kw">full_join</span>(data_ml <span class="op">%&gt;%</span><span class="st"> </span>dplyr<span class="op">::</span><span class="kw">select</span>(date), <span class="dt">by =</span> <span class="st">&quot;date&quot;</span>) <span class="op">%&gt;%</span><span class="st">  </span><span class="co"># Join!</span></a>
<a class="sourceLine" id="cb248-5" data-line-number="5"><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">spread =</span> <span class="kw">na.locf</span>(spread))                             <span class="co"># Replace NA by previous</span></a>
<a class="sourceLine" id="cb248-6" data-line-number="6">cred_spread &lt;-<span class="st"> </span>cred_spread[<span class="op">!</span><span class="kw">duplicated</span>(cred_spread),]            <span class="co"># Remove duplicates</span></a></code></pre></div>
<p></p>
<p>The creation of the augmented dataset requires some manipulation.</p>

<div class="sourceCode" id="cb249"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb249-1" data-line-number="1">data_cond &lt;-<span class="st"> </span>data_ml <span class="op">%&gt;%</span><span class="st">                                    </span><span class="co"># Create new dataset</span></a>
<a class="sourceLine" id="cb249-2" data-line-number="2"><span class="st">    </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="kw">c</span>(<span class="st">&quot;stock_id&quot;</span>, <span class="st">&quot;date&quot;</span>, features_short))</a>
<a class="sourceLine" id="cb249-3" data-line-number="3">names_cred_spread &lt;-<span class="st"> </span><span class="kw">paste0</span>(features_short, <span class="st">&quot;_cred_spread&quot;</span>) <span class="co"># New column names</span></a>
<a class="sourceLine" id="cb249-4" data-line-number="4">feat_cred_spread &lt;-<span class="st"> </span>data_cond <span class="op">%&gt;%</span><span class="st">                           </span><span class="co"># Old values</span></a>
<a class="sourceLine" id="cb249-5" data-line-number="5"><span class="st">    </span>dplyr<span class="op">::</span><span class="kw">select</span>(features_short)</a>
<a class="sourceLine" id="cb249-6" data-line-number="6">cred_spread &lt;-<span class="st"> </span>data_ml <span class="op">%&gt;%</span><span class="st">                                  </span><span class="co"># Create vector of spreads</span></a>
<a class="sourceLine" id="cb249-7" data-line-number="7"><span class="st">    </span>dplyr<span class="op">::</span><span class="kw">select</span>(date) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb249-8" data-line-number="8"><span class="st">    </span><span class="kw">left_join</span>(cred_spread, <span class="dt">by =</span> <span class="st">&quot;date&quot;</span>) </a>
<a class="sourceLine" id="cb249-9" data-line-number="9">feat_cred_spread &lt;-<span class="st"> </span>feat_cred_spread <span class="op">*</span><span class="st">                      </span><span class="co"># This product creates...</span></a>
<a class="sourceLine" id="cb249-10" data-line-number="10"><span class="st">    </span><span class="kw">matrix</span>(cred_spread<span class="op">$</span>spread,                              <span class="co"># the new values...</span></a>
<a class="sourceLine" id="cb249-11" data-line-number="11">           <span class="kw">length</span>(cred_spread<span class="op">$</span>spread),                      <span class="co"># using duplicated...</span></a>
<a class="sourceLine" id="cb249-12" data-line-number="12">           <span class="kw">length</span>(features_short))                          <span class="co"># columns</span></a>
<a class="sourceLine" id="cb249-13" data-line-number="13"><span class="kw">colnames</span>(feat_cred_spread) &lt;-<span class="st"> </span>names_cred_spread             <span class="co"># New column names</span></a>
<a class="sourceLine" id="cb249-14" data-line-number="14">data_cond &lt;-<span class="st"> </span><span class="kw">bind_cols</span>(data_cond, feat_cred_spread)         <span class="co"># Aggregate old &amp; new</span></a>
<a class="sourceLine" id="cb249-15" data-line-number="15">data_cond <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Eps_cred_spread)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_histogram</span>() <span class="co"># Plot example</span></a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:ex5b"></span>
<img src="ML_factor_files/figure-html/ex5b-1.png" alt="Distribution of Eps after conditioning." width="400px" height="150px" />
<p class="caption">
FIGURE 19.4: Distribution of Eps after conditioning.
</p>
</div>
<p></p>
<p>To prevent this issue, uniformization is required.</p>

<div class="sourceCode" id="cb250"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb250-1" data-line-number="1">data_cond &lt;-<span class="st"> </span>data_cond <span class="op">%&gt;%</span><span class="st">                   </span><span class="co"># From new dataset</span></a>
<a class="sourceLine" id="cb250-2" data-line-number="2"><span class="st">    </span><span class="kw">group_by</span>(date) <span class="op">%&gt;%</span><span class="st">                       </span><span class="co"># Group by date and...</span></a>
<a class="sourceLine" id="cb250-3" data-line-number="3"><span class="st">    </span><span class="kw">mutate_at</span>(names_cred_spread, norm_unif)  <span class="co"># Uniformize the new features</span></a>
<a class="sourceLine" id="cb250-4" data-line-number="4">data_cond <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Eps_cred_spread)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_histogram</span>(<span class="dt">bins =</span> <span class="dv">100</span>) <span class="co"># Verification</span></a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:ex51c"></span>
<img src="ML_factor_files/figure-html/ex51c-1.png" alt="Distribution of uniformized conditioned feature values." width="400px" height="150px" />
<p class="caption">
FIGURE 19.5: Distribution of uniformized conditioned feature values.
</p>
</div>
<p></p>
<p>The second question naturally requires the downloading of VIX series first and the joining with the original data.</p>

<div class="sourceCode" id="cb251"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb251-1" data-line-number="1"><span class="kw">getSymbols.FRED</span>(<span class="st">&quot;VIXCLS&quot;</span>,                           <span class="co"># Extract data</span></a>
<a class="sourceLine" id="cb251-2" data-line-number="2">                <span class="dt">env =</span> <span class="st">&quot;.GlobalEnv&quot;</span>, </a>
<a class="sourceLine" id="cb251-3" data-line-number="3">                <span class="dt">return.class =</span> <span class="st">&quot;xts&quot;</span>)</a></code></pre></div>
<pre><code>## [1] &quot;VIXCLS&quot;</code></pre>
<div class="sourceCode" id="cb253"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb253-1" data-line-number="1">vix &lt;-<span class="st"> </span><span class="kw">fortify</span>(VIXCLS)                              <span class="co"># Transform to dataframe</span></a>
<a class="sourceLine" id="cb253-2" data-line-number="2"><span class="kw">colnames</span>(vix) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;date&quot;</span>, <span class="st">&quot;vix&quot;</span>)                   <span class="co"># Change column name</span></a>
<a class="sourceLine" id="cb253-3" data-line-number="3">vix &lt;-<span class="st"> </span>vix <span class="op">%&gt;%</span><span class="st">                                      </span><span class="co"># Take extraction and...</span></a>
<a class="sourceLine" id="cb253-4" data-line-number="4"><span class="st">    </span><span class="kw">full_join</span>(data_ml <span class="op">%&gt;%</span><span class="st"> </span>dplyr<span class="op">::</span><span class="kw">select</span>(date), <span class="dt">by =</span> <span class="st">&quot;date&quot;</span>) <span class="op">%&gt;%</span><span class="st">    </span><span class="co"># Join! </span></a>
<a class="sourceLine" id="cb253-5" data-line-number="5"><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">vix =</span> <span class="kw">na.locf</span>(vix))                      <span class="co"># Replace NA by previous</span></a>
<a class="sourceLine" id="cb253-6" data-line-number="6">vix &lt;-<span class="st"> </span>vix[<span class="op">!</span><span class="kw">duplicated</span>(vix),]                       <span class="co"># Remove duplicates</span></a>
<a class="sourceLine" id="cb253-7" data-line-number="7">vix &lt;-<span class="st"> </span>data_ml <span class="op">%&gt;%</span><span class="st">                                  </span><span class="co"># Keep original data format</span></a>
<a class="sourceLine" id="cb253-8" data-line-number="8"><span class="st">    </span>dplyr<span class="op">::</span><span class="kw">select</span>(date) <span class="op">%&gt;%</span><span class="st">                         </span><span class="co"># ...</span></a>
<a class="sourceLine" id="cb253-9" data-line-number="9"><span class="st">    </span><span class="kw">left_join</span>(vix, <span class="dt">by =</span> <span class="st">&quot;date&quot;</span>)                     <span class="co"># Via left_join()</span></a></code></pre></div>
<p></p>
<p>We can then proceed with the categorization. We create the vector label in a new (smaller) dataset but not attached to the large data_ml variable. Also, we check the balance of labels and its evolution through time.</p>

<div class="sourceCode" id="cb254"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb254-1" data-line-number="1">delta &lt;-<span class="st"> </span><span class="fl">0.5</span>                                       <span class="co"># Magnitude of vix correction</span></a>
<a class="sourceLine" id="cb254-2" data-line-number="2">vix_bar &lt;-<span class="st"> </span><span class="kw">median</span>(vix<span class="op">$</span>vix)                         <span class="co"># Median of vix</span></a>
<a class="sourceLine" id="cb254-3" data-line-number="3">data_vix &lt;-<span class="st"> </span>data_ml <span class="op">%&gt;%</span><span class="st">                            </span><span class="co"># Smaller dataset</span></a>
<a class="sourceLine" id="cb254-4" data-line-number="4"><span class="st">    </span>dplyr<span class="op">::</span><span class="kw">select</span>(stock_id, date, R1M_Usd) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb254-5" data-line-number="5"><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">r_minus =</span> (<span class="op">-</span><span class="fl">0.02</span>) <span class="op">*</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>delta<span class="op">*</span>(vix<span class="op">$</span>vix<span class="op">-</span>vix_bar)),  <span class="co"># r_-</span></a>
<a class="sourceLine" id="cb254-6" data-line-number="6">           <span class="dt">r_plus =</span> <span class="fl">0.02</span> <span class="op">*</span><span class="st"> </span><span class="kw">exp</span>(delta<span class="op">*</span>(vix<span class="op">$</span>vix<span class="op">-</span>vix_bar)))       <span class="co"># r_+</span></a>
<a class="sourceLine" id="cb254-7" data-line-number="7">data_vix &lt;-<span class="st"> </span>data_vix <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb254-8" data-line-number="8"><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">R1M_Usd_Cvix =</span> <span class="kw">if_else</span>(R1M_Usd <span class="op">&lt;</span><span class="st"> </span>r_minus, <span class="dv">-1</span>,       <span class="co"># New label!</span></a>
<a class="sourceLine" id="cb254-9" data-line-number="9">                                  <span class="kw">if_else</span>(R1M_Usd <span class="op">&gt;</span><span class="st"> </span>r_plus, <span class="dv">1</span>,<span class="dv">0</span>)),</a>
<a class="sourceLine" id="cb254-10" data-line-number="10">           <span class="dt">R1M_Usd_Cvix =</span> <span class="kw">as.factor</span>(R1M_Usd_Cvix))</a>
<a class="sourceLine" id="cb254-11" data-line-number="11">data_vix <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb254-12" data-line-number="12"><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">year =</span> <span class="kw">year</span>(date)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb254-13" data-line-number="13"><span class="st">    </span><span class="kw">group_by</span>(year, R1M_Usd_Cvix) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb254-14" data-line-number="14"><span class="st">    </span><span class="kw">summarize</span>(<span class="dt">nb =</span> <span class="kw">n</span>()) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb254-15" data-line-number="15"><span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> year, <span class="dt">y =</span> nb, <span class="dt">fill =</span> R1M_Usd_Cvix)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_col</span>()</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:ex52b"></span>
<img src="ML_factor_files/figure-html/ex52b-1.png" alt="Evolution of categories through time." width="400px" height="150px" />
<p class="caption">
FIGURE 19.6: Evolution of categories through time.
</p>
</div>
<p></p>
<p>Finally, we switch to the outliers.
</p>
<div class="sourceCode" id="cb255"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb255-1" data-line-number="1">data_ml <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb255-2" data-line-number="2"><span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> R12M_Usd)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_histogram</span>()</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:ex53a"></span>
<img src="ML_factor_files/figure-html/ex53a-1.png" alt="Outliers in the dependent variable." width="400px" height="150px" />
<p class="caption">
FIGURE 19.7: Outliers in the dependent variable.
</p>
</div>
<p></p>
<p>Returns above 50 should indeed be rare.</p>

<div class="sourceCode" id="cb256"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb256-1" data-line-number="1">data_ml <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(R12M_Usd <span class="op">&gt;</span><span class="st"> </span><span class="dv">50</span>) <span class="op">%&gt;%</span><span class="st"> </span>dplyr<span class="op">::</span><span class="kw">select</span>(stock_id, date, R12M_Usd)</a></code></pre></div>
<pre><code>## # A tibble: 8 x 3
##   stock_id date       R12M_Usd
##      &lt;int&gt; &lt;date&gt;        &lt;dbl&gt;
## 1      212 2000-12-31     53.0
## 2      221 2008-12-31     53.5
## 3      221 2009-01-31     55.2
## 4      221 2009-02-28     54.8
## 5      296 2002-06-30     72.2
## 6      683 2009-02-28     96.0
## 7      683 2009-03-31     64.8
## 8      862 2009-02-28     58.0</code></pre>
<p></p>
<p>The largest return comes from stock #683. Lets have a look at the stream of monthly returns in 2009.</p>

<div class="sourceCode" id="cb258"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb258-1" data-line-number="1">data_ml <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb258-2" data-line-number="2"><span class="st">    </span><span class="kw">filter</span>(stock_id <span class="op">==</span><span class="st"> </span><span class="dv">683</span>, <span class="kw">year</span>(date) <span class="op">==</span><span class="st"> </span><span class="dv">2009</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb258-3" data-line-number="3"><span class="st">    </span>dplyr<span class="op">::</span><span class="kw">select</span>(date, R1M_Usd)</a></code></pre></div>
<pre><code>## # A tibble: 12 x 2
##    date       R1M_Usd
##    &lt;date&gt;       &lt;dbl&gt;
##  1 2009-01-31  -0.625
##  2 2009-02-28   0.472
##  3 2009-03-31   1.44 
##  4 2009-04-30   0.139
##  5 2009-05-31   0.086
##  6 2009-06-30   0.185
##  7 2009-07-31   0.363
##  8 2009-08-31   0.103
##  9 2009-09-30   9.91 
## 10 2009-10-31   0.101
## 11 2009-11-30   0.202
## 12 2009-12-31  -0.251</code></pre>
<p></p>
<p>The returns are all very high. The annual value is plausible. In addition, a quick glance at the Vol1Y values show that the stock is the most volatile of the dataset.</p>
</div>
<div id="chapter-6" class="section level2">
<h2><span class="header-section-number">19.3</span> Chapter 6</h2>
<p>We recycle the training ans testing data variables created in the chapter (coding section notably). In addition, we create a dedicated function and resort to the <em>map2</em>() function from the <em>purrr</em> package.</p>

<div class="sourceCode" id="cb260"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb260-1" data-line-number="1">alpha_seq &lt;-<span class="st"> </span>(<span class="dv">0</span><span class="op">:</span><span class="dv">10</span>)<span class="op">/</span><span class="dv">10</span>                     <span class="co"># Sequence of alpha values</span></a>
<a class="sourceLine" id="cb260-2" data-line-number="2">lambda_seq &lt;-<span class="st"> </span><span class="fl">0.1</span><span class="op">^</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">5</span>)                    <span class="co"># Sequence of lambda values</span></a>
<a class="sourceLine" id="cb260-3" data-line-number="3">pars &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(alpha_seq, lambda_seq) <span class="co"># Exploring all combinations!</span></a>
<a class="sourceLine" id="cb260-4" data-line-number="4">alpha_seq &lt;-<span class="st"> </span>pars[,<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb260-5" data-line-number="5">lambda_seq &lt;-<span class="st"> </span>pars[,<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb260-6" data-line-number="6">lasso_sens &lt;-<span class="st"> </span><span class="cf">function</span>(alpha, lambda, x_train, y_train, x_test, y_test){ <span class="co"># Function</span></a>
<a class="sourceLine" id="cb260-7" data-line-number="7">    fit_temp &lt;-<span class="st"> </span><span class="kw">glmnet</span>(x_train, y_train,                                 <span class="co"># Model</span></a>
<a class="sourceLine" id="cb260-8" data-line-number="8">                       <span class="dt">alpha =</span> alpha, <span class="dt">lambda =</span> lambda)</a>
<a class="sourceLine" id="cb260-9" data-line-number="9">    <span class="kw">return</span>(<span class="kw">sqrt</span>(<span class="kw">mean</span>((<span class="kw">predict</span>(fit_temp, x_test) <span class="op">-</span><span class="st"> </span>y_test)<span class="op">^</span><span class="dv">2</span>)))           <span class="co"># Output</span></a>
<a class="sourceLine" id="cb260-10" data-line-number="10">}</a>
<a class="sourceLine" id="cb260-11" data-line-number="11">rmse_elas &lt;-<span class="st"> </span><span class="kw">map2</span>(alpha_seq, lambda_seq, lasso_sens,                     <span class="co"># Automation</span></a>
<a class="sourceLine" id="cb260-12" data-line-number="12">                  <span class="dt">x_train =</span> x_penalized_train, <span class="dt">y_train =</span> y_penalized_train,</a>
<a class="sourceLine" id="cb260-13" data-line-number="13">                  <span class="dt">x_test =</span> x_penalized_test, <span class="dt">y_test =</span> testing_sample<span class="op">$</span>R1M_Usd)</a>
<a class="sourceLine" id="cb260-14" data-line-number="14"></a>
<a class="sourceLine" id="cb260-15" data-line-number="15"><span class="kw">bind_cols</span>(<span class="dt">alpha =</span> alpha_seq, <span class="dt">lambda =</span> <span class="kw">as.factor</span>(lambda_seq), <span class="dt">rmse =</span> <span class="kw">unlist</span>(rmse_elas)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb260-16" data-line-number="16"><span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> alpha, <span class="dt">y =</span> rmse, <span class="dt">fill =</span> lambda)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_col</span>() <span class="op">+</span><span class="st"> </span><span class="kw">facet_grid</span>(lambda <span class="op">~</span>.) <span class="op">+</span></a>
<a class="sourceLine" id="cb260-17" data-line-number="17"><span class="st">    </span><span class="kw">coord_cartesian</span>(<span class="dt">ylim =</span> <span class="kw">c</span>(<span class="fl">0.19</span>,<span class="fl">0.193</span>))</a></code></pre></div>
<div class="figure"><span id="fig:ex61"></span>
<img src="ML_factor_files/figure-html/ex61-1.png" alt="Performance of elasticnet across parameter values." width="432" />
<p class="caption">
FIGURE 19.8: Performance of elasticnet across parameter values.
</p>
</div>
<p></p>
<p>The parameters have a very marginal impact. Maybe the model is not a good fit for the task.</p>
</div>
<div id="chapter-7" class="section level2">
<h2><span class="header-section-number">19.4</span> Chapter 7</h2>

<div class="sourceCode" id="cb261"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb261-1" data-line-number="1">fit1 &lt;-<span class="st"> </span><span class="kw">rpart</span>(formula, </a>
<a class="sourceLine" id="cb261-2" data-line-number="2">              <span class="dt">data =</span> training_sample,     <span class="co"># Data source: full sample</span></a>
<a class="sourceLine" id="cb261-3" data-line-number="3">              <span class="dt">cp =</span> <span class="fl">0.001</span>)                 <span class="co"># Precision: smaller = more leaves</span></a>
<a class="sourceLine" id="cb261-4" data-line-number="4"><span class="kw">mean</span>((<span class="kw">predict</span>(fit1, testing_sample) <span class="op">-</span><span class="st"> </span>testing_sample<span class="op">$</span>R1M_Usd)<span class="op">^</span><span class="dv">2</span>) </a></code></pre></div>
<pre><code>## [1] 0.04018973</code></pre>
<div class="sourceCode" id="cb263"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb263-1" data-line-number="1">fit2 &lt;-<span class="st"> </span><span class="kw">rpart</span>(formula,</a>
<a class="sourceLine" id="cb263-2" data-line-number="2">              <span class="dt">data =</span> training_sample,     <span class="co"># Data source: full sample</span></a>
<a class="sourceLine" id="cb263-3" data-line-number="3">              <span class="dt">cp =</span> <span class="fl">0.01</span>)                  <span class="co"># Precision: smaller = more leaves</span></a>
<a class="sourceLine" id="cb263-4" data-line-number="4"><span class="kw">mean</span>((<span class="kw">predict</span>(fit2, testing_sample) <span class="op">-</span><span class="st"> </span>testing_sample<span class="op">$</span>R1M_Usd)<span class="op">^</span><span class="dv">2</span>) <span class="co"># Test!</span></a></code></pre></div>
<pre><code>## [1] 0.03699696</code></pre>
<div class="sourceCode" id="cb265"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb265-1" data-line-number="1"><span class="kw">rpart.plot</span>(fit1)                         <span class="co"># Plot the first tree</span></a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:ex71"></span>
<img src="ML_factor_files/figure-html/ex71-1.png" alt="Sample (complex) tree." width="480" />
<p class="caption">
FIGURE 19.9: Sample (complex) tree.
</p>
</div>
<p></p>
<p>The first model is <strong>too</strong> precise: going into the details of the training sample does not translate to good performance out-of-sample. The second, simpler model, yields better results.</p>

<div class="sourceCode" id="cb266"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb266-1" data-line-number="1">n_trees &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">40</span>, <span class="dv">80</span>, <span class="dv">160</span>)</a>
<a class="sourceLine" id="cb266-2" data-line-number="2">mse_RF &lt;-<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb266-3" data-line-number="3"><span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(n_trees)){       <span class="co"># No need for functional programming here...</span></a>
<a class="sourceLine" id="cb266-4" data-line-number="4">    fit_temp &lt;-<span class="st"> </span><span class="kw">randomForest</span>(</a>
<a class="sourceLine" id="cb266-5" data-line-number="5">        <span class="kw">as.formula</span>(<span class="kw">paste</span>(<span class="st">&quot;R1M_Usd ~&quot;</span>, <span class="kw">paste</span>(features_short, <span class="dt">collapse =</span> <span class="st">&quot; + &quot;</span>))),  <span class="co"># New formula!</span></a>
<a class="sourceLine" id="cb266-6" data-line-number="6">        <span class="dt">data =</span> training_sample,    <span class="co"># Data source: training sample</span></a>
<a class="sourceLine" id="cb266-7" data-line-number="7">        <span class="dt">sampsize =</span> <span class="dv">30000</span>,          <span class="co"># Size of (random) sample for each tree</span></a>
<a class="sourceLine" id="cb266-8" data-line-number="8">        <span class="dt">replace =</span> <span class="ot">TRUE</span>,            <span class="co"># Is the sampling done with replacement?</span></a>
<a class="sourceLine" id="cb266-9" data-line-number="9">        <span class="dt">ntree =</span> n_trees[j],        <span class="co"># Nb of random trees</span></a>
<a class="sourceLine" id="cb266-10" data-line-number="10">        <span class="dt">mtry =</span> <span class="dv">5</span>)                  <span class="co"># Nb of predictors for each tree</span></a>
<a class="sourceLine" id="cb266-11" data-line-number="11">    mse_RF[j] &lt;-<span class="st"> </span><span class="kw">mean</span>((<span class="kw">predict</span>(fit_temp, testing_sample) <span class="op">-</span><span class="st"> </span>testing_sample<span class="op">$</span>R1M_Usd)<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb266-12" data-line-number="12">}</a>
<a class="sourceLine" id="cb266-13" data-line-number="13">mse_RF</a></code></pre></div>
<pre><code>## [1] 0.03967754 0.03885924 0.03766900 0.03696370 0.03699772</code></pre>
<p></p>
<p>Trees are by definition random so results can vary from test to test. Overall, large number of trees are preferable and the reason is that each new tree tell a new story and diversifies the risk of the whole forest. Some more technical details of why that may be the case are outlined in the original paper <span class="citation">Breiman (<a href="#ref-breiman2001random">2001</a>)</span>.</p>
<p>For the last exercises, we recycle the <em>formula</em> used in Chapter <a href="trees.html#trees">7</a>.</p>

<div class="sourceCode" id="cb268"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb268-1" data-line-number="1">tree_<span class="dv">2008</span> &lt;-<span class="st"> </span><span class="kw">rpart</span>(formula,</a>
<a class="sourceLine" id="cb268-2" data-line-number="2">                   <span class="dt">data =</span> data_ml <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(<span class="kw">year</span>(date) <span class="op">==</span><span class="st"> </span><span class="dv">2008</span>), <span class="co"># Data source: 2008</span></a>
<a class="sourceLine" id="cb268-3" data-line-number="3">                   <span class="dt">cp =</span> <span class="fl">0.001</span>,</a>
<a class="sourceLine" id="cb268-4" data-line-number="4">                   <span class="dt">maxdepth =</span> <span class="dv">2</span>) </a>
<a class="sourceLine" id="cb268-5" data-line-number="5"><span class="kw">rpart.plot</span>(tree_<span class="dv">2008</span>)</a></code></pre></div>
<p><img src="ML_factor_files/figure-html/ex73a-1.png" width="384" style="display: block; margin: auto;" /></p>
<p></p>
<p>The first splitting criterion is enterprise value (EV). EV is an indicator that adjusts market capitalization by substracting debt and adding cash. It is a more faithful account of the true value of a company. In 2008, the companies that fared the least bad where those with the highest EV (i.e., large, robust firms).</p>

<div class="sourceCode" id="cb269"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb269-1" data-line-number="1">tree_<span class="dv">2009</span> &lt;-<span class="st"> </span><span class="kw">rpart</span>(formula,</a>
<a class="sourceLine" id="cb269-2" data-line-number="2">                   <span class="dt">data =</span> data_ml <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(<span class="kw">year</span>(date) <span class="op">==</span><span class="st"> </span><span class="dv">2009</span>), <span class="co"># Data source: 2009</span></a>
<a class="sourceLine" id="cb269-3" data-line-number="3">                   <span class="dt">cp =</span> <span class="fl">0.001</span>,</a>
<a class="sourceLine" id="cb269-4" data-line-number="4">                   <span class="dt">maxdepth =</span> <span class="dv">2</span>) </a>
<a class="sourceLine" id="cb269-5" data-line-number="5"><span class="kw">rpart.plot</span>(tree_<span class="dv">2009</span>)</a></code></pre></div>
<p><img src="ML_factor_files/figure-html/ex73b-1.png" width="384" style="display: block; margin: auto;" /></p>
<p></p>
<p>In 2009, the firms that recovered the fastest were those that experienced high volatility in the past (likely, downwards volatility). Momentum is also very important: the firms with the lowest past returns are those that rebound the fastest. This is a typical example of the momentum crash phenomenon studied in <span class="citation">Barroso and Santa-Clara (<a href="#ref-barroso2015momentum">2015</a>)</span> and <span class="citation">Daniel and Moskowitz (<a href="#ref-daniel2016momentum">2016</a>)</span>. The rationale is the following: after a market downturn, the stock with the most potential for growth are those that have suffered the largest losses. Consequently, the negative (short) leg of the momentum factor performs very well, often better than the long leg. And indeed, being long in the momentum factor in 2009 would have generated negative profits.</p>
</div>
<div id="chapter-8-the-autoencoder-model" class="section level2">
<h2><span class="header-section-number">19.5</span> Chapter 8: the autoencoder model</h2>
<p>First, it is imperative to format the inputs properly. To avoid any issues, we work with perfectly rectangular data and hence restrict the investment set to the stocks with no missing points. Dimensions must also be in the correct order.</p>

<div class="sourceCode" id="cb270"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb270-1" data-line-number="1">data_short &lt;-<span class="st"> </span>data_ml <span class="op">%&gt;%</span><span class="st">         </span><span class="co"># Shorter dataset</span></a>
<a class="sourceLine" id="cb270-2" data-line-number="2"><span class="st">    </span><span class="kw">filter</span>(stock_id <span class="op">%in%</span><span class="st"> </span>stock_ids_short) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb270-3" data-line-number="3"><span class="st">    </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="kw">c</span>(<span class="st">&quot;stock_id&quot;</span>, <span class="st">&quot;date&quot;</span>,features_short, <span class="st">&quot;R1M_Usd&quot;</span>))</a>
<a class="sourceLine" id="cb270-4" data-line-number="4">dates &lt;-<span class="st"> </span><span class="kw">unique</span>(data_short<span class="op">$</span>date)  <span class="co"># Vector of dates</span></a>
<a class="sourceLine" id="cb270-5" data-line-number="5"></a>
<a class="sourceLine" id="cb270-6" data-line-number="6">N &lt;-<span class="st"> </span><span class="kw">length</span>(stock_ids_short)      <span class="co"># Dimension for assets</span></a>
<a class="sourceLine" id="cb270-7" data-line-number="7">Tt &lt;-<span class="st"> </span><span class="kw">length</span>(dates)               <span class="co"># Dimension for dates</span></a>
<a class="sourceLine" id="cb270-8" data-line-number="8">K &lt;-<span class="st"> </span><span class="kw">length</span>(features_short)       <span class="co"># Dimension for features</span></a>
<a class="sourceLine" id="cb270-9" data-line-number="9"></a>
<a class="sourceLine" id="cb270-10" data-line-number="10">factor_data &lt;-<span class="st"> </span>data_short <span class="op">%&gt;%</span><span class="st">  </span><span class="co"># Factor side date</span></a>
<a class="sourceLine" id="cb270-11" data-line-number="11"><span class="st">    </span>dplyr<span class="op">::</span><span class="kw">select</span>(date, stock_id, R1M_Usd) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb270-12" data-line-number="12"><span class="st">    </span><span class="kw">spread</span>(<span class="dt">key =</span> stock_id, <span class="dt">value =</span> R1M_Usd) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb270-13" data-line-number="13"><span class="st">    </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="op">-</span>date) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb270-14" data-line-number="14"><span class="st">    </span><span class="kw">as.matrix</span>()</a>
<a class="sourceLine" id="cb270-15" data-line-number="15"></a>
<a class="sourceLine" id="cb270-16" data-line-number="16">beta_data &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="kw">unlist</span>(data_short <span class="op">%&gt;%</span><span class="st">  </span><span class="co"># Beta side data: beware the permutation below!</span></a>
<a class="sourceLine" id="cb270-17" data-line-number="17"><span class="st">                              </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="op">-</span>stock_id, <span class="op">-</span>date, <span class="op">-</span>R1M_Usd)), </a>
<a class="sourceLine" id="cb270-18" data-line-number="18">                   <span class="dt">dim =</span> <span class="kw">c</span>(N, Tt, K))</a>
<a class="sourceLine" id="cb270-19" data-line-number="19">beta_data &lt;-<span class="st"> </span><span class="kw">aperm</span>(beta_data, <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">3</span>))   <span class="co"># Permutation</span></a></code></pre></div>
<p></p>
<p>Next, we turn to the specification of the network, using a functional API form.</p>

<div class="sourceCode" id="cb271"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb271-1" data-line-number="1">main_input &lt;-<span class="st"> </span><span class="kw">layer_input</span>(<span class="dt">shape =</span> <span class="kw">c</span>(N), <span class="dt">name =</span> <span class="st">&quot;main_input&quot;</span>)  <span class="co"># Main input: returns      </span></a>
<a class="sourceLine" id="cb271-2" data-line-number="2">factor_network &lt;-<span class="st"> </span>main_input <span class="op">%&gt;%</span><span class="st">                              </span><span class="co"># Def of factor side network</span></a>
<a class="sourceLine" id="cb271-3" data-line-number="3"><span class="st">    </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">8</span>, <span class="dt">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="dt">name =</span> <span class="st">&quot;layer_1_r&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb271-4" data-line-number="4"><span class="st">    </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">4</span>, <span class="dt">activation =</span> <span class="st">&quot;tanh&quot;</span>, <span class="dt">name =</span> <span class="st">&quot;layer_2_r&quot;</span>) </a>
<a class="sourceLine" id="cb271-5" data-line-number="5"></a>
<a class="sourceLine" id="cb271-6" data-line-number="6">aux_input &lt;-<span class="st"> </span><span class="kw">layer_input</span>(<span class="dt">shape =</span> <span class="kw">c</span>(N,K), <span class="dt">name =</span> <span class="st">&quot;aux_input&quot;</span>)  <span class="co"># Aux input: characteristics</span></a>
<a class="sourceLine" id="cb271-7" data-line-number="7">beta_network &lt;-<span class="st"> </span>aux_input <span class="op">%&gt;%</span><span class="st">                                 </span><span class="co"># Def of beta side network</span></a>
<a class="sourceLine" id="cb271-8" data-line-number="8"><span class="st">    </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">8</span>, <span class="dt">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="dt">name =</span> <span class="st">&quot;layer_1_l&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb271-9" data-line-number="9"><span class="st">    </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">4</span>, <span class="dt">activation =</span> <span class="st">&quot;tanh&quot;</span>, <span class="dt">name =</span> <span class="st">&quot;layer_2_l&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb271-10" data-line-number="10"><span class="st">    </span><span class="kw">layer_permute</span>(<span class="dt">dims =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">1</span>), <span class="dt">name =</span> <span class="st">&quot;layer_3_l&quot;</span>)          <span class="co"># Permutation!</span></a>
<a class="sourceLine" id="cb271-11" data-line-number="11"></a>
<a class="sourceLine" id="cb271-12" data-line-number="12">main_output &lt;-<span class="st"> </span><span class="kw">layer_dot</span>(<span class="kw">c</span>(beta_network, factor_network),     <span class="co"># Product of 2 networks</span></a>
<a class="sourceLine" id="cb271-13" data-line-number="13">                         <span class="dt">axes =</span> <span class="dv">1</span>, <span class="dt">name =</span> <span class="st">&quot;main_output&quot;</span>) </a>
<a class="sourceLine" id="cb271-14" data-line-number="14"></a>
<a class="sourceLine" id="cb271-15" data-line-number="15">model_ae &lt;-<span class="st"> </span><span class="kw">keras_model</span>(                                      <span class="co"># AE Model specs</span></a>
<a class="sourceLine" id="cb271-16" data-line-number="16">    <span class="dt">inputs =</span> <span class="kw">c</span>(main_input, aux_input),</a>
<a class="sourceLine" id="cb271-17" data-line-number="17">    <span class="dt">outputs =</span> <span class="kw">c</span>(main_output)</a>
<a class="sourceLine" id="cb271-18" data-line-number="18">)</a></code></pre></div>
<p></p>
<p>Finally, we ask for the structure of the model, and train it.</p>

<div class="sourceCode" id="cb272"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb272-1" data-line-number="1"><span class="kw">summary</span>(model_ae)                      <span class="co"># See model details / architecture</span></a></code></pre></div>
<pre><code>## Model: &quot;model_25&quot;
## __________________________________________________________________________________________
## Layer (type)                 Output Shape        Param #    Connected to                  
## ==========================================================================================
## aux_input (InputLayer)       [(None, 793, 7)]    0                                        
## __________________________________________________________________________________________
## layer_1_l (Dense)            (None, 793, 8)      64         aux_input[0][0]               
## __________________________________________________________________________________________
## main_input (InputLayer)      [(None, 793)]       0                                        
## __________________________________________________________________________________________
## layer_2_l (Dense)            (None, 793, 4)      36         layer_1_l[0][0]               
## __________________________________________________________________________________________
## layer_1_r (Dense)            (None, 8)           6352       main_input[0][0]              
## __________________________________________________________________________________________
## layer_3_l (Permute)          (None, 4, 793)      0          layer_2_l[0][0]               
## __________________________________________________________________________________________
## layer_2_r (Dense)            (None, 4)           36         layer_1_r[0][0]               
## __________________________________________________________________________________________
## main_output (Dot)            (None, 793)         0          layer_3_l[0][0]               
##                                                             layer_2_r[0][0]               
## ==========================================================================================
## Total params: 6,488
## Trainable params: 6,488
## Non-trainable params: 0
## __________________________________________________________________________________________</code></pre>
<div class="sourceCode" id="cb274"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb274-1" data-line-number="1">model_ae <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">compile</span>(                  <span class="co"># Learning parameters</span></a>
<a class="sourceLine" id="cb274-2" data-line-number="2">    <span class="dt">optimizer =</span> <span class="st">&quot;rmsprop&quot;</span>,</a>
<a class="sourceLine" id="cb274-3" data-line-number="3">    <span class="dt">loss =</span> <span class="st">&quot;mse&quot;</span></a>
<a class="sourceLine" id="cb274-4" data-line-number="4">)</a>
<a class="sourceLine" id="cb274-5" data-line-number="5"></a>
<a class="sourceLine" id="cb274-6" data-line-number="6">model_ae <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(                      <span class="co"># Learning function</span></a>
<a class="sourceLine" id="cb274-7" data-line-number="7">    <span class="dt">x =</span> <span class="kw">list</span>(<span class="dt">main_input =</span> factor_data, <span class="dt">aux_input =</span> beta_data),</a>
<a class="sourceLine" id="cb274-8" data-line-number="8">    <span class="dt">y =</span> <span class="kw">list</span>(<span class="dt">main_output =</span> factor_data),</a>
<a class="sourceLine" id="cb274-9" data-line-number="9">    <span class="dt">epochs =</span> <span class="dv">20</span>,                      <span class="co"># Nb rounds</span></a>
<a class="sourceLine" id="cb274-10" data-line-number="10">    <span class="dt">batch_size =</span> <span class="dv">49</span>                   <span class="co"># Nb obs. per round</span></a>
<a class="sourceLine" id="cb274-11" data-line-number="11">)</a></code></pre></div>
<p></p>
</div>
<div id="chapter-9" class="section level2">
<h2><span class="header-section-number">19.6</span> Chapter 9</h2>
<p>Since we are going to reproduce a similar analysis several times, lets simplify the task with 2 tips. First, by using default parameter values that will be passed as common arguments to the <em>svm</em> function. Second, by creating a custom function that computes the MSE. Third, by resorting to functional calculus via the <em>map</em> function from the <em>purrr</em> package. Below, we recycle datasets created in Chapter <a href="trees.html#trees">7</a>.</p>

<div class="sourceCode" id="cb275"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb275-1" data-line-number="1">mse &lt;-<span class="st"> </span><span class="cf">function</span>(fit, features, label){             <span class="co"># MSE function</span></a>
<a class="sourceLine" id="cb275-2" data-line-number="2">    <span class="kw">return</span>(<span class="kw">mean</span>((<span class="kw">predict</span>(fit, features)<span class="op">-</span>label)<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb275-3" data-line-number="3">}</a>
<a class="sourceLine" id="cb275-4" data-line-number="4">par_list &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">y =</span> train_label_xgb[<span class="dv">1</span><span class="op">:</span><span class="dv">10000</span>],     <span class="co"># From Tree chapter</span></a>
<a class="sourceLine" id="cb275-5" data-line-number="5">                 <span class="dt">x =</span> train_features_xgb[<span class="dv">1</span><span class="op">:</span><span class="dv">10000</span>,],</a>
<a class="sourceLine" id="cb275-6" data-line-number="6">                 <span class="dt">type =</span> <span class="st">&quot;eps-regression&quot;</span>,</a>
<a class="sourceLine" id="cb275-7" data-line-number="7">                 <span class="dt">epsilon =</span> <span class="fl">0.1</span>,                    <span class="co"># Width of strip for errors</span></a>
<a class="sourceLine" id="cb275-8" data-line-number="8">                 <span class="dt">gamma =</span> <span class="fl">0.5</span>,                      <span class="co"># Constant in the radial kernel </span></a>
<a class="sourceLine" id="cb275-9" data-line-number="9">                 <span class="dt">cost =</span> <span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb275-10" data-line-number="10">svm_par &lt;-<span class="st"> </span><span class="cf">function</span>(kernel, par_list){             <span class="co"># Function for SVM fit automation</span></a>
<a class="sourceLine" id="cb275-11" data-line-number="11">    <span class="kw">require</span>(e1071)</a>
<a class="sourceLine" id="cb275-12" data-line-number="12">    <span class="kw">return</span>(<span class="kw">do.call</span>(svm, <span class="kw">c</span>(<span class="dt">kernel =</span> kernel, par_list))) </a>
<a class="sourceLine" id="cb275-13" data-line-number="13">}</a>
<a class="sourceLine" id="cb275-14" data-line-number="14">kernels &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;linear&quot;</span>, <span class="st">&quot;radial&quot;</span>, <span class="st">&quot;polynomial&quot;</span>, <span class="st">&quot;sigmoid&quot;</span>) <span class="co"># Kernels</span></a>
<a class="sourceLine" id="cb275-15" data-line-number="15">fit_svm_par &lt;-<span class="st"> </span><span class="kw">map</span>(kernels, svm_par, <span class="dt">par_list =</span> par_list) <span class="co"># SVM models</span></a>
<a class="sourceLine" id="cb275-16" data-line-number="16"><span class="kw">map</span>(fit_svm_par, mse,                                     <span class="co"># MSEs</span></a>
<a class="sourceLine" id="cb275-17" data-line-number="17">    <span class="dt">features =</span> test_feat_short,                           <span class="co"># From SVM chapter </span></a>
<a class="sourceLine" id="cb275-18" data-line-number="18">    <span class="dt">label =</span> testing_sample<span class="op">$</span>R1M_Usd)</a></code></pre></div>
<pre><code>## [[1]]
## [1] 0.03849786
## 
## [[2]]
## [1] 0.03924576
## 
## [[3]]
## [1] 0.03951328
## 
## [[4]]
## [1] 334.8173</code></pre>
<p></p>
<p>The first two kernels yield the best fit while the last one should be avoided. Note that apart from the linear kernel, all other options require parameters. We have used the default ones, which may explain the poor performance of some nonlinear kernels.</p>
<p>Below, we train an SVM model on a training sample with all observations but limited to the 7 major predictors. Even with a smaller number of features, the training is time consuming.</p>

<div class="sourceCode" id="cb277"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb277-1" data-line-number="1">svm_full &lt;-<span class="st"> </span><span class="kw">svm</span>(<span class="dt">y =</span> train_label_xgb,      <span class="co"># Train label</span></a>
<a class="sourceLine" id="cb277-2" data-line-number="2">                <span class="dt">x =</span> train_features_xgb,   <span class="co"># Training features</span></a>
<a class="sourceLine" id="cb277-3" data-line-number="3">                <span class="dt">type =</span> <span class="st">&quot;eps-regression&quot;</span>,  <span class="co"># SVM task type (see LIBSVM documentation)</span></a>
<a class="sourceLine" id="cb277-4" data-line-number="4">                <span class="dt">kernel =</span> <span class="st">&quot;linear&quot;</span>,        <span class="co"># SVM kernel </span></a>
<a class="sourceLine" id="cb277-5" data-line-number="5">                <span class="dt">epsilon =</span> <span class="fl">0.1</span>,            <span class="co"># Width of strip for errors</span></a>
<a class="sourceLine" id="cb277-6" data-line-number="6">                <span class="dt">cost =</span> <span class="fl">0.1</span>)               <span class="co"># Slack variable penalisation</span></a>
<a class="sourceLine" id="cb277-7" data-line-number="7">test_feat_short &lt;-<span class="st"> </span>dplyr<span class="op">::</span><span class="kw">select</span>(testing_sample,features_short)       <span class="co"># Test set</span></a>
<a class="sourceLine" id="cb277-8" data-line-number="8"><span class="kw">mean</span>(<span class="kw">predict</span>(svm_full, test_feat_short) <span class="op">*</span><span class="st"> </span>testing_sample<span class="op">$</span>R1M_Usd <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>) <span class="co"># Hit ratio</span></a></code></pre></div>
<pre><code>## [1] 0.490343</code></pre>
<p></p>
<p>This figure is very low. Below, we test a very simple form of boosted trees, for comparison purposes.</p>

<div class="sourceCode" id="cb279"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb279-1" data-line-number="1">xgb_full &lt;-<span class="st"> </span><span class="kw">xgb.train</span>(<span class="dt">data =</span> train_matrix_xgb,    <span class="co"># Data source </span></a>
<a class="sourceLine" id="cb279-2" data-line-number="2">                      <span class="dt">eta =</span> <span class="fl">0.3</span>,                          <span class="co"># Learning rate</span></a>
<a class="sourceLine" id="cb279-3" data-line-number="3">                      <span class="dt">objective =</span> <span class="st">&quot;reg:linear&quot;</span>,           <span class="co"># Objective function</span></a>
<a class="sourceLine" id="cb279-4" data-line-number="4">                      <span class="dt">max_depth =</span> <span class="dv">4</span>,                      <span class="co"># Maximum depth of trees</span></a>
<a class="sourceLine" id="cb279-5" data-line-number="5">                      <span class="dt">nrounds =</span> <span class="dv">60</span>                        <span class="co"># Number of trees used (bit low here)</span></a>
<a class="sourceLine" id="cb279-6" data-line-number="6">)</a>
<a class="sourceLine" id="cb279-7" data-line-number="7"><span class="kw">mean</span>(<span class="kw">predict</span>(xgb_full, xgb_test) <span class="op">*</span><span class="st"> </span>testing_sample<span class="op">$</span>R1M_Usd <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>) <span class="co"># Hit ratio</span></a></code></pre></div>
<pre><code>## [1] 0.5017377</code></pre>
<p></p>
<p>The forecasts are slightly better, but the computation time is lower. Two reasons why the models perform poorly:<br />
1. there are not enough predictors;<br />
2. the models are static: they do not adjust dynamically to macro conditions.</p>
</div>
<div id="chapter-12-ensemble-neural-network" class="section level2">
<h2><span class="header-section-number">19.7</span> Chapter 12: ensemble neural network</h2>
<p>First, we create the three feature sets. The first set gets all multiples of 3 between 3 and 93. The second one, the same indices, minus one and the third one, the initial indices minus two.</p>

<div class="sourceCode" id="cb281"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb281-1" data-line-number="1">feat_train_<span class="dv">1</span> &lt;-<span class="st"> </span>training_sample <span class="op">%&gt;%</span><span class="st"> </span>dplyr<span class="op">::</span><span class="kw">select</span>(features[<span class="dv">3</span><span class="op">*</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">31</span>)]) <span class="op">%&gt;%</span><span class="st">   </span><span class="co"># First set of feats</span></a>
<a class="sourceLine" id="cb281-2" data-line-number="2"><span class="st">    </span><span class="kw">as.matrix</span>() </a>
<a class="sourceLine" id="cb281-3" data-line-number="3">feat_train_<span class="dv">2</span> &lt;-<span class="st"> </span>training_sample <span class="op">%&gt;%</span><span class="st"> </span>dplyr<span class="op">::</span><span class="kw">select</span>(features[<span class="dv">3</span><span class="op">*</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">31</span>)<span class="op">-</span><span class="dv">1</span>]) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># Second set of feats</span></a>
<a class="sourceLine" id="cb281-4" data-line-number="4"><span class="st">    </span><span class="kw">as.matrix</span>() </a>
<a class="sourceLine" id="cb281-5" data-line-number="5">feat_train_<span class="dv">3</span> &lt;-<span class="st"> </span>training_sample <span class="op">%&gt;%</span><span class="st"> </span>dplyr<span class="op">::</span><span class="kw">select</span>(features[<span class="dv">3</span><span class="op">*</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">31</span>)<span class="op">-</span><span class="dv">2</span>]) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># Third set of feats</span></a>
<a class="sourceLine" id="cb281-6" data-line-number="6"><span class="st">    </span><span class="kw">as.matrix</span>() </a>
<a class="sourceLine" id="cb281-7" data-line-number="7">feat_test_<span class="dv">1</span> &lt;-<span class="st"> </span>testing_sample <span class="op">%&gt;%</span><span class="st"> </span>dplyr<span class="op">::</span><span class="kw">select</span>(features[<span class="dv">3</span><span class="op">*</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">31</span>)]) <span class="op">%&gt;%</span><span class="st">     </span><span class="co"># Test features 1</span></a>
<a class="sourceLine" id="cb281-8" data-line-number="8"><span class="st">    </span><span class="kw">as.matrix</span>() </a>
<a class="sourceLine" id="cb281-9" data-line-number="9">feat_test_<span class="dv">2</span> &lt;-<span class="st"> </span>testing_sample <span class="op">%&gt;%</span><span class="st"> </span>dplyr<span class="op">::</span><span class="kw">select</span>(features[<span class="dv">3</span><span class="op">*</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">31</span>)<span class="op">-</span><span class="dv">1</span>]) <span class="op">%&gt;%</span><span class="st">   </span><span class="co"># Test features 2</span></a>
<a class="sourceLine" id="cb281-10" data-line-number="10"><span class="st">    </span><span class="kw">as.matrix</span>() </a>
<a class="sourceLine" id="cb281-11" data-line-number="11">feat_test_<span class="dv">3</span> &lt;-<span class="st"> </span>testing_sample <span class="op">%&gt;%</span><span class="st"> </span>dplyr<span class="op">::</span><span class="kw">select</span>(features[<span class="dv">3</span><span class="op">*</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">31</span>)<span class="op">-</span><span class="dv">2</span>]) <span class="op">%&gt;%</span><span class="st">   </span><span class="co"># Test features 3</span></a>
<a class="sourceLine" id="cb281-12" data-line-number="12"><span class="st">    </span><span class="kw">as.matrix</span>() </a></code></pre></div>
<p></p>
<p>Then, we specify the network structure. First, the 3 independent networks, then the aggregation.</p>

<div class="sourceCode" id="cb282"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb282-1" data-line-number="1">first_input &lt;-<span class="st"> </span><span class="kw">layer_input</span>(<span class="dt">shape =</span> <span class="kw">c</span>(<span class="dv">31</span>), <span class="dt">name =</span> <span class="st">&quot;first_input&quot;</span>)   <span class="co"># First input      </span></a>
<a class="sourceLine" id="cb282-2" data-line-number="2">first_network &lt;-<span class="st"> </span>first_input <span class="op">%&gt;%</span><span class="st">                                  </span><span class="co"># Def of 1st network</span></a>
<a class="sourceLine" id="cb282-3" data-line-number="3"><span class="st">    </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">8</span>, <span class="dt">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="dt">name =</span> <span class="st">&quot;layer_1&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb282-4" data-line-number="4"><span class="st">    </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">2</span>, <span class="dt">activation =</span> <span class="st">&#39;softmax&#39;</span>)                <span class="co"># Softmax for categ. output</span></a>
<a class="sourceLine" id="cb282-5" data-line-number="5">second_input &lt;-<span class="st"> </span><span class="kw">layer_input</span>(<span class="dt">shape =</span> <span class="kw">c</span>(<span class="dv">31</span>), <span class="dt">name =</span> <span class="st">&quot;second_input&quot;</span>) <span class="co"># Second input      </span></a>
<a class="sourceLine" id="cb282-6" data-line-number="6">second_network &lt;-<span class="st"> </span>second_input <span class="op">%&gt;%</span><span class="st">                                </span><span class="co"># Def of 2nd network</span></a>
<a class="sourceLine" id="cb282-7" data-line-number="7"><span class="st">    </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">8</span>, <span class="dt">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="dt">name =</span> <span class="st">&quot;layer_2&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb282-8" data-line-number="8"><span class="st">    </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">2</span>, <span class="dt">activation =</span> <span class="st">&#39;softmax&#39;</span>)                <span class="co"># Softmax for categ. output</span></a>
<a class="sourceLine" id="cb282-9" data-line-number="9">third_input &lt;-<span class="st"> </span><span class="kw">layer_input</span>(<span class="dt">shape =</span> <span class="kw">c</span>(<span class="dv">31</span>), <span class="dt">name =</span> <span class="st">&quot;third_input&quot;</span>)  <span class="co"># Third input      </span></a>
<a class="sourceLine" id="cb282-10" data-line-number="10">third_network &lt;-<span class="st"> </span>third_input <span class="op">%&gt;%</span><span class="st">                                  </span><span class="co"># Def of 3rd network</span></a>
<a class="sourceLine" id="cb282-11" data-line-number="11"><span class="st">    </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">8</span>, <span class="dt">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="dt">name =</span> <span class="st">&quot;layer_3&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb282-12" data-line-number="12"><span class="st">    </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">2</span>, <span class="dt">activation =</span> <span class="st">&#39;softmax&#39;</span>)                <span class="co"># Softmax for categ. output</span></a>
<a class="sourceLine" id="cb282-13" data-line-number="13"></a>
<a class="sourceLine" id="cb282-14" data-line-number="14">main_output &lt;-<span class="st"> </span><span class="kw">layer_concatenate</span>(<span class="kw">c</span>(first_network, </a>
<a class="sourceLine" id="cb282-15" data-line-number="15">                                   second_network,</a>
<a class="sourceLine" id="cb282-16" data-line-number="16">                                   third_network)) <span class="op">%&gt;%</span><span class="st">            </span><span class="co"># Combination</span></a>
<a class="sourceLine" id="cb282-17" data-line-number="17"><span class="st">    </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">2</span>, <span class="dt">activation =</span> <span class="st">&#39;softmax&#39;</span>, <span class="dt">name =</span> <span class="st">&#39;main_output&#39;</span>)</a>
<a class="sourceLine" id="cb282-18" data-line-number="18"></a>
<a class="sourceLine" id="cb282-19" data-line-number="19">model_ens &lt;-<span class="st"> </span><span class="kw">keras_model</span>(                                          <span class="co"># Agg. Model specs</span></a>
<a class="sourceLine" id="cb282-20" data-line-number="20">    <span class="dt">inputs =</span> <span class="kw">c</span>(first_input, second_input, third_input),</a>
<a class="sourceLine" id="cb282-21" data-line-number="21">    <span class="dt">outputs =</span> <span class="kw">c</span>(main_output)</a>
<a class="sourceLine" id="cb282-22" data-line-number="22">)</a></code></pre></div>
<p></p>
<p>Lastly, we can train and evaluate.</p>

<div class="sourceCode" id="cb283"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb283-1" data-line-number="1"><span class="kw">summary</span>(model_ens)                      <span class="co"># See model details / architecture</span></a></code></pre></div>
<pre><code>## Model: &quot;model_26&quot;
## __________________________________________________________________________________________
## Layer (type)                 Output Shape        Param #    Connected to                  
## ==========================================================================================
## first_input (InputLayer)     [(None, 31)]        0                                        
## __________________________________________________________________________________________
## second_input (InputLayer)    [(None, 31)]        0                                        
## __________________________________________________________________________________________
## third_input (InputLayer)     [(None, 31)]        0                                        
## __________________________________________________________________________________________
## layer_1 (Dense)              (None, 8)           256        first_input[0][0]             
## __________________________________________________________________________________________
## layer_2 (Dense)              (None, 8)           256        second_input[0][0]            
## __________________________________________________________________________________________
## layer_3 (Dense)              (None, 8)           256        third_input[0][0]             
## __________________________________________________________________________________________
## dense_165 (Dense)            (None, 2)           18         layer_1[0][0]                 
## __________________________________________________________________________________________
## dense_166 (Dense)            (None, 2)           18         layer_2[0][0]                 
## __________________________________________________________________________________________
## dense_167 (Dense)            (None, 2)           18         layer_3[0][0]                 
## __________________________________________________________________________________________
## concatenate_6 (Concatenate)  (None, 6)           0          dense_165[0][0]               
##                                                             dense_166[0][0]               
##                                                             dense_167[0][0]               
## __________________________________________________________________________________________
## main_output (Dense)          (None, 2)           14         concatenate_6[0][0]           
## ==========================================================================================
## Total params: 836
## Trainable params: 836
## Non-trainable params: 0
## __________________________________________________________________________________________</code></pre>
<div class="sourceCode" id="cb285"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb285-1" data-line-number="1">model_ens <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">compile</span>(                  <span class="co"># Learning parameters</span></a>
<a class="sourceLine" id="cb285-2" data-line-number="2">    <span class="dt">optimizer =</span> <span class="kw">optimizer_adam</span>(),</a>
<a class="sourceLine" id="cb285-3" data-line-number="3">    <span class="dt">loss =</span> <span class="st">&quot;binary_crossentropy&quot;</span>,</a>
<a class="sourceLine" id="cb285-4" data-line-number="4">    <span class="dt">metrics =</span> <span class="st">&quot;categorical_accuracy&quot;</span></a>
<a class="sourceLine" id="cb285-5" data-line-number="5">)</a>
<a class="sourceLine" id="cb285-6" data-line-number="6"></a>
<a class="sourceLine" id="cb285-7" data-line-number="7">fit_NN_ens &lt;-<span class="st"> </span>model_ens <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(               <span class="co"># Learning function</span></a>
<a class="sourceLine" id="cb285-8" data-line-number="8">    <span class="dt">x =</span> <span class="kw">list</span>(<span class="dt">first_input =</span> feat_train_<span class="dv">1</span>, </a>
<a class="sourceLine" id="cb285-9" data-line-number="9">             <span class="dt">second_input =</span> feat_train_<span class="dv">2</span>,</a>
<a class="sourceLine" id="cb285-10" data-line-number="10">             <span class="dt">third_input =</span> feat_train_<span class="dv">3</span>),</a>
<a class="sourceLine" id="cb285-11" data-line-number="11">    <span class="dt">y =</span> <span class="kw">list</span>(<span class="dt">main_output =</span> NN_train_labels_C), <span class="co"># Recycled from NN Chapter</span></a>
<a class="sourceLine" id="cb285-12" data-line-number="12">    <span class="dt">epochs =</span> <span class="dv">12</span>,                               <span class="co"># Nb rounds</span></a>
<a class="sourceLine" id="cb285-13" data-line-number="13">    <span class="dt">batch_size =</span> <span class="dv">512</span>,                          <span class="co"># Nb obs. per round</span></a>
<a class="sourceLine" id="cb285-14" data-line-number="14">    <span class="dt">validation_data =</span> <span class="kw">list</span>(<span class="kw">list</span>(feat_test_<span class="dv">1</span>, feat_test_<span class="dv">2</span>, feat_test_<span class="dv">3</span>),</a>
<a class="sourceLine" id="cb285-15" data-line-number="15">                           NN_test_labels_C)</a>
<a class="sourceLine" id="cb285-16" data-line-number="16">)</a>
<a class="sourceLine" id="cb285-17" data-line-number="17"><span class="kw">plot</span>(fit_NN_ens)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:ex12c"></span>
<img src="ML_factor_files/figure-html/ex12c-1.png" alt="Learning an integrated ensemble." width="432" />
<p class="caption">
FIGURE 19.10: Learning an integrated ensemble.
</p>
</div>
<p></p>
</div>
<div id="chapter-13" class="section level2">
<h2><span class="header-section-number">19.8</span> Chapter 13</h2>
<div id="ew-portfolios-with-the-tidyverse" class="section level3">
<h3><span class="header-section-number">19.8.1</span> EW portfolios with the tidyverse</h3>
<p>This one is incredibly easy, its simpler and more compact but close in spirit to the code that generates Figure <a href="factor.html#fig:factportsort">4.1</a>.
</p>
<div class="sourceCode" id="cb286"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb286-1" data-line-number="1">data_ml <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb286-2" data-line-number="2"><span class="st">  </span><span class="kw">group_by</span>(date) <span class="op">%&gt;%</span><span class="st">                     </span><span class="co"># Group by date</span></a>
<a class="sourceLine" id="cb286-3" data-line-number="3"><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">return =</span> <span class="kw">mean</span>(R1M_Usd)) <span class="op">%&gt;%</span><span class="st">  </span><span class="co"># Compute return</span></a>
<a class="sourceLine" id="cb286-4" data-line-number="4"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> date, <span class="dt">y =</span> return)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>() <span class="co"># Plot</span></a></code></pre></div>
<p><img src="ML_factor_files/figure-html/ex130a-1.png" width="384" style="display: block; margin: auto;" /></p>
<p></p>
</div>
<div id="advanced-weighting-function" class="section level3">
<h3><span class="header-section-number">19.8.2</span> Advanced weighting function</h3>
<p>First, we code the function with all inputs.</p>

<div class="sourceCode" id="cb287"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb287-1" data-line-number="1">weights &lt;-<span class="st"> </span><span class="cf">function</span>(Sigma, mu, Lambda, lambda, k_D, k_R, w_old){</a>
<a class="sourceLine" id="cb287-2" data-line-number="2">    N &lt;-<span class="st"> </span><span class="kw">nrow</span>(Sigma)</a>
<a class="sourceLine" id="cb287-3" data-line-number="3">    M &lt;-<span class="st"> </span><span class="kw">solve</span>(lambda<span class="op">*</span>Sigma <span class="op">+</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span>k_R<span class="op">*</span>Lambda <span class="op">+</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span>k_D<span class="op">*</span><span class="kw">diag</span>(N)) <span class="co"># Inverse matrix</span></a>
<a class="sourceLine" id="cb287-4" data-line-number="4">    num &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">-</span><span class="kw">sum</span>(M <span class="op">%*%</span><span class="st"> </span>(mu <span class="op">+</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span>k_R<span class="op">*</span>Lambda <span class="op">%*%</span><span class="st"> </span>w_old))       <span class="co"># eta numerator</span></a>
<a class="sourceLine" id="cb287-5" data-line-number="5">    den &lt;-<span class="st"> </span><span class="kw">sum</span>(M <span class="op">%*%</span><span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>,N))                              <span class="co"># eta denominator</span></a>
<a class="sourceLine" id="cb287-6" data-line-number="6">    eta &lt;-<span class="st"> </span>num <span class="op">/</span><span class="st"> </span>den                                        <span class="co"># eta</span></a>
<a class="sourceLine" id="cb287-7" data-line-number="7">    vec &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st"> </span>eta <span class="op">*</span><span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>,N) <span class="op">+</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span>k_R<span class="op">*</span>Lambda <span class="op">%*%</span><span class="st"> </span>w_old     <span class="co"># Vector in weight</span></a>
<a class="sourceLine" id="cb287-8" data-line-number="8">    <span class="kw">return</span>(M <span class="op">%*%</span><span class="st"> </span>vec)</a>
<a class="sourceLine" id="cb287-9" data-line-number="9">}</a></code></pre></div>
<p></p>
<p>Second, we test it on some random dataset. We use the returns created at the end of Chapter <a href="notdata.html#notdata">2</a> and used for the Lasso allocation in Section <a href="lasso.html#sparseex">6.2.2</a>. For <span class="math inline">\(\boldsymbol{\mu}\)</span>, we use the sample average, which is rarely a good idea in practice. It serves as illustration only.</p>

<div class="sourceCode" id="cb288"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb288-1" data-line-number="1">Sigma &lt;-<span class="st"> </span>returns <span class="op">%&gt;%</span><span class="st"> </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="op">-</span>date) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.matrix</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">cov</span>()  <span class="co"># Covariance matrix</span></a>
<a class="sourceLine" id="cb288-2" data-line-number="2">mu &lt;-<span class="st"> </span>returns <span class="op">%&gt;%</span><span class="st"> </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="op">-</span>date) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">apply</span>(<span class="dv">2</span>,mean)             <span class="co"># Vector of exp. returns</span></a>
<a class="sourceLine" id="cb288-3" data-line-number="3">Lambda &lt;-<span class="st"> </span><span class="kw">diag</span>(<span class="kw">nrow</span>(Sigma))                                          <span class="co"># Trans. Cost matrix</span></a>
<a class="sourceLine" id="cb288-4" data-line-number="4">lambda &lt;-<span class="st"> </span><span class="dv">1</span>                                                          <span class="co"># Risk aversion</span></a>
<a class="sourceLine" id="cb288-5" data-line-number="5">k_D &lt;-<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb288-6" data-line-number="6">k_R &lt;-<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb288-7" data-line-number="7">w_old &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>, <span class="kw">nrow</span>(Sigma)) <span class="op">/</span><span class="st"> </span><span class="kw">nrow</span>(Sigma)                           <span class="co"># Prev. weights: EW</span></a>
<a class="sourceLine" id="cb288-8" data-line-number="8"><span class="kw">weights</span>(Sigma, mu, Lambda, lambda, k_D, k_R, w_old) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">head</span>()       <span class="co"># First weights</span></a></code></pre></div>
<pre><code>##          [,1]
## p_return    1</code></pre>
<p></p>
<p>Some weights can of course be negative. Finally, we use the map2() function to test some sensitivity. We examine 3 key indicators:<br />
- <strong>diversification</strong>, which we measure via the inverse of the sum of squared weights (inverse Hirschman-Herfindhal index);<br />
- <strong>leverage</strong>, which we assess via the absolute sum of negative weights;<br />
- <strong>in-sample volatility</strong>, which we compute as <span class="math inline">\(\textbf{w}&#39; \boldsymbol{\Sigma} \textbf{x}\)</span></p>
<p>To do so, we create a dedicated function below.</p>

<div class="sourceCode" id="cb290"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb290-1" data-line-number="1">sensi &lt;-<span class="st"> </span><span class="cf">function</span>(lambda, k_D, Sigma, mu, Lambda, k_R, w_old){</a>
<a class="sourceLine" id="cb290-2" data-line-number="2">    w &lt;-<span class="st"> </span><span class="kw">weights</span>(Sigma, mu, Lambda, lambda, k_D, k_R, w_old)</a>
<a class="sourceLine" id="cb290-3" data-line-number="3">    out &lt;-<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb290-4" data-line-number="4">    out<span class="op">$</span>div &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="kw">sum</span>(w<span class="op">^</span><span class="dv">2</span>)             <span class="co"># Diversification</span></a>
<a class="sourceLine" id="cb290-5" data-line-number="5">    out<span class="op">$</span>lev &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="kw">abs</span>(w[w<span class="op">&lt;</span><span class="dv">0</span>]))       <span class="co"># Leverage</span></a>
<a class="sourceLine" id="cb290-6" data-line-number="6">    out<span class="op">$</span>vol &lt;-<span class="st"> </span><span class="kw">t</span>(w) <span class="op">%*%</span><span class="st"> </span>Sigma <span class="op">%*%</span><span class="st"> </span>w   <span class="co"># In-sample vol</span></a>
<a class="sourceLine" id="cb290-7" data-line-number="7">    <span class="kw">return</span>(out)</a>
<a class="sourceLine" id="cb290-8" data-line-number="8">}</a></code></pre></div>
<p></p>
<p>Instead of using the baseline <em>map2</em> function, we rely on a version thereof that concatenates results into a dataframe directly.</p>

<div class="sourceCode" id="cb291"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb291-1" data-line-number="1">lambda &lt;-<span class="st"> </span><span class="dv">10</span><span class="op">^</span>(<span class="op">-</span><span class="dv">3</span><span class="op">:</span><span class="dv">2</span>)              <span class="co"># parameter values</span></a>
<a class="sourceLine" id="cb291-2" data-line-number="2">k_D &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="dv">10</span><span class="op">^</span>(<span class="op">-</span><span class="dv">3</span><span class="op">:</span><span class="dv">2</span>)               <span class="co"># parameter values</span></a>
<a class="sourceLine" id="cb291-3" data-line-number="3">pars &lt;-<span class="st"> </span><span class="kw">expand_grid</span>(lambda, k_D) <span class="co"># parameter grid</span></a>
<a class="sourceLine" id="cb291-4" data-line-number="4">lambda &lt;-<span class="st"> </span>pars<span class="op">$</span>lambda</a>
<a class="sourceLine" id="cb291-5" data-line-number="5">k_D &lt;-<span class="st"> </span>pars<span class="op">$</span>k_D</a>
<a class="sourceLine" id="cb291-6" data-line-number="6"></a>
<a class="sourceLine" id="cb291-7" data-line-number="7">res &lt;-<span class="st"> </span><span class="kw">map2_dfr</span>(lambda, k_D, sensi, </a>
<a class="sourceLine" id="cb291-8" data-line-number="8">                <span class="dt">Sigma =</span> Sigma, <span class="dt">mu =</span> mu, <span class="dt">Lambda =</span> Lambda, <span class="dt">k_R =</span> k_R, <span class="dt">w_old =</span> w_old)</a>
<a class="sourceLine" id="cb291-9" data-line-number="9"></a>
<a class="sourceLine" id="cb291-10" data-line-number="10"><span class="kw">bind_cols</span>(<span class="dt">lambda =</span> <span class="kw">as.factor</span>(lambda), <span class="dt">k_D =</span> <span class="kw">as.factor</span>(k_D), res) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb291-11" data-line-number="11"><span class="st">    </span><span class="kw">gather</span>(<span class="dt">key =</span> indicator, <span class="dt">value =</span> value, <span class="op">-</span>lambda, <span class="op">-</span>k_D) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb291-12" data-line-number="12"><span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> lambda, <span class="dt">y =</span> value, <span class="dt">fill =</span> k_D)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_col</span>(<span class="dt">position =</span> <span class="st">&quot;dodge&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb291-13" data-line-number="13"><span class="st">    </span><span class="kw">facet_grid</span>(indicator <span class="op">~</span>. , <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>)</a></code></pre></div>
<p><img src="ML_factor_files/figure-html/ex131d-1.png" width="432" style="display: block; margin: auto;" /></p>
<p></p>
<p>Each panel displays an indicator. In the first panel, we see that diversification increases with <span class="math inline">\(k_D\)</span>: indeed, as this number increases, the portfolio converges to uniform (EW) values. The parameter <span class="math inline">\(\lambda\)</span> has a minor impact. The second panel naturally shows the inverse effect for leverage: as diversification increases with <span class="math inline">\(k_D\)</span>, leverage (i.e., total negative positions - shortsales) decreases. Finally, the last panel shows that in-sample volatility is however largely driven by the risk aversion parameter. As <span class="math inline">\(\lambda\)</span> increases, volatility logically decreases. For small values of <span class="math inline">\(\lambda\)</span>, <span class="math inline">\(k_D\)</span> is negatively related to volatility but the pattern reverses for large values of <span class="math inline">\(\lambda\)</span>. This is because the equally-weighted portfolio is less risky than very leveraged mean-variance policies, but more risky than the minimum-variance portfolio.</p>
</div>
<div id="functional-programming-in-the-backtest" class="section level3">
<h3><span class="header-section-number">19.8.3</span> Functional programming in the backtest</h3>
<p>Often, programmers prefer to avoid loops. In order to avoid a loop in the backtest, we need to code what happens for one given date. This is encapsulated in the following function. For simplicity, we code it for only one strategy. Also, the function will assume the structure of the data is known, but the columns (features &amp; labels) could also be passed as arguments. We recycle the function <strong>weights_xgb</strong> from Chapter <a href="backtest.html#backtest">13</a>.</p>

<div class="sourceCode" id="cb292"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb292-1" data-line-number="1">portf_map &lt;-<span class="st"> </span><span class="cf">function</span>(t, data_ml, ticks, t_oos, m_offset, train_size, weight_func){</a>
<a class="sourceLine" id="cb292-2" data-line-number="2">    train_data &lt;-<span class="st"> </span>data_ml <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(date <span class="op">&lt;</span><span class="st"> </span>t_oos[t] <span class="op">-</span><span class="st"> </span>m_offset <span class="op">*</span><span class="st"> </span><span class="dv">30</span>,   <span class="co"># Roll. window w. buffer</span></a>
<a class="sourceLine" id="cb292-3" data-line-number="3">                                     date <span class="op">&gt;</span><span class="st"> </span>t_oos[t] <span class="op">-</span><span class="st"> </span>m_offset <span class="op">*</span><span class="st"> </span><span class="dv">30</span> <span class="op">-</span><span class="st"> </span><span class="dv">365</span> <span class="op">*</span><span class="st"> </span>train_size)</a>
<a class="sourceLine" id="cb292-4" data-line-number="4">    test_data &lt;-<span class="st"> </span>data_ml <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(date <span class="op">==</span><span class="st"> </span>t_oos[t])                   <span class="co"># Test set  </span></a>
<a class="sourceLine" id="cb292-5" data-line-number="5">    realized_returns &lt;-<span class="st"> </span>test_data <span class="op">%&gt;%</span><span class="st">                                   </span><span class="co"># Computing returns via:</span></a>
<a class="sourceLine" id="cb292-6" data-line-number="6"><span class="st">        </span>dplyr<span class="op">::</span><span class="kw">select</span>(R1M_Usd)                                          <span class="co"># 1M holding period!</span></a>
<a class="sourceLine" id="cb292-7" data-line-number="7">    temp_weights &lt;-<span class="st"> </span><span class="kw">weight_func</span>(train_data, test_data, features)        <span class="co"># Weights = &gt; recycled!</span></a>
<a class="sourceLine" id="cb292-8" data-line-number="8">    ind &lt;-<span class="st"> </span><span class="kw">match</span>(temp_weights<span class="op">$</span>names, ticks) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">na.omit</span>()               <span class="co"># Index of test assets</span></a>
<a class="sourceLine" id="cb292-9" data-line-number="9">    x &lt;-<span class="st"> </span><span class="kw">c</span>() </a>
<a class="sourceLine" id="cb292-10" data-line-number="10">    x<span class="op">$</span>weights &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">length</span>(ticks))                           <span class="co"># Empty weights</span></a>
<a class="sourceLine" id="cb292-11" data-line-number="11">    x<span class="op">$</span>weights[ind] &lt;-<span class="st"> </span>temp_weights<span class="op">$</span>weights                       <span class="co"># Locate weights correctly</span></a>
<a class="sourceLine" id="cb292-12" data-line-number="12">    x<span class="op">$</span>returns &lt;-<span class="st"> </span><span class="kw">sum</span>(temp_weights<span class="op">$</span>weights <span class="op">*</span><span class="st"> </span>realized_returns)    <span class="co"># Compute returns</span></a>
<a class="sourceLine" id="cb292-13" data-line-number="13">    <span class="kw">return</span>(x)</a>
<a class="sourceLine" id="cb292-14" data-line-number="14">}</a></code></pre></div>
<p></p>
<p>Next, we combine this function to <strong>map</strong>(). We only test the first 6 dates: this reduces the computation times.</p>

<div class="sourceCode" id="cb293"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb293-1" data-line-number="1">back_test &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">3</span> <span class="op">%&gt;%</span><span class="st">             </span><span class="co"># Test on the first 100 out-of-sample dates</span></a>
<a class="sourceLine" id="cb293-2" data-line-number="2"><span class="st">    </span><span class="kw">map</span>(portf_map, <span class="dt">data_ml =</span> data_ml, <span class="dt">ticks =</span> ticks, <span class="dt">t_oos =</span> t_oos,</a>
<a class="sourceLine" id="cb293-3" data-line-number="3">        <span class="dt">m_offset =</span> <span class="dv">1</span>, <span class="dt">train_size =</span> <span class="dv">5</span>, <span class="dt">weight_func =</span> weights_xgb)</a>
<a class="sourceLine" id="cb293-4" data-line-number="4"><span class="kw">head</span>(back_test[[<span class="dv">1</span>]]<span class="op">$</span>weights)     <span class="co"># Sample weights</span></a></code></pre></div>
<pre><code>## [1] 0.001675042 0.000000000 0.000000000 0.001675042 0.000000000 0.001675042</code></pre>
<div class="sourceCode" id="cb295"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb295-1" data-line-number="1">back_test[[<span class="dv">1</span>]]<span class="op">$</span>returns           <span class="co"># Return of first period</span></a></code></pre></div>
<pre><code>## [1] 0.0189129</code></pre>
<p></p>
<p>Each element of backtest is a list with two components: the portfolio weights and the returns. To access the data easily, functions like <em>melt</em> from the package <em>reshape2</em> are useful.</p>
</div>
</div>
<div id="chapter-16" class="section level2">
<h2><span class="header-section-number">19.9</span> Chapter 16</h2>
<p>We recycle the AE model trained in Chapter <a href="unsup.html#unsup">16</a>. Strangely, building smaller models (encoder) from larger ones (AE) requires to save and then reload the weights. This creates an external file, which we call ae_weights. We can check that the output does have 4 columns (compressed) instead of 7 (original data).</p>

<div class="sourceCode" id="cb297"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb297-1" data-line-number="1"><span class="kw">save_model_weights_hdf5</span>(<span class="dt">object =</span> ae_model,<span class="dt">filepath =</span><span class="st">&quot;ae_weights.hdf5&quot;</span>, <span class="dt">overwrite =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb297-2" data-line-number="2">encoder_model &lt;-<span class="st"> </span><span class="kw">keras_model</span>(<span class="dt">inputs =</span> input_layer, <span class="dt">outputs =</span> encoder)</a>
<a class="sourceLine" id="cb297-3" data-line-number="3">encoder_model <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb297-4" data-line-number="4"><span class="st">    </span><span class="kw">load_model_weights_hdf5</span>(<span class="dt">filepath =</span> <span class="st">&quot;ae_weights.hdf5&quot;</span>,<span class="dt">skip_mismatch =</span> <span class="ot">TRUE</span>,<span class="dt">by_name =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb297-5" data-line-number="5">encoder_model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">compile</span>(</a>
<a class="sourceLine" id="cb297-6" data-line-number="6">    <span class="dt">loss =</span> <span class="st">&#39;mean_squared_error&#39;</span>,</a>
<a class="sourceLine" id="cb297-7" data-line-number="7">    <span class="dt">optimizer =</span> <span class="st">&#39;adam&#39;</span>,</a>
<a class="sourceLine" id="cb297-8" data-line-number="8">    <span class="dt">metrics =</span> <span class="kw">c</span>(<span class="st">&#39;mean_absolute_error&#39;</span>)</a>
<a class="sourceLine" id="cb297-9" data-line-number="9">)</a>
<a class="sourceLine" id="cb297-10" data-line-number="10">encoder_model <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb297-11" data-line-number="11"><span class="st">  </span>keras<span class="op">::</span><span class="kw">predict_on_batch</span>(<span class="dt">x =</span> training_sample <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb297-12" data-line-number="12"><span class="st">                              </span>dplyr<span class="op">::</span><span class="kw">select</span>(features_short) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb297-13" data-line-number="13"><span class="st">                              </span><span class="kw">as.matrix</span>()) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb297-14" data-line-number="14"><span class="st">    </span><span class="kw">head</span>(<span class="dv">5</span>)</a></code></pre></div>
<pre><code>## tf.Tensor(
## [[ 0.10143324  0.11381275 -1.1941879   0.63921183]
##  [ 0.07638849  0.13274665 -1.2055192   0.61408967]
##  [ 0.11305814  0.12400234 -1.2196397   0.6763896 ]
##  [ 0.11777879  0.12046745 -1.2234745   0.68157256]
##  [ 0.11348058  0.11111875 -1.2083291   0.6771684 ]], shape=(5, 4), dtype=float32)</code></pre>
<p></p>

<p></p>
</div>
<div id="chapter-17" class="section level2">
<h2><span class="header-section-number">19.10</span> Chapter 17</h2>
<p>All we need to do is change the rho coefficient in the code of Chapter <a href="RL.html#RL">17</a>.</p>

<div class="sourceCode" id="cb299"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb299-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">42</span>)                                                 <span class="co"># Fixing the random seed</span></a>
<a class="sourceLine" id="cb299-2" data-line-number="2">n_sample &lt;-<span class="st"> </span><span class="dv">10</span><span class="op">^</span><span class="dv">5</span>                                             <span class="co"># Number of samples to be generated</span></a>
<a class="sourceLine" id="cb299-3" data-line-number="3">rho &lt;-<span class="st"> </span>(<span class="op">-</span><span class="fl">0.8</span>)                                                <span class="co"># Autoregressive parameter</span></a>
<a class="sourceLine" id="cb299-4" data-line-number="4">sd &lt;-<span class="st"> </span><span class="fl">0.4</span>                                                    <span class="co"># Std. dev. of noise</span></a>
<a class="sourceLine" id="cb299-5" data-line-number="5">a &lt;-<span class="st"> </span><span class="fl">0.06</span> <span class="op">*</span><span class="st"> </span>rho                                              <span class="co"># Scaled mean of returns</span></a>
<a class="sourceLine" id="cb299-6" data-line-number="6">data_RL3 &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">returns =</span> a<span class="op">/</span>rho <span class="op">+</span><span class="st"> </span><span class="kw">arima.sim</span>(<span class="dt">n =</span> n_sample, <span class="co"># Returns via AR(1) simulation</span></a>
<a class="sourceLine" id="cb299-7" data-line-number="7">                                               <span class="kw">list</span>(<span class="dt">ar =</span> rho),       </a>
<a class="sourceLine" id="cb299-8" data-line-number="8">                                               <span class="dt">sd =</span> sd),</a>
<a class="sourceLine" id="cb299-9" data-line-number="9">                   <span class="dt">action =</span> <span class="kw">round</span>(<span class="kw">runif</span>(n_sample)<span class="op">*</span><span class="dv">4</span>)<span class="op">/</span><span class="dv">4</span>) <span class="op">%&gt;%</span><span class="st">   </span><span class="co"># Random action (portfolio)</span></a>
<a class="sourceLine" id="cb299-10" data-line-number="10"><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">new_state =</span> <span class="kw">if_else</span>(returns <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span>, <span class="st">&quot;neg&quot;</span>, <span class="st">&quot;pos&quot;</span>),   <span class="co"># Coding of state</span></a>
<a class="sourceLine" id="cb299-11" data-line-number="11">           <span class="dt">reward =</span> returns <span class="op">*</span><span class="st"> </span>action,                        <span class="co"># Reward = portfolio return</span></a>
<a class="sourceLine" id="cb299-12" data-line-number="12">           <span class="dt">state =</span> <span class="kw">lag</span>(new_state),                           <span class="co"># Next state</span></a>
<a class="sourceLine" id="cb299-13" data-line-number="13">           <span class="dt">action =</span> <span class="kw">as.character</span>(action)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb299-14" data-line-number="14"><span class="st">    </span><span class="kw">na.omit</span>()                                                <span class="co"># Remove one missing state</span></a></code></pre></div>
<p></p>
<p>The learning can then proceed.</p>

<div class="sourceCode" id="cb300"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb300-1" data-line-number="1">control &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">alpha =</span> <span class="fl">0.1</span>,                        <span class="co"># Learning rate</span></a>
<a class="sourceLine" id="cb300-2" data-line-number="2">                <span class="dt">gamma =</span> <span class="fl">0.7</span>,                        <span class="co"># Discount factor for rewards</span></a>
<a class="sourceLine" id="cb300-3" data-line-number="3">                <span class="dt">epsilon =</span> <span class="fl">0.1</span>)                      <span class="co"># Exploration rate</span></a>
<a class="sourceLine" id="cb300-4" data-line-number="4">fit_RL3 &lt;-<span class="st"> </span><span class="kw">ReinforcementLearning</span>(data_RL3,          <span class="co"># Main RL function</span></a>
<a class="sourceLine" id="cb300-5" data-line-number="5">                                 <span class="dt">s =</span> <span class="st">&quot;state&quot;</span>, </a>
<a class="sourceLine" id="cb300-6" data-line-number="6">                                 <span class="dt">a =</span> <span class="st">&quot;action&quot;</span>, </a>
<a class="sourceLine" id="cb300-7" data-line-number="7">                                 <span class="dt">r =</span> <span class="st">&quot;reward&quot;</span>, </a>
<a class="sourceLine" id="cb300-8" data-line-number="8">                                 <span class="dt">s_new =</span> <span class="st">&quot;new_state&quot;</span>, </a>
<a class="sourceLine" id="cb300-9" data-line-number="9">                                 <span class="dt">control =</span> control)</a>
<a class="sourceLine" id="cb300-10" data-line-number="10"><span class="kw">print</span>(fit_RL3)   <span class="co"># Show the output</span></a></code></pre></div>
<pre><code>## State-Action function Q
##          0.25         0         1      0.75       0.5
## neg 0.7107268 0.5971710 1.4662416 0.9535698 0.8069591
## pos 0.7730842 0.7869229 0.4734467 0.4258593 0.6257039
## 
## Policy
## neg pos 
## &quot;1&quot; &quot;0&quot; 
## 
## Reward (last iteration)
## [1] 3013.162</code></pre>
<p></p>
<p>In this case, the constantly switching feature of the return process changes the outcome. The negative state is associated with large profits when the portfolio is fully invested while the positive state has the best average reward when the agent refrains from investing.</p>
<p>For the second exercise, the trick is to define all possible actions, that is all combinations (+1,0-1) for the two assets on all dates. We recycle the data from Chapter <a href="RL.html#RL">17</a>.</p>

<div class="sourceCode" id="cb302"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb302-1" data-line-number="1">pos_<span class="dv">3</span> &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>)                              <span class="co"># Possible alloc. to asset 1</span></a>
<a class="sourceLine" id="cb302-2" data-line-number="2">pos_<span class="dv">4</span> &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>)                              <span class="co"># Possible alloc. to asset 3</span></a>
<a class="sourceLine" id="cb302-3" data-line-number="3">pos &lt;-<span class="st"> </span><span class="kw">expand_grid</span>(pos_<span class="dv">3</span>, pos_<span class="dv">4</span>)                <span class="co"># All combinations</span></a>
<a class="sourceLine" id="cb302-4" data-line-number="4">pos &lt;-<span class="st"> </span><span class="kw">bind_cols</span>(pos, <span class="dt">id =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(pos))         <span class="co"># Adding combination id</span></a>
<a class="sourceLine" id="cb302-5" data-line-number="5"></a>
<a class="sourceLine" id="cb302-6" data-line-number="6">ret_pb_RL &lt;-<span class="st"> </span><span class="kw">bind_cols</span>(<span class="dt">r3 =</span> return_<span class="dv">3</span>, <span class="dt">r4 =</span> return_<span class="dv">4</span>, <span class="co"># Returns &amp; P/B dataframe</span></a>
<a class="sourceLine" id="cb302-7" data-line-number="7">                       <span class="dt">pb3 =</span> pb_<span class="dv">3</span>, <span class="dt">pb4 =</span> pb_<span class="dv">4</span>) </a>
<a class="sourceLine" id="cb302-8" data-line-number="8">data_RL4 &lt;-<span class="st"> </span><span class="kw">sapply</span>(ret_pb_RL,                        <span class="co"># Combining return &amp; positions</span></a>
<a class="sourceLine" id="cb302-9" data-line-number="9">                   rep.int, </a>
<a class="sourceLine" id="cb302-10" data-line-number="10">                   <span class="dt">times =</span> <span class="kw">nrow</span>(pos)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb302-11" data-line-number="11"><span class="st">    </span><span class="kw">data.frame</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb302-12" data-line-number="12"><span class="st">    </span><span class="kw">bind_cols</span>(<span class="dt">id =</span> <span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(pos), <span class="dv">1</span>, <span class="dt">each =</span> <span class="kw">length</span>(return_<span class="dv">3</span>))) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb302-13" data-line-number="13"><span class="st">    </span><span class="kw">left_join</span>(pos) <span class="op">%&gt;%</span><span class="st"> </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="op">-</span>id) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb302-14" data-line-number="14"><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">action =</span> <span class="kw">paste</span>(pos_<span class="dv">3</span>, pos_<span class="dv">4</span>),            <span class="co"># Uniting actions</span></a>
<a class="sourceLine" id="cb302-15" data-line-number="15">           <span class="dt">pb3 =</span> <span class="kw">round</span>(<span class="dv">5</span> <span class="op">*</span><span class="st"> </span>pb3),                    <span class="co"># Simplifying states</span></a>
<a class="sourceLine" id="cb302-16" data-line-number="16">           <span class="dt">pb4 =</span> <span class="kw">round</span>(<span class="dv">5</span> <span class="op">*</span><span class="st"> </span>pb4),                    <span class="co"># Simplifying states</span></a>
<a class="sourceLine" id="cb302-17" data-line-number="17">           <span class="dt">state =</span> <span class="kw">paste</span>(pb3, pb4),                 <span class="co"># Uniting states</span></a>
<a class="sourceLine" id="cb302-18" data-line-number="18">           <span class="dt">reward =</span> pos_<span class="dv">3</span><span class="op">*</span>r3 <span class="op">+</span><span class="st"> </span>pos_<span class="dv">4</span><span class="op">*</span>r4,            <span class="co"># Computing rewards</span></a>
<a class="sourceLine" id="cb302-19" data-line-number="19">           <span class="dt">new_state =</span> <span class="kw">lead</span>(state)) <span class="op">%&gt;%</span><span class="st">             </span><span class="co"># Infer new state</span></a>
<a class="sourceLine" id="cb302-20" data-line-number="20"><span class="st">    </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="op">-</span>pb3, <span class="op">-</span>pb4, <span class="op">-</span>pos_<span class="dv">3</span>,          <span class="co"># Remove superfluous vars.</span></a>
<a class="sourceLine" id="cb302-21" data-line-number="21">                  <span class="op">-</span>pos_<span class="dv">4</span>, <span class="op">-</span>r3, <span class="op">-</span>r4) </a></code></pre></div>
<p></p>
<p>We can the plug this data into the RL function.</p>

<div class="sourceCode" id="cb303"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb303-1" data-line-number="1">fit_RL4 &lt;-<span class="st"> </span><span class="kw">ReinforcementLearning</span>(data_RL4,           <span class="co"># Main RL function</span></a>
<a class="sourceLine" id="cb303-2" data-line-number="2">                                 <span class="dt">s =</span> <span class="st">&quot;state&quot;</span>, </a>
<a class="sourceLine" id="cb303-3" data-line-number="3">                                 <span class="dt">a =</span> <span class="st">&quot;action&quot;</span>, </a>
<a class="sourceLine" id="cb303-4" data-line-number="4">                                 <span class="dt">r =</span> <span class="st">&quot;reward&quot;</span>, </a>
<a class="sourceLine" id="cb303-5" data-line-number="5">                                 <span class="dt">s_new =</span> <span class="st">&quot;new_state&quot;</span>, </a>
<a class="sourceLine" id="cb303-6" data-line-number="6">                                 <span class="dt">control =</span> control)</a>
<a class="sourceLine" id="cb303-7" data-line-number="7">fit_RL4<span class="op">$</span>Q &lt;-<span class="st"> </span><span class="kw">round</span>(fit_RL4<span class="op">$</span>Q, <span class="dv">3</span>) <span class="co"># Round the Q-matrix</span></a>
<a class="sourceLine" id="cb303-8" data-line-number="8"><span class="kw">print</span>(fit_RL4)                   <span class="co"># Show the output </span></a></code></pre></div>
<pre><code>## State-Action function Q
##        0 0    0 1  0 -1  -1 -1   -1 0   -1 1   1 -1    1 0    1 1
## X0.2 0.000  0.000 0.002 -0.017 -0.018 -0.020  0.023  0.025  0.024
## X0.3 0.001 -0.005 0.007 -0.013 -0.019 -0.026  0.031  0.027  0.021
## X3.1 0.003  0.003 0.003  0.002  0.002  0.003  0.002  0.002  0.003
## X2.1 0.027  0.038 0.020  0.004  0.015  0.039  0.013  0.021  0.041
## X2.2 0.021  0.014 0.027  0.038  0.047  0.045 -0.004 -0.011 -0.016
## X2.3 0.007  0.006 0.008  0.054  0.057  0.056 -0.041 -0.041 -0.041
## X1.1 0.027  0.054 0.005 -0.031 -0.005  0.041  0.025  0.046  0.072
## X1.2 0.019  0.020 0.020  0.015  0.023  0.029  0.012  0.014  0.023
## X1.3 0.008  0.019 0.000 -0.036 -0.027 -0.016  0.042  0.053  0.060
## 
## Policy
##   X0.2   X0.3   X3.1   X2.1   X2.2   X2.3   X1.1   X1.2   X1.3 
##  &quot;1 0&quot; &quot;1 -1&quot; &quot;0 -1&quot;  &quot;1 1&quot; &quot;-1 0&quot; &quot;-1 0&quot;  &quot;1 1&quot; &quot;-1 1&quot;  &quot;1 1&quot; 
## 
## Reward (last iteration)
## [1] 0</code></pre>
<p></p>
<p>The matrix is less sparse compared to the one of Chapter <a href="RL.html#RL">17</a>: we have covered much more ground! Some policy recommendations have not changed compared to the smaller sample, but some have! The change occur for the states for which only a few points were available in the first trial. With more data, the decision is altered.</p>

<div id="refs" class="references">
<div>
<p>Abbasi, Ahmed, Conan Albrecht, Anthony Vance, and James Hansen. 2012. Metafraud: A Meta-Learning Framework for Detecting Financial Fraud. <em>MIS Quarterly</em>, 12931327.</p>
</div>
<div>
<p>Aboussalah, Amine Mohamed, and Chi-Guhn Lee. 2020. Continuous Control with Stacked Deep Dynamic Recurrent Reinforcement Learning for Portfolio Optimization. <em>Expert Systems with Applications</em> 140: 112891.</p>
</div>
<div>
<p>Adler, Timothy, and Mark Kritzman. 2008. The Cost of Socially Responsible Investing. <em>Journal of Portfolio Management</em> 35 (1): 5256.</p>
</div>
<div>
<p>Agarwal, Amit, Elad Hazan, Satyen Kale, and Robert E Schapire. 2006. Algorithms for Portfolio Management Based on the Newton Method. In <em>Proceedings of the 23rd International Conference on Machine Learning</em>, 916. ACM.</p>
</div>
<div>
<p>Aggarwal, Charu C. 2013. <em>Outlier Analysis</em>. Springer.</p>
</div>
<div>
<p>Aldridge, Irene, and Marco Avellaneda. 2019. Neural Networks in Finance: Design and Performance. <em>Journal of Financial Data Science</em> 1 (4): 3962.</p>
</div>
<div>
<p>Allison, Paul D. 2001. <em>Missing Data</em>. Vol. 136. Sage publications.</p>
</div>
<div>
<p>Almahdi, Saud, and Steve Y Yang. 2017. An Adaptive Portfolio Trading System: A Risk-Return Portfolio Optimization Using Recurrent Reinforcement Learning with Expected Maximum Drawdown. <em>Expert Systems with Applications</em> 87: 26779.</p>
</div>
<div>
<p>. 2019. A Constrained Portfolio Trading System Using Particle Swarm Algorithm and Recurrent Reinforcement Learning. <em>Expert Systems with Applications</em> 130: 14556.</p>
</div>
<div>
<p>Alti, Aydoan, and Sheridan Titman. 2019. A Dynamic Model of Characteristic-Based Return Predictability. <em>Journal of Finance</em> 74 (6): 31873216.</p>
</div>
<div>
<p>Ammann, Manuel, Guillaume Coqueret, and Jan-Philip Schade. 2016. Characteristics-Based Portfolio Choice with Leverage Constraints. <em>Journal of Banking &amp; Finance</em> 70: 2337.</p>
</div>
<div>
<p>Amrhein, Valentin, Sander Greenland, and Blake McShane. 2019. Scientists Rise up Against Statistical Significance. <em>Nature</em> 567: 3057.</p>
</div>
<div>
<p>Anderson, James A, and Edward Rosenfeld. 2000. <em>Talking Nets: An Oral History of Neural Networks</em>. MIT Press.</p>
</div>
<div>
<p>Andersson, Kristoffer, and Cornelis Oosterlee. 2020. A Deep Learning Approach for Computations of Exposure Profiles for High-Dimensional Bermudan Options. <em>arXiv Preprint</em>, no. 2003.01977.</p>
</div>
<div>
<p>Ang, Andrew. 2014. <em>Asset Management: A Systematic Approach to Factor Investing</em>. Oxford University Press.</p>
</div>
<div>
<p>Ang, Andrew, Robert J Hodrick, Yuhang Xing, and Xiaoyan Zhang. 2006. The Cross-Section of Volatility and Expected Returns. <em>Journal of Finance</em> 61 (1): 25999.</p>
</div>
<div>
<p>Ang, Andrew, and Dennis Kristensen. 2012. Testing Conditional Factor Models. <em>Journal of Financial Economics</em> 106 (1): 13256.</p>
</div>
<div>
<p>Ang, Andrew, Jun Liu, and Krista Schwarz. 2018. Using Individual Stocks or Portfolios in Tests of Factor Models. <em>SSRN Working Paper</em> 1106463.</p>
</div>
<div>
<p>Arik, Sercan O, and Tomas Pfister. 2019. TabNet: Attentive Interpretable Tabular Learning. <em>arXiv Preprint</em>, no. 1908.07442.</p>
</div>
<div>
<p>Arjovsky, Martin, Lon Bottou, Ishaan Gulrajani, and David Lopez-Paz. 2019. Invariant Risk Minimization. <em>arXiv Preprint</em>, no. 1907.02893.</p>
</div>
<div>
<p>Arnott, Robert D, Mark Clements, Vitali Kalesnik, and Juhani T Linnainmaa. 2020. Factor Momentum. <em>Journal of the American Statistical Association</em> 3116974.</p>
</div>
<div>
<p>Arnott, Robert D, Jason C Hsu, Jun Liu, and Harry Markowitz. 2014. Can Noise Create the Size and Value Effects? <em>Management Science</em> 61 (11): 256979.</p>
</div>
<div>
<p>Arnott, Rob, Campbell R Harvey, Vitali Kalesnik, and Juhani Linnainmaa. 2019. Alices Adventures in Factorland: Three Blunders That Plague Factor Investing. <em>Journal of Portfolio Management</em> 45 (4): 1836.</p>
</div>
<div>
<p>Arnott, Rob, Campbell R Harvey, and Harry Markowitz. 2019. A Backtesting Protocol in the Era of Machine Learning. <em>Journal of Financial Data Science</em> 1 (1): 6474.</p>
</div>
<div>
<p>Aronow, Peter M., and Fredrik Svje. 2019. Book Review. The Book of Why: The New Science of Cause and Effect. <em>Journal of the American Statistical Association</em> 115 (529): 48285.</p>
</div>
<div>
<p>Asness, Cliff, Andrea Frazzini, Niels Joachim Gormsen, and Lasse Heje Pedersen. 2020. Betting Against Correlation: Testing Theories of the Low-Risk Effect. <em>Journal of Financial Economics</em> 135 (3): 62952.</p>
</div>
<div>
<p>Asness, Clifford, Swati Chandra, Antti Ilmanen, and Ronen Israel. 2017. Contrarian Factor Timing Is Deceptively Difficult. <em>Journal of Portfolio Management</em> 43 (5): 7287.</p>
</div>
<div>
<p>Asness, Clifford, and Andrea Frazzini. 2013. The Devil in Hmls Details. <em>Journal of Portfolio Management</em> 39 (4): 4968.</p>
</div>
<div>
<p>Asness, Clifford, Andrea Frazzini, Ronen Israel, Tobias J Moskowitz, and Lasse H Pedersen. 2018. Size Matters, If You Control Your Junk. <em>Journal of Financial Economics</em> 129 (3): 479509.</p>
</div>
<div>
<p>Asness, Clifford, Antti Ilmanen, Ronen Israel, and Tobias Moskowitz. 2015. Investing with Style. <em>Journal of Investment Management</em> 13 (1): 2763.</p>
</div>
<div>
<p>Asness, Clifford S, Tobias J Moskowitz, and Lasse Heje Pedersen. 2013. Value and Momentum Everywhere. <em>Journal of Finance</em> 68 (3): 92985.</p>
</div>
<div>
<p>Astakhov, Anton, Tomas Havranek, and Jiri Novak. 2019. Firm Size and Stock Returns: A Quantitative Survey. <em>Journal of Economic Surveys</em> 33 (5): 146392.</p>
</div>
<div>
<p>Atta-Darkua, Vaska, David Chambers, Elroy Dimson, Zhenkai Ran, and Ting Yu. 2020. Strategies for Responsible Investing: Emerging Academic Evidence. <em>Journal of Portfolio Management</em> 46 (3): 2635.</p>
</div>
<div>
<p>Bache, Stefan Milton, and Hadley Wickham. 2014. Magrittr: A Forward-Pipe Operator for R. <em>R Package Version</em> 1 (1).</p>
</div>
<div>
<p>Back, Kerry. 2010. <em>Asset Pricing and Portfolio Choice Theory</em>. Oxford University Press.</p>
</div>
<div>
<p>Baesens, Bart, Veronique Van Vlasselaer, and Wouter Verbeke. 2015. <em>Fraud Analytics Using Descriptive, Predictive, and Social Network Techniques: A Guide to Data Science for Fraud Detection</em>. John Wiley &amp; Sons.</p>
</div>
<div>
<p>Bailey, David H, and Marcos Lpez de Prado. 2014. The Deflated Sharpe Ratio: Correcting for Selection Bias, Backtest Overfitting, and Non-Normality. <em>Journal of Portfolio Management</em> 40 (5): 3959.</p>
</div>
<div>
<p>Bailey, T, and A.K. Jain. 1978. A Note on Distance-Weighted K-Nearest Neighbor Rules. <em>IEEE Trans. On Systems, Man, Cybernetics</em> 8 (4): 31113.</p>
</div>
<div>
<p>Bajgrowicz, Pierre, and Olivier Scaillet. 2012. Technical Trading Revisited: False Discoveries, Persistence Tests, and Transaction Costs. <em>Journal of Financial Economics</em> 106 (3): 47391.</p>
</div>
<div>
<p>Baker, Malcolm, Brendan Bradley, and Jeffrey Wurgler. 2011. Benchmarks as Limits to Arbitrage: Understanding the Low-Volatility Anomaly. <em>Financial Analysts Journal</em> 67 (1): 4054.</p>
</div>
<div>
<p>Baker, Malcolm, Mathias F Hoeyer, and Jeffrey Wurgler. 2020. Leverage and the Beta Anomaly. <em>Journal of Financial and Quantitative Analysis</em> Forthcoming: 124.</p>
</div>
<div>
<p>Baker, Malcolm, Patrick Luo, and Ryan Taliaferro. 2017. Detecting Anomalies: The Relevance and Power of Standard Asset Pricing Tests.</p>
</div>
<div>
<p>Bali, Turan G, Robert F Engle, and Scott Murray. 2016. <em>Empirical Asset Pricing: The Cross Section of Stock Returns</em>. John Wiley &amp; Sons.</p>
</div>
<div>
<p>Ballings, Michel, Dirk Van den Poel, Nathalie Hespeels, and Ruben Gryp. 2015. Evaluating Multiple Classifiers for Stock Price Direction Prediction. <em>Expert Systems with Applications</em> 42 (20): 704656.</p>
</div>
<div>
<p>Ban, Gah-Yi, Noureddine El Karoui, and Andrew EB Lim. 2016. Machine Learning and Portfolio Optimization. <em>Management Science</em> 64 (3): 113654.</p>
</div>
<div>
<p>Bansal, Ravi, David A Hsieh, and S Viswanathan. 1993. A New Approach to International Arbitrage Pricing. <em>Journal of Finance</em> 48 (5): 171947.</p>
</div>
<div>
<p>Bansal, Ravi, and Salim Viswanathan. 1993. No Arbitrage and Arbitrage Pricing: A New Approach. <em>Journal of Finance</em> 48 (4): 123162.</p>
</div>
<div>
<p>Banz, Rolf W. 1981. The Relationship Between Return and Market Value of Common Stocks. <em>Journal of Financial Economics</em> 9 (1): 318.</p>
</div>
<div>
<p>Barberis, Nicholas. 2018. Psychology-Based Models of Asset Prices and Trading Volume. In <em>Handbook of Behavioral Economics-Foundations and Applications</em>.</p>
</div>
<div>
<p>Barberis, Nicholas, Robin Greenwood, Lawrence Jin, and Andrei Shleifer. 2015. X-Capm: An Extrapolative Capital Asset Pricing Model. <em>Journal of Financial Economics</em> 115 (1): 124.</p>
</div>
<div>
<p>Barberis, Nicholas, Lawrence J Jin, and Baolian Wang. 2020. Prospect Theory and Stock Market Anomalies. <em>SSRN Working Paper</em> 3477463.</p>
</div>
<div>
<p>Barberis, Nicholas, Abhiroop Mukherjee, and Baolian Wang. 2016. Prospect Theory and Stock Returns: An Empirical Test. <em>Review of Financial Studies</em> 29 (11): 30683107.</p>
</div>
<div>
<p>Barberis, Nicholas, and Andrei Shleifer. 2003. Style Investing. <em>Journal of Financial Economics</em> 68 (2): 16199.</p>
</div>
<div>
<p>Barillas, Francisco, and Jay Shanken. 2018. Comparing Asset Pricing Models. <em>Journal of Finance</em> 73 (2): 71554.</p>
</div>
<div>
<p>Barron, Andrew R. 1993. Universal Approximation Bounds for Superpositions of a Sigmoidal Function. <em>IEEE Transactions on Information Theory</em> 39 (3): 93045.</p>
</div>
<div>
<p>. 1994. Approximation and Estimation Bounds for Artificial Neural Networks. <em>Machine Learning</em> 14 (1): 11533.</p>
</div>
<div>
<p>Barroso, Pedro, and Pedro Santa-Clara. 2015. Momentum Has Its Moments. <em>Journal of Financial Economics</em> 116 (1): 11120.</p>
</div>
<div>
<p>Basak, Jayanta. 2004. Online Adaptive Decision Trees. <em>Neural Computation</em> 16 (9): 195981.</p>
</div>
<div>
<p>Bates, John M, and Clive WJ Granger. 1969. The Combination of Forecasts. <em>Journal of the Operational Research Society</em> 20 (4): 45168.</p>
</div>
<div>
<p>Baz, Jamil, Nicolas Granger, Campbell R Harvey, Nicolas Le Roux, and Sandy Rattray. 2015. Dissecting Investment Strategies in the Cross Section and Time Series. <em>SSRN Working Paper</em> 2695101.</p>
</div>
<div>
<p>Beery, Sara, Grant Van Horn, and Pietro Perona. 2018. Recognition in Terra Incognita. In <em>Proceedings of the European Conference on Computer Vision (Eccv)</em>, 45673.</p>
</div>
<div>
<p>Belsley, David A, Edwin Kuh, and Roy E Welsch. 2005. <em>Regression Diagnostics: Identifying Influential Data and Sources of Collinearity</em>. Vol. 571. John Wiley &amp; Sons.</p>
</div>
<div>
<p>Ben-David, Shai, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. 2010. A Theory of Learning from Different Domains. <em>Machine Learning</em> 79 (1-2): 15175.</p>
</div>
<div>
<p>Bengio, Yoshua. 2012. Practical Recommendations for Gradient-Based Training of Deep Architectures. In <em>Neural Networks: Tricks of the Trade</em>, 43778. Springer.</p>
</div>
<div>
<p>Berg, Florian, Julian F Koelbel, and Roberto Rigobon. 2019. Aggregate Confusion: The Divergence of Esg Ratings. <em>SSRN Working Paper</em> 3438533.</p>
</div>
<div>
<p>Bergstra, James, and Yoshua Bengio. 2012. Random Search for Hyper-Parameter Optimization. <em>Journal of Machine Learning Research</em> 13 (Feb): 281305.</p>
</div>
<div>
<p>Berk, Jonathan B, Richard C Green, and Vasant Naik. 1999. Optimal Investment, Growth Options, and Security Returns. <em>Journal of Finance</em> 54 (5): 15531607.</p>
</div>
<div>
<p>Bernstein, Asaf, Matthew T Gustafson, and Ryan Lewis. 2019. Disaster on the Horizon: The Price Effect of Sea Level Rise. <em>Journal of Financial Economics</em> 134 (2): 25372.</p>
</div>
<div>
<p>Bertoluzzo, Francesco, and Marco Corazza. 2012. Testing Different Reinforcement Learning Configurations for Financial Trading: Introduction and Applications. <em>Procedia Economics and Finance</em> 3: 6877.</p>
</div>
<div>
<p>Bertsekas, Dimitri P. 2017. <em>Dynamic Programming and Optimal Control - Volume Ii, Fourth Edition</em>. Athena Scientific.</p>
</div>
<div>
<p>Betermier, Sebastien, Laurent E Calvet, and Evan Jo. 2019. A Supply and Demand Approach to Equity Pricing. <em>SSRN Working Paper</em> 3440147.</p>
</div>
<div>
<p>Betermier, Sebastien, Laurent E Calvet, and Paolo Sodini. 2017. Who Are the Value and Growth Investors? <em>Journal of Finance</em> 72 (1): 546.</p>
</div>
<div>
<p>Bhamra, Harjoat S, and Raman Uppal. 2019. Does Household Finance Matter? Small Financial Errors with Large Social Costs. <em>American Economic Review</em> 109 (3): 111654.</p>
</div>
<div>
<p>Bhatia, Nitin, and others. 2010. Survey of Nearest Neighbor Techniques. <em>arXiv Preprint</em>, no. 1007.0085.</p>
</div>
<div>
<p>Bhattacharyya, Siddhartha, Sanjeev Jha, Kurian Tharakunnel, and J Christopher Westland. 2011. Data Mining for Credit Card Fraud: A Comparative Study. <em>Decision Support Systems</em> 50 (3): 60213.</p>
</div>
<div>
<p>Biau, Grard. 2012. Analysis of a Random Forests Model. <em>Journal of Machine Learning Research</em> 13 (Apr): 106395.</p>
</div>
<div>
<p>Biau, Grard, Luc Devroye, and GAbor Lugosi. 2008. Consistency of Random Forests and Other Averaging Classifiers. <em>Journal of Machine Learning Research</em> 9 (Sep): 201533.</p>
</div>
<div>
<p>Black, Fischer, and Robert Litterman. 1992. Global Portfolio Optimization. <em>Financial Analysts Journal</em> 48 (5): 2843.</p>
</div>
<div>
<p>Blank, Herbert, Richard Davis, and Shannon Greene. 2019. Using Alternative Research Data in Real-World Portfolios. <em>Journal of Investing</em> 28 (4): 95103.</p>
</div>
<div>
<p>Blitz, David, and Laurens Swinkels. 2020. Is Exclusion Effective? <em>Journal of Portfolio Management</em> 46 (3): 4248.</p>
</div>
<div>
<p>Blum, Avrim, and Adam Kalai. 1999. Universal Portfolios with and Without Transaction Costs. <em>Machine Learning</em> 35 (3): 193205.</p>
</div>
<div>
<p>Boehmke, Brad, and Brandon Greenwell. 2019. <em>Hands-on Machine Learning with R</em>. Chapman; Hall / CRC.</p>
</div>
<div>
<p>Boloorforoosh, Ali, Peter Christoffersen, Christian Gourieroux, and Mathieu Fournier. 2020. Beta Risk in the Cross-Section of Equities. <em>Review of Financial Studies</em> Forthcoming.</p>
</div>
<div>
<p>Bonaccolto, Giovanni, and Sandra Paterlini. 2019. Developing New Portfolio Strategies by Aggregation. <em>Annals of Operations Research</em>, 139.</p>
</div>
<div>
<p>Boriah, Shyam, Varun Chandola, and Vipin Kumar. 2008. Similarity Measures for Categorical Data: A Comparative Evaluation. In <em>Proceedings of the 2008 Siam International Conference on Data Mining</em>, 24354.</p>
</div>
<div>
<p>Boser, Bernhard E, Isabelle M Guyon, and Vladimir N Vapnik. 1992. A Training Algorithm for Optimal Margin Classifiers. In <em>Proceedings of the Fifth Annual Workshop on Computational Learning Theory</em>, 14452. ACM.</p>
</div>
<div>
<p>Bouchaud, Jean-philippe, Philipp Krueger, Augustin Landier, and David Thesmar. 2019. Sticky Expectations and the Profitability Anomaly. <em>Journal of Finance</em> 74 (2): 63974.</p>
</div>
<div>
<p>Bouthillier, Xavier, and Gal Varoquaux. 2020. Survey of Machine-Learning Experimental Methods at Neurips2019 and Iclr2020. Research Report. Inria Saclay Ile de France.</p>
</div>
<div>
<p>Boyd, Stephen, and Lieven Vandenberghe. 2004. <em>Convex Optimization</em>. Cambridge University Press.</p>
</div>
<div>
<p>Branch, Ben, and Li Cai. 2012. Do Socially Responsible Index Investors Incur an Opportunity Cost? <em>Financial Review</em> 47 (3): 61730.</p>
</div>
<div>
<p>Brandt, Michael W, Pedro Santa-Clara, and Rossen Valkanov. 2009. Parametric Portfolio Policies: Exploiting Characteristics in the Cross-Section of Equity Returns. <em>Review of Financial Studies</em> 22 (9): 341147.</p>
</div>
<div>
<p>Braun, Helmut, and John S Chandler. 1987. Predicting Stock Market Behavior Through Rule Induction: An Application of the Learning-from-Example Approach. <em>Decision Sciences</em> 18 (3): 41529.</p>
</div>
<div>
<p>Breiman, Leo. 1996. Stacked Regressions. <em>Machine Learning</em> 24 (1): 4964.</p>
</div>
<div>
<p>. 2001. Random Forests. <em>Machine Learning</em> 45 (1): 532.</p>
</div>
<div>
<p>Breiman, Leo, Jerome Friedman, Charles J. Stone, and R.A. Olshen. 1984. <em>Classification and Regression Trees</em>. Chapman; Hall.</p>
</div>
<div>
<p>Breiman, Leo, and others. 2004. Population Theory for Boosting Ensembles. <em>Annals of Statistics</em> 32 (1): 111.</p>
</div>
<div>
<p>Brodersen, Kay H, Fabian Gallusser, Jim Koehler, Nicolas Remy, Steven L Scott, and others. 2015. Inferring Causal Impact Using Bayesian Structural Time-Series Models. <em>Annals of Applied Statistics</em> 9 (1): 24774.</p>
</div>
<div>
<p>Brodie, Joshua, Ingrid Daubechies, Christine De Mol, Domenico Giannone, and Ignace Loris. 2009. Sparse and Stable Markowitz Portfolios. <em>Proceedings of the National Academy of Sciences</em> 106 (30): 1226772.</p>
</div>
<div>
<p>Brown, Iain, and Christophe Mues. 2012. An Experimental Comparison of Classification Algorithms for Imbalanced Credit Scoring Data Sets. <em>Expert Systems with Applications</em> 39 (3): 344653.</p>
</div>
<div>
<p>Bruder, Benjamin, Yazid Cheikh, Florent Deixonne, and Ban Zheng. 2019. Integration of Esg in Asset Allocation. <em>SSRN Working Paper</em> 3473874.</p>
</div>
<div>
<p>Bryzgalova, Svetlana. 2019. Spurious Factors in Linear Asset Pricing Models.</p>
</div>
<div>
<p>Bryzgalova, Svetlana, Jiantao Huang, and Christian Julliard. 2019. Bayesian Solutions for the Factor Zoo: We Just Ran Two Quadrillion Models. <em>SSRN Working Paper</em> 3481736.</p>
</div>
<div>
<p>Bryzgalova, Svetlana, Markus Pelger, and Jason Zhu. 2019. Forest Through the Trees: Building Cross-Sections of Stock Returns. <em>SSRN Working Paper</em> 3493458.</p>
</div>
<div>
<p>Buehler, Hans, Lukas Gonon, Josef Teichmann, and Ben Wood. 2019. Deep Hedging. <em>Quantitative Finance</em> 19 (8): 127191.</p>
</div>
<div>
<p>Burrell, Phillip R., and Bukola Otulayo Folarin. 1997. The Impact of Neural Networks in Finance. <em>Neural Computing &amp; Applications</em> 6 (4): 193200.</p>
</div>
<div>
<p>Bhlmann, Peter, Jonas Peters, Jan Ernest, and others. 2014. CAM: Causal Additive Models, High-Dimensional Order Search and Penalized Regression. <em>Annals of Statistics</em> 42 (6): 252656.</p>
</div>
<div>
<p>Campbell, John Y, and Motohiro Yogo. 2006. Efficient Tests of Stock Return Predictability. <em>Journal of Financial Economics</em> 81 (1): 2760.</p>
</div>
<div>
<p>Cao, Li-Juan, and Francis Eng Hock Tay. 2003. Support Vector Machine with Adaptive Parameters in Financial Time Series Forecasting. <em>IEEE Transactions on Neural Networks</em> 14 (6): 150618.</p>
</div>
<div>
<p>Carhart, Mark M. 1997. On Persistence in Mutual Fund Performance. <em>Journal of Finance</em> 52 (1): 5782.</p>
</div>
<div>
<p>Carlson, Murray, Adlai Fisher, and Ron Giammarino. 2004. Corporate Investment and Asset Price Dynamics: Implications for the Cross-Section of Returns. <em>Journal of Finance</em> 59 (6): 25772603.</p>
</div>
<div>
<p>Cattaneo, Matias D, Richard K Crump, Max Farrell, and Ernst Schaumburg. 2020. Characteristic-Sorted Portfolios: Estimation and Inference Forthcoming. Review of Economics; Statistics: 147.</p>
</div>
<div>
<p>Cazalet, Zlia, and Thierry Roncalli. 2014. Facts and Fantasies About Factor Investing. <em>SSRN Working Paper</em> 2524547.</p>
</div>
<div>
<p>Chakrabarti, Gagari, and Chitrakalpa Sen. 2020. Time Series Momentum Trading in Green Stocks. <em>Studies in Economics and Finance</em>.</p>
</div>
<div>
<p>Chandola, Varun, Arindam Banerjee, and Vipin Kumar. 2009. Anomaly Detection: A Survey. <em>ACM Computing Surveys (CSUR)</em> 41 (3): 15.</p>
</div>
<div>
<p>Chang, Chih-Chung, and Chih-Jen Lin. 2011. LIBSVM: A Library for Support Vector Machines. <em>ACM Transactions on Intelligent Systems and Technology (TIST)</em> 2 (3): 27.</p>
</div>
<div>
<p>Chaouki, Ayman, Stephen Hardiman, Christian Schmidt, Joachim de Lataillade, and others. 2020. Deep Deterministic Portfolio Optimization. <em>arXiv Preprint</em>, no. 2003.06497.</p>
</div>
<div>
<p>Charpentier, Arthur, Romuald Elie, and Carl Remlinger. 2020. Reinforcement Learning in Economics and Finance. <em>arXiv Preprint</em>, no. 2003.10014.</p>
</div>
<div>
<p>Che, Zhengping, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. 2018. Recurrent Neural Networks for Multivariate Time Series with Missing Values. <em>Scientific Reports</em> 8 (1): 6085.</p>
</div>
<div>
<p>Chen, Andrew Y, and Tom Zimmermann. 2020. Publication Bias and the Cross-Section of Stock Returns. <em>Review of Asset Pricing Studies</em> Forthcoming.</p>
</div>
<div>
<p>Chen, Huifen. 2001. Initialization for Norta: Generation of Random Vectors with Specified Marginals and Correlations. <em>INFORMS Journal on Computing</em> 13 (4). INFORMS: 31231.</p>
</div>
<div>
<p>Chen, Jianbo, Le Song, Martin J Wainwright, and Michael I Jordan. 2018. L-Shapley and c-Shapley: Efficient Model Interpretation for Structured Data. <em>arXiv Preprint</em>, no. 1808.02610.</p>
</div>
<div>
<p>Chen, Jou-Fan, Wei-Lun Chen, Chun-Ping Huang, Szu-Hao Huang, and An-Pin Chen. 2016. Financial Time-Series Data Analysis Using Deep Convolutional Neural Networks. In <em>2016 7th International Conference on Cloud Computing and Big Data (Ccbd)</em>, 8792. IEEE.</p>
</div>
<div>
<p>Chen, Long, Zhi Da, and Richard Priestley. 2012. Dividend Smoothing and Predictability. <em>Management Science</em> 58 (10): 183453.</p>
</div>
<div>
<p>Chen, Luyang, Markus Pelger, and Jason Zhu. 2020. Deep Learning in Asset Pricing. <em>SSRN Working Paper</em> 3350138.</p>
</div>
<div>
<p>Chen, Tianqi, and Carlos Guestrin. 2016. Xgboost: A Scalable Tree Boosting System. In <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, 78594. ACM.</p>
</div>
<div>
<p>Chen, Yingjun, and Yongtao Hao. 2017. A Feature Weighted Support Vector Machine and K-Nearest Neighbor Algorithm for Stock Market Indices Prediction. <em>Expert Systems with Applications</em> 80: 34055.</p>
</div>
<div>
<p>Chib, Siddhartha, Xiaming Zeng, and Lingxiao Zhao. 2019. On Comparing Asset Pricing Models. <em>Journal of Finance</em> Forthcoming.</p>
</div>
<div>
<p>Chinco, Alexander, Adam D Clark-Joseph, and Mao Ye. 2019. Sparse Signals in the Cross-Section of Returns. <em>Journal of Finance</em> 74 (1): 44992.</p>
</div>
<div>
<p>Chinco, Alexander, Andreas Neuhierl, and Michael Weber. 2020. Estimating the Anomaly Baserate. <em>Journal of Financial Economics</em> Forthcoming.</p>
</div>
<div>
<p>Chinco, Alex, Samuel M Hartzmark, and Abigail B Sussman. 2019. Risk-Factor Irrelevance. <em>SSRN Working Paper</em> 3487624.</p>
</div>
<div>
<p>Chipman, Hugh A, Edward I George, and Robert E McCulloch. 2010. BART: Bayesian Additive Regression Trees. <em>Annals of Applied Statistics</em> 4 (1): 26698.</p>
</div>
<div>
<p>Choi, Seung Mo, and Hwagyun Kim. 2014. Momentum Effect as Part of a Market Equilibrium. <em>Journal of Financial and Quantitative Analysis</em> 49 (1): 10730.</p>
</div>
<div>
<p>Chollet, Franois. 2017. <em>Deep Learning with Python</em>. Manning Publications Company.</p>
</div>
<div>
<p>Chordia, Tarun, Amit Goyal, and Alessio Saretto. 2020. Anomalies and False Rejections. <em>Review of Financial Studies</em> Forthcoming.</p>
</div>
<div>
<p>Chordia, Tarun, Amit Goyal, and Jay Shanken. 2019. Cross-Sectional Asset Pricing with Individual Stocks: Betas Versus Characteristics. <em>SSRN Working Paper</em> 2549578.</p>
</div>
<div>
<p>Chow, Ying-Foon, John A Cotsomitis, and Andy CC Kwan. 2002. Multivariate Cointegration and Causality Tests of Wagners Hypothesis: Evidence from the UK. <em>Applied Economics</em> 34 (13): 16717.</p>
</div>
<div>
<p>Chung, Junyoung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. 2015. Gated Feedback Recurrent Neural Networks. In <em>International Conference on Machine Learning</em>, 206775.</p>
</div>
<div>
<p>Claeskens, Gerda, and Nils Lid Hjort. 2008. <em>Model Selection and Model Averaging</em>. Cambridge University Press.</p>
</div>
<div>
<p>Clark, Todd E, and Michael W McCracken. 2009. Improving Forecast Accuracy by Combining Recursive and Rolling Forecasts. <em>International Economic Review</em> 50 (2): 36395.</p>
</div>
<div>
<p>Cocco, Joao F, Francisco Gomes, and Paula Lopes. 2020. Evidence on Expectations of Household Finances. <em>SSRN Working Paper</em> 3362495.</p>
</div>
<div>
<p>Cochrane, John H. 2009. <em>Asset Pricing: Revised Edition</em>. Princeton university press.</p>
</div>
<div>
<p>. 2011. Presidential Address: Discount Rates. <em>Journal of Finance</em> 66 (4): 10471108.</p>
</div>
<div>
<p>Cong, Lin William, Tengyuan Liang, and Xiao Zhang. 2019a. Analyzing Textual Information at Scale. <em>SSRN Working Paper</em> 3449822.</p>
</div>
<div>
<p>. 2019b. Textual Factors: A Scalable, Interpretable, and Data-Driven Approach to Analyzing Unstructured Information. <em>SSRN Working Paper</em> 3307057.</p>
</div>
<div>
<p>Cong, Lin William, and Douglas Xu. 2019. Rise of Factor Investing: Asset Prices, Informational Efficiency, and Security Design. <em>SSRN Working Paper</em> 2800590.</p>
</div>
<div>
<p>Connor, Gregory, and Robert A Korajczyk. 1988. Risk and Return in an Equilibrium Apt: Application of a New Test Methodology. <em>Journal of Financial Economics</em> 21 (2): 25589.</p>
</div>
<div>
<p>Cont, Rama. 2007. Volatility Clustering in Financial Markets: Empirical Facts and Agent-Based Models. In <em>Long Memory in Economics</em>, 289309. Springer.</p>
</div>
<div>
<p>Cooper, Ilan, and Paulo F Maio. 2019. New Evidence on Conditional Factor Models. <em>Journal of Financial and Quantitative Analysis</em> 54 (5): 19752016.</p>
</div>
<div>
<p>Coqueret, Guillaume. 2015. Diversified Minimum-Variance Portfolios. <em>Annals of Finance</em> 11 (2): 22141.</p>
</div>
<div>
<p>. 2017. Approximate Norta Simulations for Virtual Sample Generation. <em>Expert Systems with Applications</em> 73: 6981.</p>
</div>
<div>
<p>. 2020. Stock Specific Sentiment and Return Predictability. <em>Quantitative Finance</em> Forthcoming.</p>
</div>
<div>
<p>Coqueret, Guillaume, and Tony Guida. 2020. Training Trees on Tails with Applications to Portfolio Choice. <em>Annals of Operations Research</em> Forthcoming.</p>
</div>
<div>
<p>Cornuejols, Antoine, Laurent Miclet, and Vincent Barra. 2018. <em>Apprentissage Artificiel: Deep Learning, Concepts et Algorithmes</em>. Eyrolles.</p>
</div>
<div>
<p>Cortes, Corinna, and Vladimir Vapnik. 1995. Support-Vector Networks. <em>Machine Learning</em> 20 (3): 27397.</p>
</div>
<div>
<p>Costarelli, Danilo, Renato Spigler, and Gianluca Vinti. 2016. A Survey on Approximation by Means of Neural Network Operators. <em>Journal of NeuroTechnology</em> 1 (1).</p>
</div>
<div>
<p>Cover, Thomas M. 1991. Universal Portfolios. <em>Mathematical Finance</em> 1 (1): 129.</p>
</div>
<div>
<p>Cover, Thomas M, and Erik Ordentlich. 1996. Universal Portfolios with Side Information. <em>IEEE Transactions on Information Theory</em> 42 (2): 34863.</p>
</div>
<div>
<p>Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online Passive-Aggressive Algorithms. <em>Journal of Machine Learning Research</em> 7 (Mar): 55185.</p>
</div>
<div>
<p>Cronqvist, Henrik, Alessandro Previtero, Stephan Siegel, and Roderick E White. 2015. The Fetal Origins Hypothesis in Finance: Prenatal Environment, the Gender Gap, and Investor Behavior. <em>Review of Financial Studies</em> 29 (3): 73986.</p>
</div>
<div>
<p>Cronqvist, Henrik, Stephan Siegel, and Frank Yu. 2015. Value Versus Growth Investing: Why Do Different Investors Have Different Styles? <em>Journal of Financial Economics</em> 117 (2): 33349.</p>
</div>
<div>
<p>Cuchiero, Christa, Irene Klein, and Josef Teichmann. 2016. A New Perspective on the Fundamental Theorem of Asset Pricing for Large Financial Markets. <em>Theory of Probability &amp; Its Applications</em> 60 (4): 56179.</p>
</div>
<div>
<p>Cybenko, George. 1989. Approximation by Superpositions of a Sigmoidal Function. <em>Mathematics of Control, Signals and Systems</em> 2 (4): 30314.</p>
</div>
<div>
<p>Dangl, Thomas, and Michael Halling. 2012. Predictive Regressions with Time-Varying Coefficients. <em>Journal of Financial Economics</em> 106 (1): 15781.</p>
</div>
<div>
<p>Daniel, Kent D, David Hirshleifer, and Avanidhar Subrahmanyam. 2001. Overconfidence, Arbitrage, and Equilibrium Asset Pricing. <em>Journal of Finance</em> 56 (3): 92165.</p>
</div>
<div>
<p>Daniel, Kent, David Hirshleifer, and Lin Sun. 2020. Short and Long Horizon Behavioral Factors. <em>Review of Financial Studies</em> Forthcoming.</p>
</div>
<div>
<p>Daniel, Kent, and Tobias J Moskowitz. 2016. Momentum Crashes. <em>Journal of Financial Economics</em> 122 (2): 22147.</p>
</div>
<div>
<p>Daniel, Kent, Lira Mota, Simon Rottke, and Tano Santos. 2020. The Cross-Section of Risk and Return. <em>Review of Financial Studies</em> Forthcoming.</p>
</div>
<div>
<p>Daniel, Kent, and Sheridan Titman. 1997. Evidence on the Characteristics of Cross Sectional Variation in Stock Returns. <em>Journal of Finance</em> 52 (1): 133.</p>
</div>
<div>
<p>. 2012. Testing Factor-Model Explanations of Market Anomalies. <em>Critical Finance Review</em> 1 (1): 10339.</p>
</div>
<div>
<p>Daniel, Kent, Sheridan Titman, and KC John Wei. 2001. Explaining the Cross-Section of Stock Returns in Japan: Factors or Characteristics? <em>Journal of Finance</em> 56 (2): 74366.</p>
</div>
<div>
<p>dAspremont, Alexandre. 2011. Identifying Small Mean-Reverting Portfolios. <em>Quantitative Finance</em> 11 (3). Taylor &amp; Francis: 35164.</p>
</div>
<div>
<p>Delbaen, Freddy, and Walter Schachermayer. 1994. A General Version of the Fundamental Theorem of Asset Pricing. <em>Mathematische Annalen</em> 300 (1): 463520.</p>
</div>
<div>
<p>Demetrescu, Matei, Iliyan Georgiev, Paulo MM Rodrigues, and AM Taylor. 2020. Testing for Episodic Predictability in Stock Returns Forthcoming. Journal of Econometrics.</p>
</div>
<div>
<p>DeMiguel, Victor, Lorenzo Garlappi, and Raman Uppal. 2009. Optimal Versus Naive Diversification: How Inefficient Is the 1/N Portfolio Strategy? <em>Review of Financial Studies</em> 22 (5): 191553.</p>
</div>
<div>
<p>DeMiguel, Victor, Alberto Martin Utrera, and Raman Uppal. 2019. What Alleviates Crowding in Factor Investing? <em>SSRN Working Paper</em> 3392875.</p>
</div>
<div>
<p>De Moor, Lieven, Geert Dhaene, and Piet Sercu. 2015. On Comparing Zero-Alpha Tests Across Multifactor Asset Pricing Models. <em>Journal of Banking &amp; Finance</em> 61: S235S240.</p>
</div>
<div>
<p>Denil, Misha, David Matheson, and Nando De Freitas. 2014. Narrowing the Gap: Random Forests in Theory and in Practice. In <em>International Conference on Machine Learning</em>, 66573.</p>
</div>
<div>
<p>De Prado, Marcos Lopez. 2018. <em>Advances in Financial Machine Learning</em>. John Wiley &amp; Sons.</p>
</div>
<div>
<p>Dichtl, Hubert, Wolfgang Drobetz, Harald Lohre, Carsten Rother, and Patrick Vosskamp. 2019. Optimal Timing and Tilting of Equity Factors. <em>Financial Analysts Journal</em> 75 (4): 84102.</p>
</div>
<div>
<p>Dichtl, Hubert, Wolfgang Drobetz, Andreas Neuhierl, and Viktoria-Sophie Wendt. 2020. Data Snooping in Equity Premium Prediction. <em>Journal of Forecasting</em> Forthcoming.</p>
</div>
<div>
<p>Dingli, Alexiei, and Karl Sant Fournier. 2017. Financial Time Series Forecastinga Deep Learning Approach. <em>International Journal of Machine Learning and Computing</em> 7 (5): 11822.</p>
</div>
<div>
<p>Dixon, Matthew F. 2020. Industrial Forecasting with Exponentially Smoothed Recurrent Neural Networks. <em>SSRN Working Paper</em>, no. 3572181.</p>
</div>
<div>
<p>Dixon, Matthew F., Igor Halperin, and Paul Bilokon. 2020. <em>Machine Learning in Finance: From Theory to Practice</em>. Springer.</p>
</div>
<div>
<p>Donaldson, R Glen, and Mark Kamstra. 1996. Forecast Combining with Neural Networks. <em>Journal of Forecasting</em> 15 (1): 4961.</p>
</div>
<div>
<p>Drucker, Harris. 1997. Improving Regressors Using Boosting Techniques. In <em>International Conference on Machine Learning</em>, 97:10715.</p>
</div>
<div>
<p>Drucker, Harris, Christopher JC Burges, Linda Kaufman, Alex J Smola, and Vladimir Vapnik. 1997. Support Vector Regression Machines. In <em>Advances in Neural Information Processing Systems</em>, 15561.</p>
</div>
<div>
<p>Du, Ke-Lin, and Madisetti NS Swamy. 2013. <em>Neural Networks and Statistical Learning</em>. Springer Science &amp; Business Media.</p>
</div>
<div>
<p>Duchi, John, Elad Hazan, and Yoram Singer. 2011. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. <em>Journal of Machine Learning Research</em> 12 (Jul): 212159.</p>
</div>
<div>
<p>Dunis, Christian L, Spiros D Likothanassis, Andreas S Karathanasopoulos, Georgios S Sermpinis, and Konstantinos A Theofilatos. 2013. A Hybrid Genetic AlgorithmSupport Vector Machine Approach in the Task of Forecasting and Trading. <em>Journal of Asset Management</em> 14 (1): 5271.</p>
</div>
<div>
<p>Eakins, Stanley G, Stanley R Stansell, and James F Buck. 1998. Analyzing the Nature of Institutional Demand for Common Stocks. <em>Quarterly Journal of Business and Economics</em>. JSTOR, 3348.</p>
</div>
<div>
<p>Efimov, Dmitry, and Di Xu. 2019. Using Generative Adversarial Networks to Synthesize Artificial Financial Datasets. <em>Proceedings of the Conference on Neural Information Processing Systems</em>.</p>
</div>
<div>
<p>Ehsani, Sina, and Juhani T Linnainmaa. 2019. Factor Momentum and the Momentum Factor. <em>SSRN Working Paper</em> 3014521.</p>
</div>
<div>
<p>Elliott, Graham, Nikolay Kudrin, and Kaspar Wuthrich. 2019. Detecting P-Hacking. <em>arXiv Preprint</em>, no. 1906.06711.</p>
</div>
<div>
<p>Elman, Jeffrey L. 1990. Finding Structure in Time. <em>Cognitive Science</em> 14 (2): 179211.</p>
</div>
<div>
<p>Enders, Craig K. 2001. A Primer on Maximum Likelihood Algorithms Available for Use with Missing Data. <em>Structural Equation Modeling</em> 8 (1). Taylor &amp; Francis: 12841.</p>
</div>
<div>
<p>. 2010. <em>Applied Missing Data Analysis</em>. Guilford press.</p>
</div>
<div>
<p>Engilberge, Martin, Louis Chevallier, Patrick Prez, and Matthieu Cord. 2019. SoDeep: A Sorting Deep Net to Learn Ranking Loss Surrogates. In <em>Proceedings of the Ieee Conference on Computer Vision and Pattern Recognition</em>, 1079210801.</p>
</div>
<div>
<p>Engle, Robert F. 1982. Autoregressive Conditional Heteroscedasticity with Estimates of the Variance of United Kingdom Inflation. <em>Econometrica</em>, 9871007.</p>
</div>
<div>
<p>Enke, David, and Suraphan Thawornwong. 2005. The Use of Data Mining and Neural Networks for Forecasting Stock Market Returns. <em>Expert Systems with Applications</em> 29 (4): 92740.</p>
</div>
<div>
<p>Fabozzi, Frank J. 2020. Introduction: Special Issue on Ethical Investing. <em>Journal of Portfolio Management</em> 46 (3): 14.</p>
</div>
<div>
<p>Fabozzi, Frank J, and Marcos Lpez de Prado. 2018. Being Honest in Backtest Reporting: A Template for Disclosing Multiple Tests. <em>Journal of Portfolio Management</em> 45 (1): 14147.</p>
</div>
<div>
<p>Fama, Eugene F, and Kenneth R French. 1992. The Cross-Section of Expected Stock Returns. <em>Journal of Finance</em> 47 (2): 42765.</p>
</div>
<div>
<p>. 1993. Common Risk Factors in the Returns on Stocks and Bonds. <em>Journal of Financial Economics</em> 33 (1): 356.</p>
</div>
<div>
<p>. 2015. A Five-Factor Asset Pricing Model. <em>Journal of Financial Economics</em> 116 (1): 122.</p>
</div>
<div>
<p>. 2018. Choosing Factors. <em>Journal of Financial Economics</em> 128 (2): 23452.</p>
</div>
<div>
<p>Fama, Eugene F, and James D MacBeth. 1973. Risk, Return, and Equilibrium: Empirical Tests. <em>Journal of Political Economy</em> 81 (3): 60736.</p>
</div>
<div>
<p>Farmer, Leland, Lawrence Schmidt, and Allan Timmermann. 2019. Pockets of Predictability. <em>SSRN Working Paper</em> 3152386.</p>
</div>
<div>
<p>Fastrich, Bjrn, Sandra Paterlini, and Peter Winker. 2015. Constructing Optimal Sparse Portfolios Using Regularization Methods. <em>Computational Management Science</em> 12 (3): 41734.</p>
</div>
<div>
<p>Feng, Guanhao, Stefano Giglio, and Dacheng Xiu. 2020. Taming the Factor Zoo: A Test of New Factors. <em>Journal of Finance</em> Forthcoming.</p>
</div>
<div>
<p>Feng, Guanhao, Nicholas G Polson, and Jianeng Xu. 2019. Deep Learning in Characteristics-Sorted Factor Models. <em>SSRN Working Paper</em> 3243683.</p>
</div>
<div>
<p>Fischer, Thomas, and Christopher Krauss. 2018. Deep Learning with Long Short-Term Memory Networks for Financial Market Predictions. <em>European Journal of Operational Research</em> 270 (2): 65469.</p>
</div>
<div>
<p>Fisher, Aaron, Cynthia Rudin, and Francesca Dominici. 2018. All Models Are Wrong but Many Are Useful: Variable Importance for Black-Box, Proprietary, or Misspecified Prediction Models, Using Model Class Reliance. <em>arXiv Preprint</em>, no. 1801.01489.</p>
</div>
<div>
<p>Franco, Carmine de, Christophe Geissler, Vincent Margot, and Bruno Monnier. 2020. ESG Investments: Filtering Versus Machine Learning Approaches. <em>arXiv Preprint</em>, no. 2002.07477.</p>
</div>
<div>
<p>Frazier, Peter I. 2018. A Tutorial on Bayesian Optimization. <em>arXiv Preprint</em>, no. 1807.02811.</p>
</div>
<div>
<p>Frazzini, Andrea, and Lasse Heje Pedersen. 2014. Betting Against Beta. <em>Journal of Financial Economics</em> 111 (1): 125.</p>
</div>
<div>
<p>Freeman, Robert N, and Senyo Y Tse. 1992. A Nonlinear Model of Security Price Responses to Unexpected Earnings. <em>Journal of Accounting Research</em>, 185209.</p>
</div>
<div>
<p>Freund, Yoav, and Robert E Schapire. 1996. Experiments with a New Boosting Algorithm. In <em>Machine Learning: Proceedings of the Thirteenth International Conference</em>, 96:14856.</p>
</div>
<div>
<p>. 1997. A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting. <em>Journal of Computer and System Sciences</em> 55 (1): 11939.</p>
</div>
<div>
<p>Freyberger, Joachim, Andreas Neuhierl, and Michael Weber. 2020. Dissecting Characteristics Nonparametrically. <em>Review of Financial Studies</em> Forthcoming.</p>
</div>
<div>
<p>Friede, Gunnar, Timo Busch, and Alexander Bassen. 2015. ESG and Financial Performance: Aggregated Evidence from More Than 2000 Empirical Studies. <em>Journal of Sustainable Finance &amp; Investment</em> 5 (4): 21033.</p>
</div>
<div>
<p>Friedman, Jerome H. 2001. Greedy Function Approximation: A Gradient Boosting Machine. <em>Annals of Statistics</em>, 11891232.</p>
</div>
<div>
<p>. 2002. Stochastic Gradient Boosting. <em>Computational Statistics &amp; Data Analysis</em> 38 (4): 36778.</p>
</div>
<div>
<p>Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2008. Sparse Inverse Covariance Estimation with the Graphical Lasso. <em>Biostatistics</em> 9 (3): 43241.</p>
</div>
<div>
<p>Friedman, Jerome, Trevor Hastie, Robert Tibshirani, and others. 2000. Additive Logistic Regression: A Statistical View of Boosting (with Discussion and a Rejoinder by the Authors). <em>Annals of Statistics</em> 28 (2): 337407.</p>
</div>
<div>
<p>Friedman, Nir, Dan Geiger, and Moises Goldszmidt. 1997. Bayesian Network Classifiers. <em>Machine Learning</em> 29 (2-3): 13163.</p>
</div>
<div>
<p>Fu, XingYu, JinHong Du, YiFeng Guo, MingWen Liu, Tao Dong, and XiuWen Duan. 2018. A Machine Learning Framework for Stock Selection. <em>arXiv Preprint</em>, no. 1806.01743.</p>
</div>
<div>
<p>Gaba, Anil, Ilia Tsetlin, and Robert L Winkler. 2017. Combining Interval Forecasts. <em>Decision Analysis</em> 14 (1): 120.</p>
</div>
<div>
<p>Gagliardini, Patrick, Elisa Ossola, and Olivier Scaillet. 2016. Time-Varying Risk Premium in Large Cross-Sectional Equity Data Sets. <em>Econometrica</em> 84 (3): 9851046.</p>
</div>
<div>
<p>. 2019. Estimation of Large Dimensional Conditional Factor Models in Finance. <em>SSRN Working Paper</em> 3443426.</p>
</div>
<div>
<p>Galema, Rients, Auke Plantinga, and Bert Scholtens. 2008. The Stocks at Stake: Return and Risk in Socially Responsible Investment. <em>Journal of Banking &amp; Finance</em> 32 (12): 264654.</p>
</div>
<div>
<p>Galili, Tal, and Isaac Meilijson. 2016. Splitting Matters: How Monotone Transformation of Predictor Variables May Improve the Predictions of Decision Tree Models. <em>arXiv Preprint</em>, no. 1611.04561.</p>
</div>
<div>
<p>Garc'a-Galicia, Mauricio, Alin A Carsteanu, and Julio B Clempner. 2019. Continuous-Time Reinforcement Learning Approach for Portfolio Management with Time Penalization. <em>Expert Systems with Applications</em> 129: 2736.</p>
</div>
<div>
<p>Garc'a-Laencina, Pedro J, Jos-Luis Sancho-Gmez, An'bal R Figueiras-Vidal, and Michel Verleysen. 2009. K Nearest Neighbours with Mutual Information for Simultaneous Classification and Missing Data Imputation. <em>Neurocomputing</em> 72 (7-9). Elsevier: 148393.</p>
</div>
<div>
<p>Gelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. 2013. <em>Bayesian Data Analysis, 3rd Edition</em>. Chapman; Hall / CRC.</p>
</div>
<div>
<p>Geman, Stuart, Elie Bienenstock, and Ren Doursat. 1992. Neural Networks and the Bias/Variance Dilemma. <em>Neural Computation</em> 4 (1): 158.</p>
</div>
<div>
<p>Genre, Vronique, Geoff Kenny, Aidan Meyler, and Allan Timmermann. 2013. Combining Expert Forecasts: Can Anything Beat the Simple Average? <em>International Journal of Forecasting</em> 29 (1): 10821.</p>
</div>
<div>
<p>Gentzkow, Matthew, Bryan Kelly, and Matt Taddy. 2019. Text as Data. <em>Journal of Economic Literature</em> 57 (3): 53574.</p>
</div>
<div>
<p>Ghosh, Anil K. 2006. On Optimum Choice of K in Nearest Neighbor Classification. <em>Computational Statistics &amp; Data Analysis</em> 50 (11): 311323.</p>
</div>
<div>
<p>Giglio, Stefano, and Dacheng Xiu. 2019. Asset Pricing with Omitted Factors. <em>SSRN Working Paper</em> 2865922.</p>
</div>
<div>
<p>Gomes, Joao, Leonid Kogan, and Lu Zhang. 2003. Equilibrium Cross Section of Returns. <em>Journal of Political Economy</em> 111 (4): 693732.</p>
</div>
<div>
<p>Gong, Qiang, Ming Liu, and Qianqiu Liu. 2015. Momentum Is Really Short-Term Momentum. <em>Journal of Banking &amp; Finance</em> 50: 16982.</p>
</div>
<div>
<p>Gonzalo, Jess, and Jean-Yves Pitarakis. 2018. Predictive Regressions. In <em>Oxford Research Encyclopedia of Economics and Finance</em>.</p>
</div>
<div>
<p>Goodfellow, Ian, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. 2016. <em>Deep Learning</em>. MIT press Cambridge.</p>
</div>
<div>
<p>Goodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative Adversarial Nets. In <em>Advances in Neural Information Processing Systems</em>, 267280.</p>
</div>
<div>
<p>Gospodinov, Nikolay, Raymond Kan, and Cesare Robotti. 2019. Too Good to Be True? Fallacies in Evaluating Risk Factor Models. <em>Journal of Financial Economics</em> 132 (2): 45171.</p>
</div>
<div>
<p>Goto, Shingo, and Yan Xu. 2015. Improving Mean Variance Optimization Through Sparse Hedging Restrictions. <em>Journal of Financial and Quantitative Analysis</em> 50 (6): 141541.</p>
</div>
<div>
<p>Gower, John C. 1971. A General Coefficient of Similarity and Some of Its Properties. <em>Biometrics</em>, 85771.</p>
</div>
<div>
<p>Goyal, Amit. 2012. Empirical Cross-Sectional Asset Pricing: A Survey. <em>Financial Markets and Portfolio Management</em> 26 (1): 338.</p>
</div>
<div>
<p>Goyal, Amit, and Sunil Wahal. 2015. Is Momentum an Echo? <em>Journal of Financial and Quantitative Analysis</em> 50 (6): 123767.</p>
</div>
<div>
<p>Granger, Clive WJ. 1969. Investigating Causal Relations by Econometric Models and Cross-Spectral Methods. <em>Econometrica</em>, 42438.</p>
</div>
<div>
<p>Green, Jeremiah, John RM Hand, and X Frank Zhang. 2013. The Supraview of Return Predictive Signals. <em>Review of Accounting Studies</em> 18 (3): 692730.</p>
</div>
<div>
<p>. 2017. The Characteristics That Provide Independent Information About Average Us Monthly Stock Returns. <em>Review of Financial Studies</em> 30 (12): 43894436.</p>
</div>
<div>
<p>Greene, William H. 2018. <em>Econometric Analysis, Eighth Edition</em>. Pearson Education.</p>
</div>
<div>
<p>Greenwell, Brandon M. 2017. Pdp: An R Package for Constructing Partial Dependence Plots. <em>R Journal</em> 9 (1): 42136.</p>
</div>
<div>
<p>Greenwell, Brandon M, and Bradley C Boehmke. n.d. Variable Importance Plots: An Introduction to the Vip Package. <em>R Journal</em>.</p>
</div>
<div>
<p>Greenwood, Robin, and Samuel G Hanson. 2012. Share Issuance and Factor Timing. <em>Journal of Finance</em> 67 (2): 76198.</p>
</div>
<div>
<p>Grinblatt, Mark, and Bing Han. 2005. Prospect Theory, Mental Accounting, and Momentum. <em>Journal of Financial Economics</em> 78 (2). Elsevier: 31139.</p>
</div>
<div>
<p>Grushka-Cockayne, Yael, Victor Richmond R Jose, and Kenneth C Lichtendahl Jr. 2016. Ensembles of Overfit and Overconfident Forecasts. <em>Management Science</em> 63 (4): 111030.</p>
</div>
<div>
<p>Gu, Shihao, Bryan T Kelly, and Dacheng Xiu. 2020a. Autoencoder Asset Pricing Models. <em>Journal of Econometrics</em> Forthcoming.</p>
</div>
<div>
<p>. 2020b. Empirical Asset Pricing via Machine Learning. <em>Review of Financial Studies</em> Forthcoming.</p>
</div>
<div>
<p>Guida, Tony, and Guillaume Coqueret. 2018a. Ensemble Learning Applied to Quant Equity: Gradient Boosting in a Multifactor Framework. In <em>Big Data and Machine Learning in Quantitative Investment</em>, 12948. Wiley.</p>
</div>
<div>
<p>. 2018b. Machine Learning in Systematic Equity Allocation: A Model Comparison. <em>Wilmott</em> 2018 (98): 2433.</p>
</div>
<div>
<p>Guliyev, Namig J, and Vugar E Ismailov. 2018. On the Approximation by Single Hidden Layer Feedforward Neural Networks with Fixed Weights. <em>Neural Networks</em> 98: 296304.</p>
</div>
<div>
<p>Gupta, Manish, Jing Gao, Charu Aggarwal, and Jiawei Han. 2014. Outlier Detection for Temporal Data. <em>IEEE Transactions on Knowledge and Data Engineering</em> 26 (9): 225067.</p>
</div>
<div>
<p>Gupta, Tarun, and Bryan Kelly. 2019. Factor Momentum Everywhere. <em>Journal of Portfolio Management</em> 45 (3): 1336.</p>
</div>
<div>
<p>Guresen, Erkam, Gulgun Kayakutlu, and Tugrul U Daim. 2011. Using Artificial Neural Network Models in Stock Market Index Prediction. <em>Expert Systems with Applications</em> 38 (8): 1038997.</p>
</div>
<div>
<p>Guyon, Isabelle, and Andr Elisseeff. 2003. An Introduction to Variable and Feature Selection. <em>Journal of Lachine Learning Research</em> 3 (Mar): 115782.</p>
</div>
<div>
<p>Haddad, Valentin, Serhiy Kozak, and Shrihari Santosh. 2020. Factor Timing. <em>Review of Financial Studies</em> Forthcoming.</p>
</div>
<div>
<p>Hahn, P Richard, Jared S Murray, and Carlos Carvalho. 2019. Bayesian Regression Tree Models for Causal Inference: Regularization, Confounding, and Heterogeneous Effects. <em>arXiv Preprint</em>, no. 1706.09523.</p>
</div>
<div>
<p>Hall, Patrick, and Navdeep Gill. 2019. <em>An Introduction to Machine Learning Interpretability - Second Edition</em>. OReilly.</p>
</div>
<div>
<p>Hall, Peter, Byeong U Park, Richard J Samworth, and others. 2008. Choice of Neighbor Order in Nearest-Neighbor Classification. <em>Annals of Statistics</em> 36 (5): 213552.</p>
</div>
<div>
<p>Halperin, Igor, and Ilya Feldshteyn. 2018. Market Self-Learning of Signals, Impact and Optimal Trading: Invisible Hand Inference with Free Energy. <em>arXiv Preprint</em>, no. 1805.06126.</p>
</div>
<div>
<p>Han, Yufeng, Ai He, D Rapach, and Guofu Zhou. 2019. Firm Characteristics and Expected Stock Returns. <em>SSRN Working Paper</em> 3185335.</p>
</div>
<div>
<p>Hansen, Lars Peter. 1982. Large Sample Properties of Generalized Method of Moments Estimators. <em>Econometrica</em>, 102954.</p>
</div>
<div>
<p>Harrald, Paul G, and Mark Kamstra. 1997. Evolving Artificial Neural Networks to Combine Financial Forecasts. <em>IEEE Transactions on Evolutionary Computation</em> 1 (1): 4052.</p>
</div>
<div>
<p>Hartzmark, Samuel M, and David H Solomon. 2019. The Dividend Disconnect. <em>Journal of Finance</em> 74 (5): 215399.</p>
</div>
<div>
<p>Harvey, Campbell, and Yan Liu. 2019. Lucky Factors. <em>SSRN Working Paper</em> 2528780.</p>
</div>
<div>
<p>Harvey, Campbell R. 2017. Presidential Address: The Scientific Outlook in Financial Economics. <em>Journal of Finance</em> 72 (4): 13991440.</p>
</div>
<div>
<p>. 2020. Replication in Financial Economics. <em>Critical Finance Review</em>, 19.</p>
</div>
<div>
<p>Harvey, Campbell R, John C Liechty, Merrill W Liechty, and Peter Mller. 2010. Portfolio Selection with Higher Moments. <em>Quantitative Finance</em> 10 (5): 46985.</p>
</div>
<div>
<p>Harvey, Campbell R, and Yan Liu. 2015. Backtesting. <em>Journal of Portfolio Management</em> 42 (1): 1328.</p>
</div>
<div>
<p>. 2019a. A Census of the Factor Zoo. <em>SSRN Working Paper</em> 3341728.</p>
</div>
<div>
<p>. 2019b. False (and Missed) Discoveries in Financial Economics. <em>SSRN Working Paper</em> 3073799.</p>
</div>
<div>
<p>Harvey, Campbell R, Yan Liu, and Heqing Zhu. 2016.  And the Cross-Section of Expected Returns. <em>Review of Financial Studies</em> 29 (1): 568.</p>
</div>
<div>
<p>Hassan, Md Rafiul, Baikunth Nath, and Michael Kirley. 2007. A Fusion Model of Hmm, Ann and Ga for Stock Market Forecasting. <em>Expert Systems with Applications</em> 33 (1): 17180.</p>
</div>
<div>
<p>Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning</em>. Springer.</p>
</div>
<div>
<p>Haykin, Simon S. 2009. <em>Neural Networks and Learning Machines</em>. Prentice Hall.</p>
</div>
<div>
<p>Hazan, Elad, Amit Agarwal, and Satyen Kale. 2007. Logarithmic Regret Algorithms for Online Convex Optimization. <em>Machine Learning</em> 69 (2-3): 16992.</p>
</div>
<div>
<p>Hazan, Elad, and others. 2016. Introduction to Online Convex Optimization. <em>Foundations and Trends in Optimization</em> 2 (3-4). Now Publishers, Inc.: 157325.</p>
</div>
<div>
<p>He, Ai, Dashan Huang, and Guofu Zhou. 2020. New Factors Wanted: Evidence from a Simple Specification Test. <em>SSRN Working Paper</em> 3143752.</p>
</div>
<div>
<p>Head, Megan L, Luke Holman, Rob Lanfear, Andrew T Kahn, and Michael D Jennions. 2015. The Extent and Consequences of P-Hacking in Science. <em>PLoS Biology</em> 13 (3): e1002106.</p>
</div>
<div>
<p>Heinze-Deml, Christina, Jonas Peters, and Nicolai Meinshausen. 2018. Invariant Causal Prediction for Nonlinear Models. <em>Journal of Causal Inference</em> 6 (2).</p>
</div>
<div>
<p>Henkel, Sam James, J Spencer Martin, and Federico Nardari. 2011. Time-Varying Short-Horizon Predictability. <em>Journal of Financial Economics</em> 99 (3): 56080.</p>
</div>
<div>
<p>Henrique, Bruno Miranda, Vinicius Amorim Sobreiro, and Herbert Kimura. 2019. Literature Review: Machine Learning Techniques Applied to Financial Market Prediction. <em>Expert Systems with Applications</em> 124: 22651.</p>
</div>
<div>
<p>Hiemstra, Craig, and Jonathan D Jones. 1994. Testing for Linear and Nonlinear Granger Causality in the Stock Price-Volume Relation. <em>Journal of Finance</em> 49 (5): 163964.</p>
</div>
<div>
<p>Hill, Ronald Paul, Thomas Ainscough, Todd Shank, and Daryl Manullang. 2007. Corporate Social Responsibility and Socially Responsible Investing: A Global Perspective. <em>Journal of Business Ethics</em> 70 (2): 16574.</p>
</div>
<div>
<p>Hjalmarsson, Erik. 2011. New Methods for Inference in Long-Horizon Regressions. <em>Journal of Financial and Quantitative Analysis</em> 46 (3): 81539.</p>
</div>
<div>
<p>Hjalmarsson, Erik, and Petar Manchev. 2012. Characteristic-Based Mean-Variance Portfolio Choice. <em>Journal of Banking &amp; Finance</em> 36 (5): 13921401.</p>
</div>
<div>
<p>Ho, Tin Kam. 1995. Random Decision Forests. In <em>Proceedings of 3rd International Conference on Document Analysis and Recognition</em>, 1:27882. IEEE.</p>
</div>
<div>
<p>Ho, Yu-Chi, and David L Pepyne. 2002. Simple Explanation of the No-Free-Lunch Theorem and Its Implications. <em>Journal of Optimization Theory and Applications</em> 115 (3): 54970.</p>
</div>
<div>
<p>Hochreiter, Sepp, and Jrgen Schmidhuber. 1997. Long Short-Term Memory. <em>Neural Computation</em> 9 (8). MIT Press: 173580.</p>
</div>
<div>
<p>Hodge, Victoria, and Jim Austin. 2004. A Survey of Outlier Detection Methodologies. <em>Artificial Intelligence Review</em> 22 (2): 85126.</p>
</div>
<div>
<p>Hodges, Philip, Ked Hogan, Justin R Peterson, and Andrew Ang. 2017. Factor Timing with Cross-Sectional and Time-Series Predictors. <em>Journal of Portfolio Management</em> 44 (1): 3043.</p>
</div>
<div>
<p>Hoechle, Daniel, Markus Schmid, and Heinz Zimmermann. 2018. Correcting Alpha Misattribution in Portfolio Sorts. <em>SSRN Working Paper</em> 3190310.</p>
</div>
<div>
<p>Hoi, Steven CH, Doyen Sahoo, Jing Lu, and Peilin Zhao. 2018. Online Learning: A Comprehensive Survey. <em>arXiv Preprint</em>, no. 1802.02871.</p>
</div>
<div>
<p>Honaker, James, and Gary King. 2010. What to Do About Missing Values in Time-Series Cross-Section Data. <em>American Journal of Political Science</em> 54 (2): 56181.</p>
</div>
<div>
<p>Hong, Harrison, G Andrew Karolyi, and Jos A Scheinkman. 2020. Climate Finance. <em>Review of Financial Studies</em> 33 (3): 101123.</p>
</div>
<div>
<p>Hong, Harrison, Frank Weikai Li, and Jiangmin Xu. 2019. Climate Risks and Market Efficiency. <em>Journal of Econometrics</em> 208 (1): 26581.</p>
</div>
<div>
<p>Horel, Enguerrand, and Kay Giesecke. 2019. Towards Explainable AI: Significance Tests for Neural Networks. <em>arXiv Preprint</em>, no. 1902.06021.</p>
</div>
<div>
<p>Hoseinzade, Ehsan, and Saman Haratizadeh. 2019. CNNpred: CNN-Based Stock Market Prediction Using a Diverse Set of Variables. <em>Expert Systems with Applications</em> 129: 27385.</p>
</div>
<div>
<p>Hou, Kewei, Chen Xue, and Lu Zhang. 2015. Digesting Anomalies: An Investment Approach. <em>Review of Financial Studies</em> 28 (3): 650705.</p>
</div>
<div>
<p>. 2020. Replicating Anomalies. <em>Review of Financial Studies</em> Forthcoming.</p>
</div>
<div>
<p>Hsu, Po-Hsuan, Qiheng Han, Wensheng Wu, and Zhiguang Cao. 2018. Asset Allocation Strategies, Data Snooping, and the 1/N Rule. <em>Journal of Banking &amp; Finance</em> 97: 25769.</p>
</div>
<div>
<p>Huang, Wei, Yoshiteru Nakamori, and Shou-Yang Wang. 2005. Forecasting Stock Market Movement Direction with Support Vector Machine. <em>Computers &amp; Operations Research</em> 32 (10): 251322.</p>
</div>
<div>
<p>Huck, Nicolas. 2019. Large Data Sets and Machine Learning: Applications to Statistical Arbitrage. <em>European Journal of Operational Research</em> 278 (1): 33042.</p>
</div>
<div>
<p>Hbner, Georges. 2005. The Generalized Treynor Ratio. <em>Review of Finance</em> 9 (3): 41535.</p>
</div>
<div>
<p>Hnermund, Paul, and Elias Bareinboim. 2019. Causal Inference and Data-Fusion in Econometrics. <em>arXiv Preprint</em>, no. 1912.09104.</p>
</div>
<div>
<p>Ilmanen, Antti. 2011. <em>Expected Returns: An Investors Guide to Harvesting Market Rewards</em>. John Wiley &amp; Sons.</p>
</div>
<div>
<p>Ilmanen, Antti, Ronen Israel, Tobias J Moskowitz, Ashwin K Thapar, and Franklin Wang. 2019. Factor Premia and Factor Timing: A Century of Evidence. <em>SSRN Working Paper</em> 3400998.</p>
</div>
<div>
<p>Jacobs, Heiko, and Sebastian Mller. 2020. Anomalies Across the Globe: Once Public, No Longer Existent? <em>Journal of Financial Economics</em> 135 (1): 21330.</p>
</div>
<div>
<p>Jacobs, Robert A, Michael I Jordan, Steven J Nowlan, Geoffrey E Hinton, and others. 1991. Adaptive Mixtures of Local Experts. <em>Neural Computation</em> 3 (1): 7987.</p>
</div>
<div>
<p>Jagannathan, Ravi, and Tongshu Ma. 2003. Risk Reduction in Large Portfolios: Why Imposing the Wrong Constraints Helps. <em>Journal of Finance</em> 58 (4): 165183.</p>
</div>
<div>
<p>Jagannathan, Ravi, and Zhenyu Wang. 1998. An Asymptotic Theory for Estimating Beta-Pricing Models Using Cross-Sectional Regression. <em>Journal of Finance</em> 53 (4): 12851309.</p>
</div>
<div>
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. Vol. 112. Springer.</p>
</div>
<div>
<p>Jegadeesh, Narasimhan, Joonki Noh, Kuntara Pukthuanthong, Richard Roll, and Junbo L Wang. 2019. Empirical Tests of Asset Pricing Models with Individual Assets: Resolving the Errors-in-Variables Bias in Risk Premium Estimation. <em>Journal of Financial Economics</em> 133 (2): 27398.</p>
</div>
<div>
<p>Jegadeesh, Narasimhan, and Sheridan Titman. 1993. Returns to Buying Winners and Selling Losers: Implications for Stock Market Efficiency. <em>Journal of Finance</em> 48 (1): 6591.</p>
</div>
<div>
<p>Jensen, Michael C. 1968. The Performance of Mutual Funds in the Period 19451964. <em>Journal of Finance</em> 23 (2): 389416.</p>
</div>
<div>
<p>Jha, Vinesh. 2019. Implementing Alternative Data in an Investment Process. In <em>Big Data and Machine Learning in Quantitative Investment</em>, 5174. Wiley.</p>
</div>
<div>
<p>Jiang, Weiwei. 2020. Applications of Deep Learning in Stock Market Prediction: Recent Progress. <em>arXiv Preprint</em>, no. 2003.01859.</p>
</div>
<div>
<p>Jiang, Zhengyao, Dixing Xu, and Jinjun Liang. 2017. A Deep Reinforcement Learning Framework for the Financial Portfolio Management Problem. <em>arXiv Preprint</em>, no. 1706.10059.</p>
</div>
<div>
<p>Johnson, Timothy C. 2002. Rational Momentum Effects. <em>Journal of Finance</em> 57 (2). Wiley Online Library: 585608.</p>
</div>
<div>
<p>Jordan, Michael I. 1997. Serial Order: A Parallel Distributed Processing Approach. In <em>Advances in Psychology</em>, 121:47195. Elsevier.</p>
</div>
<div>
<p>Jurczenko, Emmanuel. 2017. <em>Factor Investing: From Traditional to Alternative Risk Premia</em>. Elsevier.</p>
</div>
<div>
<p>Kalisch, Markus, Martin Mchler, Diego Colombo, Marloes H Maathuis, Peter Bhlmann, and others. 2012. Causal Inference Using Graphical Models with the R Package Pcalg. <em>Journal of Statistical Software</em> 47 (11): 126.</p>
</div>
<div>
<p>Ke, Guolin, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 2017. Lightgbm: A Highly Efficient Gradient Boosting Decision Tree. In <em>Advances in Neural Information Processing Systems</em>, 314654.</p>
</div>
<div>
<p>Ke, Zheng Tracy, Bryan T Kelly, and Dacheng Xiu. 2019. Predicting Returns with Text Data. <em>SSRN Working Paper</em> 3388293.</p>
</div>
<div>
<p>Kearns, Michael, and Yuriy Nevmyvaka. 2013. Machine Learning for Market Microstructure and High Frequency Trading. <em>High Frequency Trading: New Realities for Traders, Markets, and Regulators</em>.</p>
</div>
<div>
<p>Kelly, Bryan T, Seth Pruitt, and Yinan Su. 2019. Characteristics Are Covariances: A Unified Model of Risk and Return. <em>Journal of Financial Economics</em> 134 (3): 50124.</p>
</div>
<div>
<p>Kempf, Alexander, and Peer Osthoff. 2007. The Effect of Socially Responsible Investing on Portfolio Performance. <em>European Financial Management</em> 13 (5): 90822.</p>
</div>
<div>
<p>Kim, Kyoung-jae. 2003. Financial Time Series Forecasting Using Support Vector Machines. <em>Neurocomputing</em> 55 (1-2). Elsevier: 30719.</p>
</div>
<div>
<p>Kim, Soohun, Robert A Korajczyk, and Andreas Neuhierl. 2019. Arbitrage Portfolios. <em>SSRN Working Paper</em> 3263001.</p>
</div>
<div>
<p>Kimoto, Takashi, Kazuo Asakawa, Morio Yoda, and Masakazu Takeoka. 1990. Stock Market Prediction System with Modular Neural Networks. In <em>1990 Ijcnn International Joint Conference on Neural Networks</em>, 16. IEEE.</p>
</div>
<div>
<p>Kingma, Diederik P, and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimization. <em>arXiv Preprint</em>, no. 1412.6980.</p>
</div>
<div>
<p>Koijen, Ralph SJ, Robert J Richmond, and Motohiro Yogo. 2019. Which Investors Matter for Global Equity Valuations and Expected Returns? <em>SSRN Working Paper</em> 3378340.</p>
</div>
<div>
<p>Koijen, Ralph S.J., and Motohiro Yogo. 2019. A Demand System Approach to Asset Pricing. <em>Journal of Political Economy</em> 127 (4): 14751515.</p>
</div>
<div>
<p>Kolm, Petter N, and Gordon Ritter. 2019a. Dynamic Replication and Hedging: A Reinforcement Learning Approach. <em>Journal of Financial Data Science</em> 1 (1): 15971.</p>
</div>
<div>
<p>. 2019b. Modern Perspectives on Reinforcement Learning in Finance. <em>Journal of Machine Learning in Finance</em> 1 (1).</p>
</div>
<div>
<p>Kong, Weiwei, Christopher Liaw, Aranyak Mehta, and D Sivakumar. 2019. A New Dog Learns Old Tricks: RL Finds Classic Optimization Algorithms. <em>Proceedings of the ICLR Conference</em>, 125.</p>
</div>
<div>
<p>Koshiyama, Adriano, Sebastian Flennerhag, Stefano B Blumberg, Nick Firoozye, and Philip Treleaven. 2020. QuantNet: Transferring Learning Across Systematic Trading Strategies. <em>arXiv Preprint</em>, no. 2004.03445.</p>
</div>
<div>
<p>Kozak, Serhiy, Stefan Nagel, and Shrihari Santosh. 2018. Interpreting Factor Models. <em>Journal of Finance</em> 73 (3): 11831223.</p>
</div>
<div>
<p>. 2019. Shrinking the Cross-Section. <em>Journal of Financial Economics</em> 135: 27192.</p>
</div>
<div>
<p>Krauss, Christopher, Xuan Anh Do, and Nicolas Huck. 2017. Deep Neural Networks, Gradient-Boosted Trees, Random Forests: Statistical Arbitrage on the S&amp;P 500. <em>European Journal of Operational Research</em> 259 (2): 689702.</p>
</div>
<div>
<p>Kremer, Philipp J, Sangkyun Lee, Magorzata Bogdan, and Sandra Paterlini. 2019. Sparse Portfolio Selection via the Sorted L1-Norm. <em>Journal of Banking &amp; Finance</em>, 105687.</p>
</div>
<div>
<p>Krkoska, Eduard, and Klaus Reiner Schenk-Hopp. 2019. Herding in Smart-Beta Investment Products. <em>Journal of Risk and Financial Management</em> 12 (1): 47.</p>
</div>
<div>
<p>Kruschke, John. 2014. <em>Doing Bayesian Data Analysis: A Tutorial with R, Jags, and Stan (2nd Ed.)</em>. Academic Press.</p>
</div>
<div>
<p>Kuhn, Max, and Kjell Johnson. 2019. <em>Feature Engineering and Selection: A Practical Approach for Predictive Models</em>. CRC Press.</p>
</div>
<div>
<p>Kurtz, Lloyd. 2020. Three Pillars of Modern Responsible Investment. <em>Journal of Investing</em> 29 (2): 2132.</p>
</div>
<div>
<p>Lakonishok, Josef, Andrei Shleifer, and Robert W Vishny. 1994. Contrarian Investment, Extrapolation, and Risk. <em>Journal of Finance</em> 49 (5): 154178.</p>
</div>
<div>
<p>Leary, Mark T, and Roni Michaely. 2011. Determinants of Dividend Smoothing: Empirical Evidence. <em>Review of Financial Studies</em> 24 (10): 31973249.</p>
</div>
<div>
<p>Ledoit, Oliver, and Michael Wolf. 2008. Robust Performance Hypothesis Testing with the Sharpe Ratio. <em>Journal of Empirical Finance</em> 15 (5): 85059.</p>
</div>
<div>
<p>Ledoit, Olivier, and Michael Wolf. 2004. A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices. <em>Journal of Multivariate Analysis</em> 88 (2): 365411.</p>
</div>
<div>
<p>. 2017. Nonlinear Shrinkage of the Covariance Matrix for Portfolio Selection: Markowitz Meets Goldilocks. <em>Review of Financial Studies</em> 30 (12): 434988.</p>
</div>
<div>
<p>Ledoit, Olivier, Michael Wolf, and Zhao Zhao. 2020. Efficient Sorting: A More Powerful Test for Cross-Sectional Anomalies. <em>Journal of Financial Econometrics</em> Forthcoming.</p>
</div>
<div>
<p>Lee, Sang Il. 2020. Hyperparameter Optimization for Forecasting Stock Returns. <em>arXiv Preprint</em>, no. 2001.10278.</p>
</div>
<div>
<p>Legendre, Adrien Marie. 1805. <em>Nouvelles Mthodes Pour La dtermination Des Orbites Des Comtes</em>. F. Didot.</p>
</div>
<div>
<p>Lemprire, Yves, Cyril Deremble, Philip Seager, Marc Potters, and Jean-Philippe Bouchaud. 2014. Two Centuries of Trend Following. <em>arXiv Preprint</em>, no. 1404.3274.</p>
</div>
<div>
<p>Lettau, Martin, and Markus Pelger. 2020a. Estimating Latent Asset-Pricing Factors. <em>Journal of Econometrics</em> Forthcoming.</p>
</div>
<div>
<p>. 2020b. Factors That Fit the Time Series and Cross-Section of Stock Returns. <em>Review of Financial Studies</em> Forthcoming.</p>
</div>
<div>
<p>Leung, Mark T, Hazem Daouk, and An-Sing Chen. 2001. Using Investment Portfolio Return to Combine Forecasts: A Multiobjective Approach. <em>European Journal of Operational Research</em> 134 (1): 84102.</p>
</div>
<div>
<p>Li, Bin, and Steven CH Hoi. 2014. Online Portfolio Selection: A Survey. <em>ACM Computing Surveys (CSUR)</em> 46 (3): 35.</p>
</div>
<div>
<p>Li, Bin, and Steven Chu Hong Hoi. 2018. <em>Online Portfolio Selection: Principles and Algorithms</em>. CRC Press.</p>
</div>
<div>
<p>Li, Jia, Zhipeng Liao, and Rogier Quaedvlieg. 2020. Conditional Superior Predictive Ability. <em>SSRN Working Paper</em> 3536461.</p>
</div>
<div>
<p>Linnainmaa, Juhani T, and Michael R Roberts. 2018. The History of the Cross-Section of Stock Returns. <em>Review of Financial Studies</em> 31 (7): 260649.</p>
</div>
<div>
<p>Lintner, John. 1965. The Valuation of Risk Assets and the Selection of Risky Investments in Stock Portfolios and Capital Budgets. <em>Review of Economics and Statistics</em> 47 (1): 1337.</p>
</div>
<div>
<p>Lioui, Abraham. 2018. ESG Factor Investing: Myth or Reality? <em>SSRN Working Paper</em> 3272090.</p>
</div>
<div>
<p>Lioui, Abraham, and Andrea Tarelli. 2020. Factor Investing for the Long Run. <em>SSRN Working Paper</em> 3531946.</p>
</div>
<div>
<p>Little, Roderick JA, and Donald B Rubin. 2014. <em>Statistical Analysis with Missing Data</em>. Vol. 333. John Wiley &amp; Sons.</p>
</div>
<div>
<p>Lo, Andrew W, and A Craig MacKinlay. 1990. When Are Contrarian Profits Due to Stock Market Overreaction? <em>Review of Financial Studies</em> 3 (2): 175205.</p>
</div>
<div>
<p>Loreggia, Andrea, Yuri Malitsky, Horst Samulowitz, and Vijay Saraswat. 2016. Deep Learning for Algorithm Portfolios. In <em>Proceedings of the Thirtieth Aaai Conference on Artificial Intelligence</em>, 12806. AAAI Press.</p>
</div>
<div>
<p>Loughran, Tim, and Bill McDonald. 2016. Textual Analysis in Accounting and Finance: A Survey. <em>Journal of Accounting Research</em> 54 (4): 11871230.</p>
</div>
<div>
<p>Lundberg, Scott M, and Su-In Lee. 2017. A Unified Approach to Interpreting Model Predictions. In <em>Advances in Neural Information Processing Systems</em>, 476574.</p>
</div>
<div>
<p>Luo, Jiang, Avanidhar Subrahmanyam, and Sheridan Titman. 2020. Momentum and Reversals When Overconfident Investors Underestimate Their Competition. <em>Review of Financial Studies</em> Forthcoming.</p>
</div>
<div>
<p>Ma, Shujie, Wei Lan, Liangjun Su, and Chih-Ling Tsai. 2020. Testing Alphas in Conditional Time-Varying Factor Models with High Dimensional Assets. <em>Journal of Business &amp; Economic Statistics</em> 38 (1). Taylor &amp; Francis: 21427.</p>
</div>
<div>
<p>Maathuis, Marloes, Mathias Drton, Steffen Lauritzen, and Martin Wainwright. 2018. <em>Handbook of Graphical Models</em>. CRC Press.</p>
</div>
<div>
<p>Maclaurin, Dougal, David Duvenaud, and Ryan Adams. 2015. Gradient-Based Hyperparameter Optimization Through Reversible Learning. In <em>International Conference on Machine Learning</em>, 211322.</p>
</div>
<div>
<p>Maillard, Sbastien, Thierry Roncalli, and Jrme Teiletche. 2010. The Properties of Equally Weighted Risk Contribution Portfolios. <em>Journal of Portfolio Management</em> 36 (4): 6070.</p>
</div>
<div>
<p>Mailund, Thomas. 2019. Pipelines: Magrittr. In <em>R Data Science Quick Reference</em>, 7181. Springer.</p>
</div>
<div>
<p>Markowitz, Harry. 1952. Portfolio Selection. <em>Journal of Finance</em> 7 (1): 7791.</p>
</div>
<div>
<p>Marti, Gautier. 2019. CorrGAN: Sampling Realistic Financial Correlation Matrices Using Generative Adversarial Networks. <em>arXiv Preprint</em>, no. 1910.09504.</p>
</div>
<div>
<p>Martin, Evan A, and Audrey Qiuyan Fu. 2019. A Bayesian Approach to Directed Acyclic Graphs with a Candidate Graph. <em>arXiv Preprint</em>, no. 1909.10678.</p>
</div>
<div>
<p>Martin, Ian, and Stefan Nagel. 2019. Market Efficiency in the Age of Big Data. <em>SSRN Working Paper</em> 3511296.</p>
</div>
<div>
<p>Martin Utrera, Alberto, Victor DeMiguel, Raman Uppal, and Francisco J Nogales. 2020. A Transaction-Cost Perspective on the Multitude of Firm Characteristics. <em>Review of Financial Studies</em> Forthcoming.</p>
</div>
<div>
<p>Mason, Llew, Jonathan Baxter, Peter L Bartlett, and Marcus R Frean. 2000. Boosting Algorithms as Gradient Descent. In <em>Advances in Neural Information Processing Systems</em>, 51218.</p>
</div>
<div>
<p>Masters, Timothy. 1993. <em>Practical Neural Network Recipes in C++</em>. Morgan Kaufmann.</p>
</div>
<div>
<p>Mat'as, Jos M, and Juan C Reboredo. 2012. Forecasting Performance of Nonlinear Models for Intraday Stock Returns. <em>Journal of Forecasting</em> 31 (2). Wiley Online Library: 17288.</p>
</div>
<div>
<p>McLean, R David, and Jeffrey Pontiff. 2016. Does Academic Research Destroy Stock Return Predictability? <em>Journal of Finance</em> 71 (1). Wiley Online Library: 532.</p>
</div>
<div>
<p>Meng, Terry Lingze, and Matloob Khushi. 2019. Reinforcement Learning in Financial Markets. <em>Data</em> 4 (3): 110.</p>
</div>
<div>
<p>Metropolis, Nicholas, and Stanislaw Ulam. 1949. The Monte Carlo Method. <em>Journal of the American Statistical Association</em> 44 (247): 33541.</p>
</div>
<div>
<p>Meyer, Carl D. 2000. <em>Matrix Analysis and Applied Linear Algebra</em>. Vol. 71. SIAM.</p>
</div>
<div>
<p>Mohri, Mehryar, Afshin Rostamizadeh, and Ameet Talwalkar. 2018. <em>Foundations of Machine Learning</em>. MIT press.</p>
</div>
<div>
<p>Molnar, Christoph. 2019. Interpretable Machine Learning: A Guide for Making Black Box Models Explainable. LeanPub / Lulu.</p>
</div>
<div>
<p>Molnar, Christoph, Giuseppe Casalicchio, and Bernd Bischl. 2018. Iml: An R Package for Interpretable Machine Learning. <em>Journal of Open Source Software</em> 3 (27): 786.</p>
</div>
<div>
<p>Moody, John, and Lizhong Wu. 1997. Optimization of Trading Systems and Portfolios. In <em>Proceedings of the Ieee/Iafe 1997 Computational Intelligence for Financial Engineering (Cifer)</em>, 300307. IEEE.</p>
</div>
<div>
<p>Moody, John, Lizhong Wu, Yuansong Liao, and Matthew Saffell. 1998. Performance Functions and Reinforcement Learning for Trading Systems and Portfolios. <em>Journal of Forecasting</em> 17 (5-6): 44170.</p>
</div>
<div>
<p>Moritz, Benjamin, and Tom Zimmermann. 2016. Tree-Based Conditional Portfolio Sorts: The Relation Between Past and Future Stock Returns. <em>SSRN Working Paper</em> 2740751.</p>
</div>
<div>
<p>Mosavi, Amir, Pedram Ghamisi, Yaser Faghan, Puhong Duan, and Shahab Shamshirband. 2020. Comprehensive Review of Deep Reinforcement Learning Methods and Applications in Economics. <em>arXiv Preprint</em>, no. 2004.01509.</p>
</div>
<div>
<p>Moskowitz, Tobias J, and Mark Grinblatt. 1999. Do Industries Explain Momentum? <em>Journal of Finance</em> 54 (4): 124990.</p>
</div>
<div>
<p>Moskowitz, Tobias J, Yao Hua Ooi, and Lasse Heje Pedersen. 2012. Time Series Momentum. <em>Journal of Financial Economics</em> 104 (2): 22850.</p>
</div>
<div>
<p>Mossin, Jan. 1966. Equilibrium in a Capital Asset Market. <em>Econometrica: Journal of the Econometric Society</em> 34 (4): 76883.</p>
</div>
<div>
<p>Nagy, Zoltn, Altaf Kassam, and Linda-Eling Lee. 2016. Can Esg Add Alpha? An Analysis of Esg Tilt and Momentum Strategies. <em>The Journal of Investing</em> 25 (2): 11324.</p>
</div>
<div>
<p>Nesterov, Yurii. 1983. A Method for Unconstrained Convex Minimization Problem with the Rate of Convergence O (1/K^ 2). In <em>Doklady an Ussr</em>, 269:54347.</p>
</div>
<div>
<p>Neuneier, Ralph. 1996. Optimal Asset Allocation Using Adaptive Dynamic Programming. In <em>Advances in Neural Information Processing Systems</em>, 95258.</p>
</div>
<div>
<p>. 1998. Enhancing Q-Learning for Optimal Asset Allocation. In <em>Advances in Neural Information Processing Systems</em>, 93642.</p>
</div>
<div>
<p>Ngai, Eric WT, Yong Hu, YH Wong, Yijun Chen, and Xin Sun. 2011. The Application of Data Mining Techniques in Financial Fraud Detection: A Classification Framework and an Academic Review of Literature. <em>Decision Support Systems</em> 50 (3): 55969.</p>
</div>
<div>
<p>Novy-Marx, Robert. 2012. Is Momentum Really Momentum? <em>Journal of Financial Economics</em> 103 (3): 42953.</p>
</div>
<div>
<p>Novy-Marx, Robert, and Mihail Velikov. 2015. A Taxonomy of Anomalies and Their Trading Costs. <em>Review of Financial Studies</em> 29 (1): 10447.</p>
</div>
<div>
<p>Nuti, Giuseppe, Llu's Antoni Jimnez Rugama, and Kaspar Thommen. 2019. Adaptive Reticulum. <em>arXiv Preprint</em>, no. 1912.05901.</p>
</div>
<div>
<p>Okun, Oleg, Giorgio Valentini, and Matteo Re. 2011. <em>Ensembles in Machine Learning Applications</em>. Vol. 373. Springer Science &amp; Business Media.</p>
</div>
<div>
<p>Olazaran, Mikel. 1996. A Sociological Study of the Official History of the Perceptrons Controversy. <em>Social Studies of Science</em> 26 (3): 61159.</p>
</div>
<div>
<p>Olson, Randal S, William La Cava, Zairah Mustahsan, Akshay Varik, and Jason H Moore. 2018. Data-Driven Advice for Applying Machine Learning to Bioinformatics Problems. <em>arXiv Preprint</em>, no. 1708.05070.</p>
</div>
<div>
<p>Orimoloye, Larry Olanrewaju, Ming-Chien Sung, Tiejun Ma, and Johnnie EV Johnson. 2019. Comparing the Effectiveness of Deep Feedforward Neural Networks and Shallow Architectures for Predicting Stock Price Indices. <em>Expert Systems with Applications</em>. Elsevier, 112828.</p>
</div>
<div>
<p>Pan, Sinno Jialin, and Qiang Yang. 2009. A Survey on Transfer Learning. <em>IEEE Transactions on Knowledge and Data Engineering</em> 22 (10): 134559.</p>
</div>
<div>
<p>Patel, Jigar, Sahil Shah, Priyank Thakkar, and K Kotecha. 2015a. Predicting Stock and Stock Price Index Movement Using Trend Deterministic Data Preparation and Machine Learning Techniques. <em>Expert Systems with Applications</em> 42 (1): 25968.</p>
</div>
<div>
<p>Patel, Jigar, Sahil Shah, Priyank Thakkar, and Ketan Kotecha. 2015b. Predicting Stock Market Index Using Fusion of Machine Learning Techniques. <em>Expert Systems with Applications</em> 42 (4): 216272.</p>
</div>
<div>
<p>Patton, Andrew J, and Allan Timmermann. 2010. Monotonicity in Asset Returns: New Tests with Applications to the Term Structure, the CAPM, and Portfolio Sorts. <em>Journal of Financial Economics</em> 98 (3). Elsevier: 60525.</p>
</div>
<div>
<p>Pearl, Judea. 2009. <em>Causality: Models, Reasoning and Inference. Second Edition</em>. Vol. 29. Cambridge University Press.</p>
</div>
<div>
<p>Penasse, Julien. 2019. Understanding Alpha Decay. <em>SSRN Working Paper</em> 2953614.</p>
</div>
<div>
<p>Pendharkar, Parag C, and Patrick Cusatis. 2018. Trading Financial Indices with Reinforcement Learning Agents. <em>Expert Systems with Applications</em> 103: 113.</p>
</div>
<div>
<p>Perrin, Sarah, and Thierry Roncalli. 2019. Machine Learning Optimization Algorithms &amp; Portfolio Allocation. <em>SSRN Working Paper</em> 3425827.</p>
</div>
<div>
<p>Peters, Jonas, Dominik Janzing, and Bernhard Schlkopf. 2017. <em>Elements of Causal Inference: Foundations and Learning Algorithms</em>. MIT press.</p>
</div>
<div>
<p>Petersen, Mitchell A. 2009. Estimating Standard Errors in Finance Panel Data Sets: Comparing Approaches. <em>Review of Financial Studies</em> 22 (1): 43580.</p>
</div>
<div>
<p>Plyakha, Yuliya, Raman Uppal, and Grigory Vilkov. 2016. Equal or Value Weighting? Implications for Asset-Pricing Tests. <em>SSRN Working Paper</em> 1787045.</p>
</div>
<div>
<p>Polyak, Boris T. 1964. Some Methods of Speeding up the Convergence of Iteration Methods. <em>USSR Computational Mathematics and Mathematical Physics</em> 4 (5): 117.</p>
</div>
<div>
<p>Popov, Sergei, Stanislav Morozov, and Artem Babenko. 2019. Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data. <em>arXiv Preprint</em>, no. 1909.06312.</p>
</div>
<div>
<p>Powell, Warren B, and Jun Ma. 2011. A Review of Stochastic Algorithms with Continuous Value Function Approximation and Some New Approximate Policy Iteration Algorithms for Multidimensional Continuous Applications. <em>Journal of Control Theory and Applications</em> 9 (3): 33652.</p>
</div>
<div>
<p>Prado, Marcos Lpez de, and Frank J Fabozzi. 2020. Crowdsourced Investment Research Through Tournaments. <em>Journal of Financial Data Science</em> 2 (1): 8693.</p>
</div>
<div>
<p>Probst, Philipp, Bernd Bischl, and Anne-Laure Boulesteix. 2018. Tunability: Importance of Hyperparameters of Machine Learning Algorithms. <em>arXiv Preprint</em>, no. 1802.09596.</p>
</div>
<div>
<p>Pukthuanthong, Kuntara, Richard Roll, and Avanidhar Subrahmanyam. 2018. A Protocol for Factor Identification. <em>Review of Financial Studies</em> 32 (4): 15731607.</p>
</div>
<div>
<p>Quionero-Candela, Joaquin, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. 2009. <em>Dataset Shift in Machine Learning</em>. The MIT Press.</p>
</div>
<div>
<p>Rapach, David E, Jack K Strauss, and Guofu Zhou. 2013. International Stock Return Predictability: What Is the Role of the United States? <em>Journal of Finance</em> 68 (4): 163362.</p>
</div>
<div>
<p>Rapach, David, and Guofu Zhou. 2019. Time-Series and Cross-Sectional Stock Return Forecasting: New Machine Learning Methods. <em>SSRN Working Paper</em> 3428095.</p>
</div>
<div>
<p>Rashmi, Korlakai Vinayak, and Ran Gilad-Bachrach. 2015. DART: Dropouts Meet Multiple Additive Regression Trees. In <em>AISTATS</em>, 48997.</p>
</div>
<div>
<p>Ravisankar, Pediredla, Vadlamani Ravi, G Raghava Rao, and Indranil Bose. 2011. Detection of Financial Statement Fraud and Feature Selection Using Data Mining Techniques. <em>Decision Support Systems</em> 50 (2): 491500.</p>
</div>
<div>
<p>Reboredo, Juan C, Jos M Mat'as, and Raquel Garcia-Rubio. 2012. Nonlinearity in Forecasting of High-Frequency Stock Returns. <em>Computational Economics</em> 40 (3): 24564.</p>
</div>
<div>
<p>Regenstein, Jonathan K. 2018. <em>Reproducible Finance with R: Code Flows and Shiny Apps for Portfolio Analysis</em>. Chapman; Hall/CRC.</p>
</div>
<div>
<p>Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. Why Should I Trust You?: Explaining the Predictions of Any Classifier. In <em>Proceedings of the 22nd Acm Sigkdd International Conference on Knowledge Discovery and Data Mining</em>, 113544. ACM.</p>
</div>
<div>
<p>Ridgeway, Greg, David Madigan, and Thomas Richardson. 1999. Boosting Methodology for Regression Problems. In <em>AISTATS</em>.</p>
</div>
<div>
<p>Ripley, Brian D. 2007. <em>Pattern Recognition and Neural Networks</em>. Cambridge University Press.</p>
</div>
<div>
<p>Roberts, Gareth O, and Adrian FM Smith. 1994. Simple Conditions for the Convergence of the Gibbs Sampler and Metropolis-Hastings Algorithms. <em>Stochastic Processes and Their Applications</em> 49 (2): 20716.</p>
</div>
<div>
<p>Romano, Joseph P, and Michael Wolf. 2005. Stepwise Multiple Testing as Formalized Data Snooping. <em>Econometrica</em> 73 (4): 123782.</p>
</div>
<div>
<p>. 2013. Testing for Monotonicity in Expected Asset Returns. <em>Journal of Empirical Finance</em> 23: 93116.</p>
</div>
<div>
<p>Rosenblatt, Frank. 1958. The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain. <em>Psychological Review</em> 65 (6): 386.</p>
</div>
<div>
<p>Ross, Stephen A. 1976. The Arbitrage Theory of Capital Asset Pricing. <em>Journal of Economic Theory</em> 13 (3): 34160.</p>
</div>
<div>
<p>Rousseeuw, Peter J, and Annick M Leroy. 2005. <em>Robust Regression and Outlier Detection</em>. Vol. 589. Wiley.</p>
</div>
<div>
<p>Ruf, Johannes, and Weiguan Wang. 2019. Neural Networks for Option Pricing and Hedging: A Literature Review. <em>arXiv Preprint</em>, no. 1911.05620.</p>
</div>
<div>
<p>Santi, Caterina, and Remco CJ Zwinkels. 2018. Exploring Style Herding by Mutual Funds. <em>SSRN Working Paper</em> 2986059.</p>
</div>
<div>
<p>Sato, Yoshiharu. 2019. Model-Free Reinforcement Learning for Financial Portfolios: A Brief Survey. <em>arXiv Preprint</em>, no. 1904.04973.</p>
</div>
<div>
<p>Schafer, Joseph L. 1999. Multiple Imputation: A Primer. <em>Statistical Methods in Medical Research</em> 8 (1): 315.</p>
</div>
<div>
<p>Schapire, Robert E. 1990. The Strength of Weak Learnability. <em>Machine Learning</em> 5 (2): 197227.</p>
</div>
<div>
<p>. 2003. The Boosting Approach to Machine Learning: An Overview. In <em>Nonlinear Estimation and Classification</em>, 14971. Springer.</p>
</div>
<div>
<p>Schapire, Robert E, and Yoav Freund. 2012. <em>Boosting: Foundations and Algorithms</em>. MIT press.</p>
</div>
<div>
<p>Schnaubelt, Matthias. 2019. A Comparison of Machine Learning Model Validation Schemes for Non-Stationary Time Series Data. FAU Discussion Papers in Economics.</p>
</div>
<div>
<p>Schueth, Steve. 2003. Socially Responsible Investing in the United States. <em>Journal of Business Ethics</em> 43 (3): 18994.</p>
</div>
<div>
<p>Scornet, Erwan, Grard Biau, Jean-Philippe Vert, and others. 2015. Consistency of Random Forests. <em>Annals of Statistics</em> 43 (4): 171641.</p>
</div>
<div>
<p>Seni, Giovanni, and John F Elder. 2010. Ensemble Methods in Data Mining: Improving Accuracy Through Combining Predictions. <em>Synthesis Lectures on Data Mining and Knowledge Discovery</em> 2 (1): 1126.</p>
</div>
<div>
<p>Settles, Burr. 2009. Active Learning Literature Survey. University of Wisconsin-Madison Department of Computer Sciences.</p>
</div>
<div>
<p>. 2012. Active Learning. <em>Synthesis Lectures on Artificial Intelligence and Machine Learning</em> 6 (1): 1114.</p>
</div>
<div>
<p>Sezer, Omer Berat, Mehmet Ugur Gudelek, and Ahmet Murat Ozbayoglu. 2019. Financial Time Series Forecasting with Deep Learning: A Systematic Literature Review: 2005-2019. <em>arXiv Preprint</em>, no. 1911.13288.</p>
</div>
<div>
<p>Shah, Anoop D, Jonathan W Bartlett, James Carpenter, Owen Nicholas, and Harry Hemingway. 2014. Comparison of Random Forest and Parametric Imputation Models for Imputing Missing Data Using Mice: A Caliber Study. <em>American Journal of Epidemiology</em> 179 (6): 76474.</p>
</div>
<div>
<p>Shanken, Jay. 1992. On the Estimation of Beta-Pricing Models. <em>Review of Financial Studies</em> 5 (1): 133.</p>
</div>
<div>
<p>Shapley, Lloyd S. 1953. A Value for N-Person Games. <em>Contributions to the Theory of Games</em> 2 (28): 30717.</p>
</div>
<div>
<p>Sharpe, William F. 1964. Capital Asset Prices: A Theory of Market Equilibrium Under Conditions of Risk. <em>Journal of Finance</em> 19 (3): 42542.</p>
</div>
<div>
<p>. 1966. Mutual Fund Performance. <em>Journal of Business</em> 39 (1): 11938.</p>
</div>
<div>
<p>Silver, David, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, and Marc Lanctot. 2016. Mastering the Game of Go with Deep Neural Networks and Tree Search. <em>Nature</em> 529: 48489.</p>
</div>
<div>
<p>Simonian, Joseph, Chenwei Wu, Daniel Itano, and Vyshaal Narayanam. 2019. A Machine Learning Approach to Risk Factors: A Case Study Using the Fama-French-Carhart Model. <em>Journal of Financial Data Science</em> 1 (1): 3244.</p>
</div>
<div>
<p>Simonsohn, Uri, Leif D Nelson, and Joseph P Simmons. 2014. P-Curve: A Key to the File-Drawer. <em>Journal of Experimental Psychology: General</em> 143 (2): 534.</p>
</div>
<div>
<p>Sirignano, Justin, and Rama Cont. 2019. Universal Features of Price Formation in Financial Markets: Perspectives from Deep Learning. <em>Quantitative Finance</em> 19 (9): 144959.</p>
</div>
<div>
<p>Smith, Leslie N. 2018. A Disciplined Approach to Neural Network Hyper-Parameters: Part 1Learning Rate, Batch Size, Momentum, and Weight Decay. <em>arXiv Preprint</em>, no. 1803.09820.</p>
</div>
<div>
<p>Snoek, Jasper, Hugo Larochelle, and Ryan P Adams. 2012. Practical Bayesian Optimization of Machine Learning Algorithms. In <em>Advances in Neural Information Processing Systems</em>, 29519.</p>
</div>
<div>
<p>Snow, Derek. 2020. Machine Learning in Asset Management: Part 2: Portfolio ConstructionWeight Optimization. <em>Journal of Financial Data Science</em> Forthcoming.</p>
</div>
<div>
<p>Sparapani, Rodney, Charles Spanbauer, and Robert McCulloch. 2019. The BART R Package. Comprehensive R Archive Network. <a href="https://cran.r-project.org/web/packages/BART/vignettes/the-BART-R-package.pdf">https://cran.r-project.org/web/packages/BART/vignettes/the-BART-R-package.pdf</a>.</p>
</div>
<div>
<p>Spirtes, Peter, Clark N Glymour, Richard Scheines, and David Heckerman. 2000. <em>Causation, Prediction, and Search</em>. MIT press.</p>
</div>
<div>
<p>Srivastava, Nitish, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A Simple Way to Prevent Neural Networks from Overfitting. <em>Journal of Machine Learning Research</em> 15 (1): 192958.</p>
</div>
<div>
<p>Stambaugh, Robert F. 1999. Predictive Regressions. <em>Journal of Financial Economics</em> 54 (3): 375421.</p>
</div>
<div>
<p>Staniak, Mateusz, and Przemyslaw Biecek. 2018. Explanations of Model Predictions with Live and breakDown Packages. <em>arXiv Preprint</em>, no. 1804.01955.</p>
</div>
<div>
<p>Stekhoven, Daniel J, and Peter Bhlmann. 2011. MissForestNon-Parametric Missing Value Imputation for Mixed-Type Data. <em>Bioinformatics</em> 28 (1): 11218.</p>
</div>
<div>
<p>Stevens, Guy VG. 1998. On the Inverse of the Covariance Matrix in Portfolio Analysis. <em>Journal of Finance</em> 53 (5): 18217.</p>
</div>
<div>
<p>Suhonen, Antti, Matthias Lennkh, and Fabrice Perez. 2017. Quantifying Backtest Overfitting in Alternative Beta Strategies. <em>Journal of Portfolio Management</em> 43 (2): 90104.</p>
</div>
<div>
<p>Sutton, Richard S, and Andrew G Barto. 2018. <em>Reinforcement Learning: An Introduction (2nd Edition)</em>. MIT press.</p>
</div>
<div>
<p>Tibshirani, Robert. 1996. Regression Shrinkage and Selection via the Lasso. <em>Journal of the Royal Statistical Society. Series B (Methodological)</em>, 26788.</p>
</div>
<div>
<p>Tierney, Luke. 1994. Markov Chains for Exploring Posterior Distributions. <em>Annals of Statistics</em>, 170128.</p>
</div>
<div>
<p>Tikka, Santtu, and Juha Karvanen. 2017. Identifying Causal Effects with the R Package Causaleffect. <em>Journal of Statistical Software</em> 76 (1): 130.</p>
</div>
<div>
<p>Timmermann, Allan. 2018. Forecasting Methods in Finance. <em>Annual Review of Financial Economics</em> 10: 44979.</p>
</div>
<div>
<p>Ting, Kai Ming. 2002. An Instance-Weighting Method to Induce Cost-Sensitive Trees. <em>IEEE Transactions on Knowledge &amp; Data Engineering</em>, no. 3: 65965.</p>
</div>
<div>
<p>Treynor, Jack L. 1965. How to Rate Management of Investment Funds. <em>Harvard Business Review</em> 43 (1): 6375.</p>
</div>
<div>
<p>Tsantekidis, Avraam, Nikolaos Passalis, Anastasios Tefas, Juho Kanniainen, Moncef Gabbouj, and Alexandros Iosifidis. 2017. Forecasting Stock Prices from the Limit Order Book Using Convolutional Neural Networks. In <em>2017 Ieee 19th Conference on Business Informatics (Cbi)</em>, 1:712.</p>
</div>
<div>
<p>Uematsu, Yoshimasa, and Shinya Tanaka. 2019. High-Dimensional Macroeconomic Forecasting and Variable Selection via Penalized Regression. <em>Econometrics Journal</em> 22 (1): 3456.</p>
</div>
<div>
<p>Van Buuren, Stef. 2018. <em>Flexible Imputation of Missing Data</em>. Chapman; Hall/CRC.</p>
</div>
<div>
<p>Van Dijk, Mathijs A. 2011. Is Size Dead? A Review of the Size Effect in Equity Returns. <em>Journal of Banking &amp; Finance</em> 35 (12): 326374.</p>
</div>
<div>
<p>Vapnik, Vladimir, and A. Lerner. 1963. Pattern Recognition Using Generalized Portrait Method. <em>Automation and Remote Control</em> 24: 77480.</p>
</div>
<div>
<p>Vayanos, Dimitri, and Paul Woolley. 2013. An Institutional Theory of Momentum and Reversal. <em>Review of Financial Studies</em> 26 (5): 10871145.</p>
</div>
<div>
<p>Vidal, Thibaut, Toni Pacheco, and Maximilian Schiffer. 2020. Born-Again Tree Ensembles. <em>arXiv Preprint</em>, no. 2003.11132.</p>
</div>
<div>
<p>Virtanen, Ilkka, and Paavo Yli-Olli. 1987. Forecasting Stock Market Prices in a Thin Security Market. <em>Omega</em> 15 (2): 14555.</p>
</div>
<div>
<p>Volpati, Valerio, Michael Benzaquen, Zoltan Eisler, Iacopo Mastromatteo, Bence Toth, and Jean-Philippe Bouchaud. 2020. Zooming in on Equity Factor Crowding. <em>arXiv Preprint</em>, no. 2001.04185.</p>
</div>
<div>
<p>Von Holstein, Carl-Axel S Stal. 1972. Probabilistic Forecasting: An Experiment Related to the Stock Market. <em>Organizational Behavior and Human Performance</em> 8 (1): 13958.</p>
</div>
<div>
<p>Wallbridge, James. 2020. Transformers for Limit Order Books. <em>arXiv Preprint</em>, no. 2003.00130.</p>
</div>
<div>
<p>Wang, Gang, Jinxing Hao, Jian Ma, and Hongbing Jiang. 2011. A Comparative Assessment of Ensemble Learning for Credit Scoring. <em>Expert Systems with Applications</em> 38 (1): 22330.</p>
</div>
<div>
<p>Wang, Haoran, and Xun Yu Zhou. 2019. Continuous-Time Mean-Variance Portfolio Selection: A Reinforcement Learning Framework. <em>SSRN Working Paper</em> 3382932.</p>
</div>
<div>
<p>Wang, Ju-Jie, Jian-Zhou Wang, Zhe-George Zhang, and Shu-Po Guo. 2012. Stock Index Forecasting Based on a Hybrid Model. <em>Omega</em> 40 (6): 75866.</p>
</div>
<div>
<p>Wang, Wuyu, Weizi Li, Ning Zhang, and Kecheng Liu. 2020. Portfolio Formation with Preselection Using Deep Learning from Long-Term Financial Data. <em>Expert Systems with Applications</em> 143: 113042.</p>
</div>
<div>
<p>Watkins, Christopher JCH, and Peter Dayan. 1992. Q-Learning. <em>Machine Learning</em> 8 (3-4): 27992.</p>
</div>
<div>
<p>Weiss, Karl, Taghi M Khoshgoftaar, and DingDing Wang. 2016. A Survey of Transfer Learning. <em>Journal of Big Data</em> 3 (1): 9.</p>
</div>
<div>
<p>White, Halbert. 1988. Economic Prediction Using Neural Networks: The Case of Ibm Daily Stock Returns.</p>
</div>
<div>
<p>. 2000. A Reality Check for Data Snooping. <em>Econometrica</em> 68 (5): 10971126.</p>
</div>
<div>
<p>Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, L McGowan, Romain Franois, Garrett Grolemund, et al. 2019. Welcome to the Tidyverse. <em>Journal of Open Source Software</em> 4 (43): 1686.</p>
</div>
<div>
<p>Widrow, Bernard, and Marcian E Hoff. 1960. Adaptive Switching Circuits. In <em>IRE Wescon Convention Record</em>, 4:96104.</p>
</div>
<div>
<p>Wiese, Magnus, Robert Knobloch, Ralf Korn, and Peter Kretschmer. 2020. Quant Gans: Deep Generation of Financial Time Series. <em>Quantitative Finance</em> Forthcoming.</p>
</div>
<div>
<p>Wolpert, David H. 1992a. On the Connection Between in-Sample Testing and Generalization Error. <em>Complex Systems</em> 6 (1): 47.</p>
</div>
<div>
<p>. 1992b. Stacked Generalization. <em>Neural Networks</em> 5 (2): 24159.</p>
</div>
<div>
<p>Wolpert, David H, and William G Macready. 1997. No Free Lunch Theorems for Optimization. <em>IEEE Transactions on Evolutionary Computation</em> 1 (1): 6782.</p>
</div>
<div>
<p>Wong, Steven YK, Jennifer Chan, Lamiae Azizi, and Richard YD Xu. 2020. Time-Varying Neural Network for Stock Return Prediction. <em>arXiv Preprint</em>, no. 2003.02515.</p>
</div>
<div>
<p>Xiong, Zhuoran, Xiao-Yang Liu, Shan Zhong, Hongyang Yang, and Anwar Walid. 2018. Practical Deep Reinforcement Learning Approach for Stock Trading. <em>arXiv Preprint</em>, no. 1811.07522.</p>
</div>
<div>
<p>Xu, Ke-Li. 2020. Testing for Multiple-Horizon Predictability: Direct Regression Based Versus Implication Based. <em>Review of Financial Studies</em> Forthcoming.</p>
</div>
<div>
<p>Yang, Steve Y, Yangyang Yu, and Saud Almahdi. 2018. An Investor Sentiment Reward-Based Trading System Using Gaussian Inverse Reinforcement Learning Algorithm. <em>Expert Systems with Applications</em> 114: 388401.</p>
</div>
<div>
<p>Yu, Pengqian, Joon Sern Lee, Ilya Kulyatin, Zekun Shi, and Sakyasingha Dasgupta. 2019. Model-Based Deep Reinforcement Learning for Dynamic Portfolio Optimization. <em>arXiv Preprint</em>, no. 1901.08740.</p>
</div>
<div>
<p>Zeiler, Matthew D. 2012. ADADELTA: An Adaptive Learning Rate Method. <em>arXiv Preprint</em>, no. 1212.5701.</p>
</div>
<div>
<p>Zhang, Cha, and Yunqian Ma. 2012. <em>Ensemble Machine Learning: Methods and Applications</em>. Springer.</p>
</div>
<div>
<p>Zhang, Yudong, and Lenan Wu. 2009. Stock Market Prediction of S&amp;P 500 via Combination of Improved Bco Approach and Bp Neural Network. <em>Expert Systems with Applications</em> 36 (5): 884954.</p>
</div>
<div>
<p>Zhao, Qingyuan, and Trevor Hastie. 2019. Causal Interpretations of Black-Box Models. <em>Journal of Business &amp; Economic Statistics</em>, nos. just-accepted: 119.</p>
</div>
<div>
<p>Zhou, Zhi-Hua. 2012. <em>Ensemble Methods: Foundations and Algorithms</em>. Chapman; Hall/CRC.</p>
</div>
<div>
<p>Zou, Hui, and Trevor Hastie. 2005. Regularization and Variable Selection via the Elastic Net. <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 67 (2): 30120.</p>
</div>
<div>
<p>Zuckerman, Gregory. 2019. <em>The Man Who Solved the Market: How Jim Simons Launched the Quant Revolution</em>. Penguin Random House.</p>
</div>
</div>
</div>
</div>






































<h3>References</h3>
<div id="refs" class="references">
<div id="ref-barroso2015momentum">
<p>Barroso, Pedro, and Pedro Santa-Clara. 2015. Momentum Has Its Moments. <em>Journal of Financial Economics</em> 116 (1): 11120.</p>
</div>
<div id="ref-breiman2001random">
<p>Breiman, Leo. 2001. Random Forests. <em>Machine Learning</em> 45 (1): 532.</p>
</div>
<div id="ref-daniel2016momentum">
<p>Daniel, Kent, and Tobias J Moskowitz. 2016. Momentum Crashes. <em>Journal of Financial Economics</em> 122 (2): 22147.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="data-description.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["ML_factor.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true,
"info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
