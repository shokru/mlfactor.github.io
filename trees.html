<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Machine Learning for Factor Investing</title>
  <meta name="description" content="Machine Learning for Factor Investing">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Machine Learning for Factor Investing" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Machine Learning for Factor Investing" />
  
  
  

<meta name="author" content="Guillaume Coqueret and Tony Guida">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="lasso.html">
<link rel="next" href="NN.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="preface.html"><a href="preface.html#foreword"><i class="fa fa-check"></i><b>1.1</b> Foreword</a></li>
<li class="chapter" data-level="1.2" data-path="preface.html"><a href="preface.html#what-this-book-is-not-about"><i class="fa fa-check"></i><b>1.2</b> What this book is not about</a></li>
<li class="chapter" data-level="1.3" data-path="preface.html"><a href="preface.html#the-targeted-audience"><i class="fa fa-check"></i><b>1.3</b> The targeted audience</a></li>
<li class="chapter" data-level="1.4" data-path="preface.html"><a href="preface.html#how-this-book-is-structured"><i class="fa fa-check"></i><b>1.4</b> How this book is structured</a></li>
<li class="chapter" data-level="1.5" data-path="preface.html"><a href="preface.html#companion-website"><i class="fa fa-check"></i><b>1.5</b> Companion website</a></li>
<li class="chapter" data-level="1.6" data-path="preface.html"><a href="preface.html#why-r"><i class="fa fa-check"></i><b>1.6</b> Why R?</a></li>
<li class="chapter" data-level="1.7" data-path="preface.html"><a href="preface.html#coding-instructions"><i class="fa fa-check"></i><b>1.7</b> Coding instructions</a></li>
<li class="chapter" data-level="1.8" data-path="preface.html"><a href="preface.html#acknowledgements"><i class="fa fa-check"></i><b>1.8</b> Acknowledgements</a></li>
<li class="chapter" data-level="1.9" data-path="preface.html"><a href="preface.html#future-developments"><i class="fa fa-check"></i><b>1.9</b> Future developments</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="notdata.html"><a href="notdata.html"><i class="fa fa-check"></i><b>2</b> Notations and data</a><ul>
<li class="chapter" data-level="2.1" data-path="notdata.html"><a href="notdata.html#notations"><i class="fa fa-check"></i><b>2.1</b> Notations</a></li>
<li class="chapter" data-level="2.2" data-path="notdata.html"><a href="notdata.html#dataset"><i class="fa fa-check"></i><b>2.2</b> Dataset</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>3</b> Introduction</a><ul>
<li class="chapter" data-level="3.1" data-path="intro.html"><a href="intro.html#context"><i class="fa fa-check"></i><b>3.1</b> Context</a></li>
<li class="chapter" data-level="3.2" data-path="intro.html"><a href="intro.html#portfolio-construction-the-workflow"><i class="fa fa-check"></i><b>3.2</b> Portfolio construction: the workflow</a></li>
<li class="chapter" data-level="3.3" data-path="intro.html"><a href="intro.html#machine-learning-is-no-magic-wand"><i class="fa fa-check"></i><b>3.3</b> Machine Learning is no Magic Wand</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="factor.html"><a href="factor.html"><i class="fa fa-check"></i><b>4</b> Factor investing and asset pricing anomalies</a><ul>
<li class="chapter" data-level="4.1" data-path="factor.html"><a href="factor.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="factor.html"><a href="factor.html#detecting-anomalies"><i class="fa fa-check"></i><b>4.2</b> Detecting anomalies</a><ul>
<li class="chapter" data-level="4.2.1" data-path="factor.html"><a href="factor.html#simple-portfolio-sorts"><i class="fa fa-check"></i><b>4.2.1</b> Simple portfolio sorts</a></li>
<li class="chapter" data-level="4.2.2" data-path="factor.html"><a href="factor.html#factors"><i class="fa fa-check"></i><b>4.2.2</b> Factors</a></li>
<li class="chapter" data-level="4.2.3" data-path="factor.html"><a href="factor.html#predictive-regressions-sorts-and-p-value-issues"><i class="fa fa-check"></i><b>4.2.3</b> Predictive regressions, sorts, and p-value issues</a></li>
<li class="chapter" data-level="4.2.4" data-path="factor.html"><a href="factor.html#fama-macbeth-regressions"><i class="fa fa-check"></i><b>4.2.4</b> Fama-Macbeth regressions</a></li>
<li class="chapter" data-level="4.2.5" data-path="factor.html"><a href="factor.html#factor-competition"><i class="fa fa-check"></i><b>4.2.5</b> Factor competition</a></li>
<li class="chapter" data-level="4.2.6" data-path="factor.html"><a href="factor.html#advanced-techniques"><i class="fa fa-check"></i><b>4.2.6</b> Advanced techniques</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="factor.html"><a href="factor.html#factors-or-characteristics"><i class="fa fa-check"></i><b>4.3</b> Factors or characteristics?</a></li>
<li class="chapter" data-level="4.4" data-path="factor.html"><a href="factor.html#factor-momentum"><i class="fa fa-check"></i><b>4.4</b> Factor momentum</a><ul>
<li class="chapter" data-level="4.4.1" data-path="factor.html"><a href="factor.html#a-short-list-of-recent-references"><i class="fa fa-check"></i><b>4.4.1</b> A short list of recent references</a></li>
<li class="chapter" data-level="4.4.2" data-path="factor.html"><a href="factor.html#explicit-connexions-with-asset-pricing-models"><i class="fa fa-check"></i><b>4.4.2</b> Explicit connexions with asset pricing models</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="factor.html"><a href="factor.html#coding-exercises"><i class="fa fa-check"></i><b>4.5</b> Coding exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Data.html"><a href="Data.html"><i class="fa fa-check"></i><b>5</b> Data preprocessing</a><ul>
<li class="chapter" data-level="5.1" data-path="Data.html"><a href="Data.html#know-your-data"><i class="fa fa-check"></i><b>5.1</b> Know your data</a></li>
<li class="chapter" data-level="5.2" data-path="Data.html"><a href="Data.html#missing-data"><i class="fa fa-check"></i><b>5.2</b> Missing data</a></li>
<li class="chapter" data-level="5.3" data-path="Data.html"><a href="Data.html#outlier-detection"><i class="fa fa-check"></i><b>5.3</b> Outlier detection</a></li>
<li class="chapter" data-level="5.4" data-path="Data.html"><a href="Data.html#feateng"><i class="fa fa-check"></i><b>5.4</b> Feature engineering</a><ul>
<li class="chapter" data-level="5.4.1" data-path="Data.html"><a href="Data.html#feature-selection"><i class="fa fa-check"></i><b>5.4.1</b> Feature selection</a></li>
<li class="chapter" data-level="5.4.2" data-path="Data.html"><a href="Data.html#scaling"><i class="fa fa-check"></i><b>5.4.2</b> Scaling the predictors</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="Data.html"><a href="Data.html#labelling"><i class="fa fa-check"></i><b>5.5</b> Labelling</a><ul>
<li class="chapter" data-level="5.5.1" data-path="Data.html"><a href="Data.html#simple-labels"><i class="fa fa-check"></i><b>5.5.1</b> Simple labels</a></li>
<li class="chapter" data-level="5.5.2" data-path="Data.html"><a href="Data.html#categorical-labels"><i class="fa fa-check"></i><b>5.5.2</b> Categorical labels</a></li>
<li class="chapter" data-level="5.5.3" data-path="Data.html"><a href="Data.html#the-triple-barrier-method"><i class="fa fa-check"></i><b>5.5.3</b> The triple barrier method</a></li>
<li class="chapter" data-level="5.5.4" data-path="Data.html"><a href="Data.html#filtering-the-sample"><i class="fa fa-check"></i><b>5.5.4</b> Filtering the sample</a></li>
<li class="chapter" data-level="5.5.5" data-path="Data.html"><a href="Data.html#horizons"><i class="fa fa-check"></i><b>5.5.5</b> Return horizons</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="Data.html"><a href="Data.html#pers"><i class="fa fa-check"></i><b>5.6</b> Handling persistence</a></li>
<li class="chapter" data-level="5.7" data-path="Data.html"><a href="Data.html#extensions"><i class="fa fa-check"></i><b>5.7</b> Extensions</a><ul>
<li class="chapter" data-level="5.7.1" data-path="Data.html"><a href="Data.html#transforming-features"><i class="fa fa-check"></i><b>5.7.1</b> Transforming features</a></li>
<li class="chapter" data-level="5.7.2" data-path="Data.html"><a href="Data.html#macro-economic-variables"><i class="fa fa-check"></i><b>5.7.2</b> Macro-economic variables</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="Data.html"><a href="Data.html#additional-code-and-results"><i class="fa fa-check"></i><b>5.8</b> Additional code and results</a><ul>
<li class="chapter" data-level="5.8.1" data-path="Data.html"><a href="Data.html#impact-of-rescaling-graphical-representation"><i class="fa fa-check"></i><b>5.8.1</b> Impact of rescaling: graphical representation</a></li>
<li class="chapter" data-level="5.8.2" data-path="Data.html"><a href="Data.html#impact-of-rescaling-toy-example"><i class="fa fa-check"></i><b>5.8.2</b> Impact of rescaling: toy example</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="Data.html"><a href="Data.html#coding-exercises-1"><i class="fa fa-check"></i><b>5.9</b> Coding exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>6</b> Penalized regressions and sparse hedging for minimum variance portfolios</a><ul>
<li class="chapter" data-level="6.1" data-path="lasso.html"><a href="lasso.html#penalised-regressions"><i class="fa fa-check"></i><b>6.1</b> Penalised regressions</a><ul>
<li class="chapter" data-level="6.1.1" data-path="lasso.html"><a href="lasso.html#simple-regressions"><i class="fa fa-check"></i><b>6.1.1</b> Simple regressions</a></li>
<li class="chapter" data-level="6.1.2" data-path="lasso.html"><a href="lasso.html#forms-of-penalizations"><i class="fa fa-check"></i><b>6.1.2</b> Forms of penalizations</a></li>
<li class="chapter" data-level="6.1.3" data-path="lasso.html"><a href="lasso.html#illustrations"><i class="fa fa-check"></i><b>6.1.3</b> Illustrations</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="lasso.html"><a href="lasso.html#sparse-hedging-for-minimum-variance-portfolios"><i class="fa fa-check"></i><b>6.2</b> Sparse hedging for minimum variance portfolios</a><ul>
<li class="chapter" data-level="6.2.1" data-path="lasso.html"><a href="lasso.html#presentation-and-derivations"><i class="fa fa-check"></i><b>6.2.1</b> Presentation and derivations</a></li>
<li class="chapter" data-level="6.2.2" data-path="lasso.html"><a href="lasso.html#sparseex"><i class="fa fa-check"></i><b>6.2.2</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="lasso.html"><a href="lasso.html#predictive-regressions"><i class="fa fa-check"></i><b>6.3</b> Predictive regressions</a><ul>
<li class="chapter" data-level="6.3.1" data-path="lasso.html"><a href="lasso.html#literature-review-and-principle"><i class="fa fa-check"></i><b>6.3.1</b> Literature review and principle</a></li>
<li class="chapter" data-level="6.3.2" data-path="lasso.html"><a href="lasso.html#code-and-results"><i class="fa fa-check"></i><b>6.3.2</b> Code and results</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="lasso.html"><a href="lasso.html#coding-exercises-2"><i class="fa fa-check"></i><b>6.4</b> Coding exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="trees.html"><a href="trees.html"><i class="fa fa-check"></i><b>7</b> Tree-based methods</a><ul>
<li class="chapter" data-level="7.1" data-path="trees.html"><a href="trees.html#simple-trees"><i class="fa fa-check"></i><b>7.1</b> Simple trees</a><ul>
<li class="chapter" data-level="7.1.1" data-path="trees.html"><a href="trees.html#principle"><i class="fa fa-check"></i><b>7.1.1</b> Principle</a></li>
<li class="chapter" data-level="7.1.2" data-path="trees.html"><a href="trees.html#further-details-on-classification"><i class="fa fa-check"></i><b>7.1.2</b> Further details on classification</a></li>
<li class="chapter" data-level="7.1.3" data-path="trees.html"><a href="trees.html#pruning-criteria"><i class="fa fa-check"></i><b>7.1.3</b> Pruning criteria</a></li>
<li class="chapter" data-level="7.1.4" data-path="trees.html"><a href="trees.html#code-and-interpretation"><i class="fa fa-check"></i><b>7.1.4</b> Code and interpretation</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="trees.html"><a href="trees.html#random-forests"><i class="fa fa-check"></i><b>7.2</b> Random forests</a><ul>
<li class="chapter" data-level="7.2.1" data-path="trees.html"><a href="trees.html#principle-1"><i class="fa fa-check"></i><b>7.2.1</b> Principle</a></li>
<li class="chapter" data-level="7.2.2" data-path="trees.html"><a href="trees.html#code-and-results-1"><i class="fa fa-check"></i><b>7.2.2</b> Code and results</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="trees.html"><a href="trees.html#adaboost"><i class="fa fa-check"></i><b>7.3</b> Boosted trees: Adaboost</a><ul>
<li class="chapter" data-level="7.3.1" data-path="trees.html"><a href="trees.html#methodology"><i class="fa fa-check"></i><b>7.3.1</b> Methodology</a></li>
<li class="chapter" data-level="7.3.2" data-path="trees.html"><a href="trees.html#illustration"><i class="fa fa-check"></i><b>7.3.2</b> Illustration</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="trees.html"><a href="trees.html#boosted-trees-extreme-gradient-boosting"><i class="fa fa-check"></i><b>7.4</b> Boosted trees: extreme gradient boosting</a><ul>
<li class="chapter" data-level="7.4.1" data-path="trees.html"><a href="trees.html#managing-loss"><i class="fa fa-check"></i><b>7.4.1</b> Managing Loss</a></li>
<li class="chapter" data-level="7.4.2" data-path="trees.html"><a href="trees.html#penalisation"><i class="fa fa-check"></i><b>7.4.2</b> Penalisation</a></li>
<li class="chapter" data-level="7.4.3" data-path="trees.html"><a href="trees.html#aggregation"><i class="fa fa-check"></i><b>7.4.3</b> Aggregation</a></li>
<li class="chapter" data-level="7.4.4" data-path="trees.html"><a href="trees.html#tree-structure"><i class="fa fa-check"></i><b>7.4.4</b> Tree structure</a></li>
<li class="chapter" data-level="7.4.5" data-path="trees.html"><a href="trees.html#boostext"><i class="fa fa-check"></i><b>7.4.5</b> Extensions</a></li>
<li class="chapter" data-level="7.4.6" data-path="trees.html"><a href="trees.html#boostcode"><i class="fa fa-check"></i><b>7.4.6</b> Code and results</a></li>
<li class="chapter" data-level="7.4.7" data-path="trees.html"><a href="trees.html#instweight"><i class="fa fa-check"></i><b>7.4.7</b> Instance weighting</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="trees.html"><a href="trees.html#discussion"><i class="fa fa-check"></i><b>7.5</b> Discussion</a></li>
<li class="chapter" data-level="7.6" data-path="trees.html"><a href="trees.html#coding-exercises-3"><i class="fa fa-check"></i><b>7.6</b> Coding exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="NN.html"><a href="NN.html"><i class="fa fa-check"></i><b>8</b> Neural networks</a><ul>
<li class="chapter" data-level="8.1" data-path="NN.html"><a href="NN.html#the-original-perceptron"><i class="fa fa-check"></i><b>8.1</b> The original perceptron</a></li>
<li class="chapter" data-level="8.2" data-path="NN.html"><a href="NN.html#multilayer-perceptron"><i class="fa fa-check"></i><b>8.2</b> Multilayer perceptron</a><ul>
<li class="chapter" data-level="8.2.1" data-path="NN.html"><a href="NN.html#introduction-and-notations"><i class="fa fa-check"></i><b>8.2.1</b> Introduction and notations</a></li>
<li class="chapter" data-level="8.2.2" data-path="NN.html"><a href="NN.html#universal-approximation"><i class="fa fa-check"></i><b>8.2.2</b> Universal approximation</a></li>
<li class="chapter" data-level="8.2.3" data-path="NN.html"><a href="NN.html#backprop"><i class="fa fa-check"></i><b>8.2.3</b> Learning via back-propagation</a></li>
<li class="chapter" data-level="8.2.4" data-path="NN.html"><a href="NN.html#further-details-on-classification-1"><i class="fa fa-check"></i><b>8.2.4</b> Further details on classification</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="NN.html"><a href="NN.html#howdeep"><i class="fa fa-check"></i><b>8.3</b> How deep should we go? And other practical issues</a><ul>
<li class="chapter" data-level="8.3.1" data-path="NN.html"><a href="NN.html#architectural-choices"><i class="fa fa-check"></i><b>8.3.1</b> Architectural choices</a></li>
<li class="chapter" data-level="8.3.2" data-path="NN.html"><a href="NN.html#frequency-of-weight-updates-and-learning-duration"><i class="fa fa-check"></i><b>8.3.2</b> Frequency of weight updates and learning duration</a></li>
<li class="chapter" data-level="8.3.3" data-path="NN.html"><a href="NN.html#penalizations-and-dropout"><i class="fa fa-check"></i><b>8.3.3</b> Penalizations and dropout</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="NN.html"><a href="NN.html#code-samples-and-comments-for-vanilla-mlp"><i class="fa fa-check"></i><b>8.4</b> Code samples and comments for vanilla MLP</a><ul>
<li class="chapter" data-level="8.4.1" data-path="NN.html"><a href="NN.html#regression-example"><i class="fa fa-check"></i><b>8.4.1</b> Regression example</a></li>
<li class="chapter" data-level="8.4.2" data-path="NN.html"><a href="NN.html#classification-example"><i class="fa fa-check"></i><b>8.4.2</b> Classification example</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="NN.html"><a href="NN.html#recurrent-networks"><i class="fa fa-check"></i><b>8.5</b> Recurrent networks</a><ul>
<li class="chapter" data-level="8.5.1" data-path="NN.html"><a href="NN.html#presentation"><i class="fa fa-check"></i><b>8.5.1</b> Presentation</a></li>
<li class="chapter" data-level="8.5.2" data-path="NN.html"><a href="NN.html#code-and-results-2"><i class="fa fa-check"></i><b>8.5.2</b> Code and results</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="NN.html"><a href="NN.html#other-common-architectures"><i class="fa fa-check"></i><b>8.6</b> Other common architectures</a><ul>
<li class="chapter" data-level="8.6.1" data-path="NN.html"><a href="NN.html#generative-aversarial-networks"><i class="fa fa-check"></i><b>8.6.1</b> Generative adversarial networks</a></li>
<li class="chapter" data-level="8.6.2" data-path="NN.html"><a href="NN.html#autoencoders"><i class="fa fa-check"></i><b>8.6.2</b> Auto-encoders</a></li>
<li class="chapter" data-level="8.6.3" data-path="NN.html"><a href="NN.html#a-word-on-convolutional-networks"><i class="fa fa-check"></i><b>8.6.3</b> A word on convolutional networks</a></li>
<li class="chapter" data-level="8.6.4" data-path="NN.html"><a href="NN.html#advanced-architectures"><i class="fa fa-check"></i><b>8.6.4</b> Advanced architectures</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="NN.html"><a href="NN.html#coding-exercises-4"><i class="fa fa-check"></i><b>8.7</b> Coding exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="svm.html"><a href="svm.html"><i class="fa fa-check"></i><b>9</b> Support vector machines</a><ul>
<li class="chapter" data-level="9.1" data-path="svm.html"><a href="svm.html#svm-for-classification"><i class="fa fa-check"></i><b>9.1</b> SVM for classification</a></li>
<li class="chapter" data-level="9.2" data-path="svm.html"><a href="svm.html#svm-for-regression"><i class="fa fa-check"></i><b>9.2</b> SVM for regression</a></li>
<li class="chapter" data-level="9.3" data-path="svm.html"><a href="svm.html#practice"><i class="fa fa-check"></i><b>9.3</b> Practice</a></li>
<li class="chapter" data-level="9.4" data-path="svm.html"><a href="svm.html#coding-exercises-5"><i class="fa fa-check"></i><b>9.4</b> Coding exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bayes.html"><a href="bayes.html"><i class="fa fa-check"></i><b>10</b> Bayesian methods</a><ul>
<li class="chapter" data-level="10.1" data-path="bayes.html"><a href="bayes.html#the-bayesian-framework"><i class="fa fa-check"></i><b>10.1</b> The Bayesian framework</a></li>
<li class="chapter" data-level="10.2" data-path="bayes.html"><a href="bayes.html#bayesian-sampling"><i class="fa fa-check"></i><b>10.2</b> Bayesian sampling</a><ul>
<li class="chapter" data-level="10.2.1" data-path="bayes.html"><a href="bayes.html#gibbs-sampling"><i class="fa fa-check"></i><b>10.2.1</b> Gibbs sampling</a></li>
<li class="chapter" data-level="10.2.2" data-path="bayes.html"><a href="bayes.html#metropolis-hastings-sampling"><i class="fa fa-check"></i><b>10.2.2</b> Metropolis-Hastings sampling</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="bayes.html"><a href="bayes.html#bayesian-linear-regression"><i class="fa fa-check"></i><b>10.3</b> Bayesian linear regression</a></li>
<li class="chapter" data-level="10.4" data-path="bayes.html"><a href="bayes.html#naive-bayes-classifier"><i class="fa fa-check"></i><b>10.4</b> Naive Bayes classifier</a></li>
<li class="chapter" data-level="10.5" data-path="bayes.html"><a href="bayes.html#BART"><i class="fa fa-check"></i><b>10.5</b> Bayesian additive trees</a><ul>
<li class="chapter" data-level="10.5.1" data-path="bayes.html"><a href="bayes.html#general-formulation"><i class="fa fa-check"></i><b>10.5.1</b> General formulation</a></li>
<li class="chapter" data-level="10.5.2" data-path="bayes.html"><a href="bayes.html#priors"><i class="fa fa-check"></i><b>10.5.2</b> Priors</a></li>
<li class="chapter" data-level="10.5.3" data-path="bayes.html"><a href="bayes.html#sampling-and-predictions"><i class="fa fa-check"></i><b>10.5.3</b> Sampling and predictions</a></li>
<li class="chapter" data-level="10.5.4" data-path="bayes.html"><a href="bayes.html#code"><i class="fa fa-check"></i><b>10.5.4</b> Code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="valtune.html"><a href="valtune.html"><i class="fa fa-check"></i><b>11</b> Validating and tuning</a><ul>
<li class="chapter" data-level="11.1" data-path="valtune.html"><a href="valtune.html#mlmetrics"><i class="fa fa-check"></i><b>11.1</b> Learning metrics</a><ul>
<li class="chapter" data-level="11.1.1" data-path="valtune.html"><a href="valtune.html#regression-analysis"><i class="fa fa-check"></i><b>11.1.1</b> Regression analysis</a></li>
<li class="chapter" data-level="11.1.2" data-path="valtune.html"><a href="valtune.html#classification-analysis"><i class="fa fa-check"></i><b>11.1.2</b> Classification analysis</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="valtune.html"><a href="valtune.html#validation"><i class="fa fa-check"></i><b>11.2</b> Validation</a><ul>
<li class="chapter" data-level="11.2.1" data-path="valtune.html"><a href="valtune.html#the-variance-bias-tradeoff-theory"><i class="fa fa-check"></i><b>11.2.1</b> The variance-bias tradeoff: theory</a></li>
<li class="chapter" data-level="11.2.2" data-path="valtune.html"><a href="valtune.html#the-variance-bias-tradeoff-illustration"><i class="fa fa-check"></i><b>11.2.2</b> The variance-bias tradeoff: illustration</a></li>
<li class="chapter" data-level="11.2.3" data-path="valtune.html"><a href="valtune.html#the-risk-of-overfitting-principle"><i class="fa fa-check"></i><b>11.2.3</b> The risk of overfitting: principle</a></li>
<li class="chapter" data-level="11.2.4" data-path="valtune.html"><a href="valtune.html#the-risk-of-overfitting-some-solutions"><i class="fa fa-check"></i><b>11.2.4</b> The risk of overfitting: some solutions</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="valtune.html"><a href="valtune.html#the-search-for-good-hyperparameters"><i class="fa fa-check"></i><b>11.3</b> The search for good hyperparameters</a><ul>
<li class="chapter" data-level="11.3.1" data-path="valtune.html"><a href="valtune.html#methods"><i class="fa fa-check"></i><b>11.3.1</b> Methods</a></li>
<li class="chapter" data-level="11.3.2" data-path="valtune.html"><a href="valtune.html#example-grid-search"><i class="fa fa-check"></i><b>11.3.2</b> Example: grid search</a></li>
<li class="chapter" data-level="11.3.3" data-path="valtune.html"><a href="valtune.html#example-bayesian-optimization"><i class="fa fa-check"></i><b>11.3.3</b> Example: Bayesian optimization</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="valtune.html"><a href="valtune.html#short-discussion-on-validation-in-backtests"><i class="fa fa-check"></i><b>11.4</b> Short discussion on validation in backtests</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ensemble.html"><a href="ensemble.html"><i class="fa fa-check"></i><b>12</b> Ensemble models</a><ul>
<li class="chapter" data-level="12.1" data-path="ensemble.html"><a href="ensemble.html#linear-ensembles"><i class="fa fa-check"></i><b>12.1</b> Linear ensembles</a><ul>
<li class="chapter" data-level="12.1.1" data-path="ensemble.html"><a href="ensemble.html#principles"><i class="fa fa-check"></i><b>12.1.1</b> Principles</a></li>
<li class="chapter" data-level="12.1.2" data-path="ensemble.html"><a href="ensemble.html#example"><i class="fa fa-check"></i><b>12.1.2</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="ensemble.html"><a href="ensemble.html#stacked-ensembles"><i class="fa fa-check"></i><b>12.2</b> Stacked ensembles</a><ul>
<li class="chapter" data-level="12.2.1" data-path="ensemble.html"><a href="ensemble.html#two-stage-training"><i class="fa fa-check"></i><b>12.2.1</b> Two stage training</a></li>
<li class="chapter" data-level="12.2.2" data-path="ensemble.html"><a href="ensemble.html#code-and-results-3"><i class="fa fa-check"></i><b>12.2.2</b> Code and results</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="ensemble.html"><a href="ensemble.html#extensions-1"><i class="fa fa-check"></i><b>12.3</b> Extensions</a><ul>
<li class="chapter" data-level="12.3.1" data-path="ensemble.html"><a href="ensemble.html#exogenous-variables"><i class="fa fa-check"></i><b>12.3.1</b> Exogenous variables</a></li>
<li class="chapter" data-level="12.3.2" data-path="ensemble.html"><a href="ensemble.html#shrinking-inter-model-correlations"><i class="fa fa-check"></i><b>12.3.2</b> Shrinking inter-model correlations</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="ensemble.html"><a href="ensemble.html#exercise"><i class="fa fa-check"></i><b>12.4</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="backtest.html"><a href="backtest.html"><i class="fa fa-check"></i><b>13</b> Portfolio backtesting</a><ul>
<li class="chapter" data-level="13.1" data-path="backtest.html"><a href="backtest.html#protocol"><i class="fa fa-check"></i><b>13.1</b> Setting the protocol</a></li>
<li class="chapter" data-level="13.2" data-path="backtest.html"><a href="backtest.html#turning-signals-into-portfolio-weights"><i class="fa fa-check"></i><b>13.2</b> Turning signals into portfolio weights</a></li>
<li class="chapter" data-level="13.3" data-path="backtest.html"><a href="backtest.html#perfmet"><i class="fa fa-check"></i><b>13.3</b> Performance metrics</a><ul>
<li class="chapter" data-level="13.3.1" data-path="backtest.html"><a href="backtest.html#discussion-1"><i class="fa fa-check"></i><b>13.3.1</b> Discussion</a></li>
<li class="chapter" data-level="13.3.2" data-path="backtest.html"><a href="backtest.html#pure-performance-and-risk-indicators"><i class="fa fa-check"></i><b>13.3.2</b> Pure performance and risk indicators</a></li>
<li class="chapter" data-level="13.3.3" data-path="backtest.html"><a href="backtest.html#factor-based-evaluation"><i class="fa fa-check"></i><b>13.3.3</b> Factor-based evaluation</a></li>
<li class="chapter" data-level="13.3.4" data-path="backtest.html"><a href="backtest.html#risk-adjusted-measures"><i class="fa fa-check"></i><b>13.3.4</b> Risk-adjusted measures</a></li>
<li class="chapter" data-level="13.3.5" data-path="backtest.html"><a href="backtest.html#transaction-costs-and-turnover"><i class="fa fa-check"></i><b>13.3.5</b> Transaction costs and turnover</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="backtest.html"><a href="backtest.html#common-errors-and-issues"><i class="fa fa-check"></i><b>13.4</b> Common errors and issues</a><ul>
<li class="chapter" data-level="13.4.1" data-path="backtest.html"><a href="backtest.html#forward-looking-data"><i class="fa fa-check"></i><b>13.4.1</b> Forward looking data</a></li>
<li class="chapter" data-level="13.4.2" data-path="backtest.html"><a href="backtest.html#backtest-overfitting"><i class="fa fa-check"></i><b>13.4.2</b> Backtest overfitting</a></li>
<li class="chapter" data-level="13.4.3" data-path="backtest.html"><a href="backtest.html#simple-saveguards"><i class="fa fa-check"></i><b>13.4.3</b> Simple saveguards</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="backtest.html"><a href="backtest.html#implication-of-non-stationarity-forecasting-is-hard"><i class="fa fa-check"></i><b>13.5</b> Implication of non-stationarity: forecasting is hard</a><ul>
<li class="chapter" data-level="13.5.1" data-path="backtest.html"><a href="backtest.html#general-comments"><i class="fa fa-check"></i><b>13.5.1</b> General comments</a></li>
<li class="chapter" data-level="13.5.2" data-path="backtest.html"><a href="backtest.html#the-no-free-lunch-theorem"><i class="fa fa-check"></i><b>13.5.2</b> The no free lunch theorem</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="backtest.html"><a href="backtest.html#example-1"><i class="fa fa-check"></i><b>13.6</b> Example</a></li>
<li class="chapter" data-level="13.7" data-path="backtest.html"><a href="backtest.html#coding-exercises-6"><i class="fa fa-check"></i><b>13.7</b> Coding exercises</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="interp.html"><a href="interp.html"><i class="fa fa-check"></i><b>14</b> Interpretability</a><ul>
<li class="chapter" data-level="14.1" data-path="interp.html"><a href="interp.html#global-interpretations"><i class="fa fa-check"></i><b>14.1</b> Global interpretations</a><ul>
<li class="chapter" data-level="14.1.1" data-path="interp.html"><a href="interp.html#variable-importance"><i class="fa fa-check"></i><b>14.1.1</b> Variable importance (tree-based)</a></li>
<li class="chapter" data-level="14.1.2" data-path="interp.html"><a href="interp.html#variable-importance-agnostic"><i class="fa fa-check"></i><b>14.1.2</b> Variable importance (agnostic)</a></li>
<li class="chapter" data-level="14.1.3" data-path="interp.html"><a href="interp.html#partial-dependence-plot"><i class="fa fa-check"></i><b>14.1.3</b> Partial dependence plot</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="interp.html"><a href="interp.html#local-interpretations"><i class="fa fa-check"></i><b>14.2</b> Local interpretations</a><ul>
<li class="chapter" data-level="14.2.1" data-path="interp.html"><a href="interp.html#lime"><i class="fa fa-check"></i><b>14.2.1</b> LIME</a></li>
<li class="chapter" data-level="14.2.2" data-path="interp.html"><a href="interp.html#shapley-values"><i class="fa fa-check"></i><b>14.2.2</b> Shapley values</a></li>
<li class="chapter" data-level="14.2.3" data-path="interp.html"><a href="interp.html#breakdown"><i class="fa fa-check"></i><b>14.2.3</b> Breakdown</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="causality.html"><a href="causality.html"><i class="fa fa-check"></i><b>15</b> Two key concepts: causality and non-stationarity</a><ul>
<li class="chapter" data-level="15.1" data-path="causality.html"><a href="causality.html#causality-1"><i class="fa fa-check"></i><b>15.1</b> Causality</a><ul>
<li class="chapter" data-level="15.1.1" data-path="causality.html"><a href="causality.html#granger"><i class="fa fa-check"></i><b>15.1.1</b> Granger causality</a></li>
<li class="chapter" data-level="15.1.2" data-path="causality.html"><a href="causality.html#causal-additive-models"><i class="fa fa-check"></i><b>15.1.2</b> Causal additive models</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="causality.html"><a href="causality.html#nonstat"><i class="fa fa-check"></i><b>15.2</b> Dealing with changing environments</a><ul>
<li class="chapter" data-level="15.2.1" data-path="causality.html"><a href="causality.html#non-stationarity-an-obvious-illustration"><i class="fa fa-check"></i><b>15.2.1</b> Non-stationarity: an obvious illustration</a></li>
<li class="chapter" data-level="15.2.2" data-path="causality.html"><a href="causality.html#online-learning"><i class="fa fa-check"></i><b>15.2.2</b> Online learning</a></li>
<li class="chapter" data-level="15.2.3" data-path="causality.html"><a href="causality.html#homogeneous-transfer-learning"><i class="fa fa-check"></i><b>15.2.3</b> Homogeneous transfer learning</a></li>
<li class="chapter" data-level="15.2.4" data-path="causality.html"><a href="causality.html#active-learning"><i class="fa fa-check"></i><b>15.2.4</b> Active learning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="unsup.html"><a href="unsup.html"><i class="fa fa-check"></i><b>16</b> Unsupervised learning</a><ul>
<li class="chapter" data-level="16.1" data-path="unsup.html"><a href="unsup.html#corpred"><i class="fa fa-check"></i><b>16.1</b> The problem with correlated predictors</a></li>
<li class="chapter" data-level="16.2" data-path="unsup.html"><a href="unsup.html#principal-component-analysis-and-autoencoders"><i class="fa fa-check"></i><b>16.2</b> Principal component analysis and autoencoders</a><ul>
<li class="chapter" data-level="16.2.1" data-path="unsup.html"><a href="unsup.html#a-bit-of-algebra"><i class="fa fa-check"></i><b>16.2.1</b> A bit of algebra</a></li>
<li class="chapter" data-level="16.2.2" data-path="unsup.html"><a href="unsup.html#pca"><i class="fa fa-check"></i><b>16.2.2</b> PCA</a></li>
<li class="chapter" data-level="16.2.3" data-path="unsup.html"><a href="unsup.html#ae"><i class="fa fa-check"></i><b>16.2.3</b> Autoencoders</a></li>
<li class="chapter" data-level="16.2.4" data-path="unsup.html"><a href="unsup.html#application"><i class="fa fa-check"></i><b>16.2.4</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="unsup.html"><a href="unsup.html#clustering-via-k-means"><i class="fa fa-check"></i><b>16.3</b> Clustering via k-means</a></li>
<li class="chapter" data-level="16.4" data-path="unsup.html"><a href="unsup.html#nearest-neighbors"><i class="fa fa-check"></i><b>16.4</b> Nearest neighbors</a></li>
<li class="chapter" data-level="16.5" data-path="unsup.html"><a href="unsup.html#coding-exercise"><i class="fa fa-check"></i><b>16.5</b> Coding exercise</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="RL.html"><a href="RL.html"><i class="fa fa-check"></i><b>17</b> Reinforcement learning</a><ul>
<li class="chapter" data-level="17.1" data-path="RL.html"><a href="RL.html#theoretical-layout"><i class="fa fa-check"></i><b>17.1</b> Theoretical layout</a><ul>
<li class="chapter" data-level="17.1.1" data-path="RL.html"><a href="RL.html#general-framework"><i class="fa fa-check"></i><b>17.1.1</b> General framework</a></li>
<li class="chapter" data-level="17.1.2" data-path="RL.html"><a href="RL.html#q-learning"><i class="fa fa-check"></i><b>17.1.2</b> Q-learning</a></li>
<li class="chapter" data-level="17.1.3" data-path="RL.html"><a href="RL.html#sarsa"><i class="fa fa-check"></i><b>17.1.3</b> SARSA</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="RL.html"><a href="RL.html#issues-and-potential-solutions"><i class="fa fa-check"></i><b>17.2</b> Issues and potential solutions</a><ul>
<li class="chapter" data-level="17.2.1" data-path="RL.html"><a href="RL.html#the-curse-of-dimensionality"><i class="fa fa-check"></i><b>17.2.1</b> The curse of dimensionality</a></li>
<li class="chapter" data-level="17.2.2" data-path="RL.html"><a href="RL.html#policy-gradient"><i class="fa fa-check"></i><b>17.2.2</b> Policy gradient</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="RL.html"><a href="RL.html#simple-examples"><i class="fa fa-check"></i><b>17.3</b> Simple examples</a><ul>
<li class="chapter" data-level="17.3.1" data-path="RL.html"><a href="RL.html#q-learning-with-simulations"><i class="fa fa-check"></i><b>17.3.1</b> Q-learning with simulations</a></li>
<li class="chapter" data-level="17.3.2" data-path="RL.html"><a href="RL.html#q-learning-with-market-data"><i class="fa fa-check"></i><b>17.3.2</b> Q-learning with market data</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="RL.html"><a href="RL.html#concluding-remarks"><i class="fa fa-check"></i><b>17.4</b> Concluding remarks</a></li>
<li class="chapter" data-level="17.5" data-path="RL.html"><a href="RL.html#exercises"><i class="fa fa-check"></i><b>17.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="NLP.html"><a href="NLP.html"><i class="fa fa-check"></i><b>18</b> Natural Language Processing</a></li>
<li class="chapter" data-level="19" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i><b>19</b> Conclusion</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="data-description.html"><a href="data-description.html"><i class="fa fa-check"></i><b>A</b> Data Description</a></li>
<li class="chapter" data-level="B" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html"><i class="fa fa-check"></i><b>B</b> Solution to exercises</a><ul>
<li class="chapter" data-level="B.1" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-4"><i class="fa fa-check"></i><b>B.1</b> Chapter 4</a></li>
<li class="chapter" data-level="B.2" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-5"><i class="fa fa-check"></i><b>B.2</b> Chapter 5</a></li>
<li class="chapter" data-level="B.3" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-6"><i class="fa fa-check"></i><b>B.3</b> Chapter 6</a></li>
<li class="chapter" data-level="B.4" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-7"><i class="fa fa-check"></i><b>B.4</b> Chapter 7</a></li>
<li class="chapter" data-level="B.5" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-8"><i class="fa fa-check"></i><b>B.5</b> Chapter 8</a></li>
<li class="chapter" data-level="B.6" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-9"><i class="fa fa-check"></i><b>B.6</b> Chapter 9</a></li>
<li class="chapter" data-level="B.7" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-12"><i class="fa fa-check"></i><b>B.7</b> Chapter 12</a></li>
<li class="chapter" data-level="B.8" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-13"><i class="fa fa-check"></i><b>B.8</b> Chapter 13</a><ul>
<li class="chapter" data-level="B.8.1" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#functional-programming-in-the-backtest"><i class="fa fa-check"></i><b>B.8.1</b> Functional programming in the backtest</a></li>
<li class="chapter" data-level="B.8.2" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#advanced-weighting-function"><i class="fa fa-check"></i><b>B.8.2</b> Advanced weighting function</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>C</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning for Factor Investing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="trees" class="section level1">
<h1><span class="header-section-number">Chapter 7</span> Tree-based methods</h1>
<p></p>
<p>Classification and regression trees are simple yet powerful clustering algorithms popularised by the monograph <span class="citation">Breiman et al. (<a href="#ref-breiman1984classification">1984</a>)</span>. Decision trees and their extensions are known to be quite efficient forecasting tools when working on tabular data. A large proportion of winning solutions in ML contests (especially on the Kaggle webiste<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a>) resort to improvements of trees.</p>
<p>Recently, the surge in Machine Learning applications in Finance has led to multiple publications that use trees in portfolio allocation problems. A long though not exhaustive list includes: <span class="citation">Ballings et al. (<a href="#ref-ballings2015evaluating">2015</a>)</span>, <span class="citation">Patel, Shah, Thakkar, and Kotecha (<a href="#ref-patel2015predicting">2015</a><a href="#ref-patel2015predicting">a</a>)</span>, <span class="citation">Patel, Shah, Thakkar, and Kotecha (<a href="#ref-patel2015bpredicting">2015</a><a href="#ref-patel2015bpredicting">b</a>)</span>, <span class="citation">Moritz and Zimmermann (<a href="#ref-moritz2016tree">2016</a>)</span>, <span class="citation">Krauss, Do, and Huck (<a href="#ref-krauss2017deep">2017</a>)</span>, <span class="citation">Gu, Kelly, and Xiu (<a href="#ref-gu2018empirical">2018</a>)</span>, <span class="citation">Guida and Coqueret (<a href="#ref-guida2019big">2018</a><a href="#ref-guida2019big">a</a>)</span>, <span class="citation">Coqueret and Guida (<a href="#ref-coqueret2019training">2019</a>)</span> and <span class="citation">Simonian et al. (<a href="#ref-simonian2019machine">2019</a>)</span>. One notable contribution is <span class="citation">Bryzgalova, Pelger, and Zhu (<a href="#ref-bryzgalova2019forest">2019</a>)</span> in which the authors create factors from trees by sorting portfolios via simple trees, which they call <em>Asset Pricing Trees</em>.</p>
<p>In this chapter, we review the methodologies associated to trees and their applications in portfolio choice.</p>
<div id="simple-trees" class="section level2">
<h2><span class="header-section-number">7.1</span> Simple trees</h2>
<div id="principle" class="section level3">
<h3><span class="header-section-number">7.1.1</span> Principle</h3>
<p>Decision trees seek to partition datasets into homogeneous clusters. Given an exogenous variable <span class="math inline">\(\mathbf{Y}\)</span> and features <span class="math inline">\(\mathbf{X}\)</span>, trees iteratively split the sample into groups (usually two at a time) which are as homogeneous in <span class="math inline">\(\mathbf{Y}\)</span> as possible. The splits are made according to one variable within the set of features. A short word on nomenclature: when <span class="math inline">\(\mathbf{Y}\)</span> consists of real numbers, we talk about <em>regression trees</em> and when <span class="math inline">\(\mathbf{Y}\)</span> is categorical, we use the term <em>classification trees</em>.</p>
<p>Before formalising this idea, we illustrate this process in Figure <a href="trees.html#fig:treescheme">7.1</a>. There are 12 stars with three features: color, size and complexity (number of branches).</p>
<div class="figure"><span id="fig:treescheme"></span>
<img src="images/tree_scheme.png" alt="Elementary tree scheme: visualization of the splitting process." width="850px" />
<p class="caption">
FIGURE 7.1: Elementary tree scheme: visualization of the splitting process.
</p>
</div>
<p>The dependent variable is the color (letâ€™s consider the wavelength associated to the color for simplicity). The first split is made according to size or complexity. Clearly, complexity is the better choice: complicated stars are blue and green while simple stars are yellow, orange and red. Splitting according to size would have mixed blue and yellow stars (small ones) and green and orange stars (large ones).</p>
<p>The second step is to split the two clusters one level further. Since only one variable (size) is relevant, the secondary splits are straightforward. In the end, our stylised tree has four consistent clusters. The analogy with factor investing is simple: the color represents performance: red for high performance and blue for mediocre performance. The features (size and complexity of stars) are replaced by firm-specific attributes, such as capitalization, accounting ratios, etc. Hence, the purpose of the exercise is to find the characteristics that allow to split firms into the ones that will perform well versus those likely to fare more poorly.</p>
<p>We now turn to the technical construction of regression trees. We follow the standard literature as exposed in <span class="citation">Breiman et al. (<a href="#ref-breiman1984classification">1984</a>)</span> or in Chapter 9 of <span class="citation">Hastie, Tibshirani, and Friedman (<a href="#ref-friedman2009elements">2009</a>)</span>. Given a sample of (<span class="math inline">\(y_i\)</span>,<span class="math inline">\(\mathbf{x}_i\)</span>) of size <span class="math inline">\(I\)</span>, a <em>regression</em> tree seeks the splitting points that minimize the total variation of the <span class="math inline">\(y_i\)</span> inside the two child clusters. In order to do that, it proceeds in two steps. First, it finds, for each feature <span class="math inline">\(x_i^{(k)}\)</span>, the best splitting point (so that the clusters are homogeneous in <span class="math inline">\(\mathbf{Y}\)</span>). Second, it selects the feature that achieves the highest level of homogeneity.</p>
<p>Homogeneity in regression trees is closely linked to variance. Since we want the <span class="math inline">\(y_i\)</span> inside each cluster to be similar, we seek to minimize their variability inside each cluster and then sum the two figures. We cannot sum the variances because this would not take into account the relative sizes of clusters. Hence, we work with <em>total</em> variation, which is the variance times the number of elements in the clusters.</p>
<p>Below, the notation is a bit heavy because we resort to superscripts <span class="math inline">\(k\)</span> (the index of the feature), but it is largely possible to ignore these superscripts to ease understanding. The first step is to find the best split for each feature, that is, solve <span class="math inline">\(\underset{c^{(k)}}{\text{argmin}} \ V^{(k)}_I(c^{(k)})\)</span> with
<span class="math display" id="eq:node">\[\begin{equation}
\tag{7.1}
V^{(k)}_I(c^{(k}))= \underbrace{\sum_{x_i^{(k)}&lt;c^{(k)}}\left(y_i-m_I^{k,-}(c^{(k)}) \right)^2}_{\text{Total dispersion of first cluster}} + \underbrace{\sum_{x_i^{(k)}&gt;c^{(k)}}\left(y_i-m_I^{k,+}(c^{(k)}) \right)^2}_{\text{Total dispersion of second cluster}},
\end{equation}\]</span>
where <span class="math inline">\(m_I^{k,-}(c^{(k)})=\frac{1}{\#\{i,x_i^{(k)}&lt;c^{(k)} \}}\sum_{\{x_i^{(k)}&lt;c^{(k)} \}}y_i\)</span> and <span class="math inline">\(m_I^{k,+}(c^{(k)})=\frac{1}{\#\{i,x_i^{(k)}&gt;c^{(k)} \}}\sum_{\{x_i^{(k)}&gt;c^{(k)} \}}y_i\)</span> are the average values of <span class="math inline">\(Y\)</span>, conditional on <span class="math inline">\(X^{(k)}\)</span> being smaller or larger than <span class="math inline">\(c\)</span>. The cardinal function <span class="math inline">\(\#\{\cdot\}\)</span> counts the number of instances of its argument. For feature <span class="math inline">\(k\)</span>, the optimal split <span class="math inline">\(c^{k,*}\)</span> is thus the one for which the total dispersion over the two subgroups is the smallest.</p>
<p>The optimal splits satisfy <span class="math inline">\(c^{k,*}= \underset{c^{(k)}}{\text{argmin}} \ V^{(k)}_I(c^{(k)})\)</span>. Of all the possible splitting variables, the tree will choose the one that minimizes the total dispersion: <span class="math inline">\(k^*=\underset{k}{\text{argmin}} \ V^{(k)}_I(c^{k,*})\)</span>.</p>
<p>After one split is performed, the procedure continues on the two newly formed clusters. There are several criteria that can determine when to stop the splitting process (see Section <a href="trees.html#pruning-criteria">7.1.3</a>). One simple criterion is to fix a maximum number of levels (the depth) for the tree. A usual condition is to impose a minimum gain that is expected for each split. If the reduction in dispersion after the split is only marginal and below a specified threshold, then the split is not executed. For further technical discussions on decision trees, we refer for instance to section 9.2.4 of <span class="citation">Hastie, Tibshirani, and Friedman (<a href="#ref-friedman2009elements">2009</a>)</span>.</p>
<p>When the tree is built (trained), a prediction for new instances is easy to make. Given its feature values, the instance ends up in one leaf of the tree. Each leaf has an average value for the label: this is the predicted outcome. Of course, this only works when the label is numerical. We discuss below the changes that occur when it is categorical.</p>
</div>
<div id="further-details-on-classification" class="section level3">
<h3><span class="header-section-number">7.1.2</span> Further details on classification</h3>
<p>Classification exercises are somewhat more complex than regression tasks. The most obvious difference is the loss function which must take into account the fact that the final output is not a simple number, but a vector. The output <span class="math inline">\(\tilde{\textbf{y}}_i\)</span> has as many elements as there are categories in the label and each element is the probability that the instance belong to the corresponding category.</p>
<p>For instance, if there are 3 categories: <em>buy</em>, <em>hold</em> and <em>sell</em>, then each instance would have a label with as many columns as there are classes. Following our example, one label would be (1,0,0) for a <em>buy</em> position for instance. We refer to Section <a href="Data.html#categorical-labels">5.5.2</a> for a introduction on this topic.</p>
<p>Inside a tree, labels are aggregated at each cluster level. A typical output would look like (0.6,0.1,0.3): they are the proportions of each class represented within the cluster. In this case, the cluster has 60% of <em>buy</em>, 10% of <em>hold</em> and 30% of <em>sell</em>.</p>
<p>The loss function must take into account this multidimensionality of the label. When building trees, since the aim is to favor homogeneity, the loss penalizes outputs that are not concentrated towards one class. Indeed, facing a diversified output of (0.3,0.4,0.3) is much harder to handle than the concentrated case of (0.8,0.1,0.1).</p>
<p>The algorithm is thus seeking purity: it searches a splitting criterion that will lead to clusters that are as pure as possible, i.e., with one very dominant class, or a few dominant classes. There are several metrics proposed by the literature and all are based on the proportions coded in the output. If there are <span class="math inline">\(J\)</span> classes, we denote these proportions with <span class="math inline">\(p_j\)</span>. The usual loss functions are:</p>
<ul>
<li>the Gini impurity index: <span class="math inline">\(1-\sum_{j=1}^Jp_j^2;\)</span><br />
</li>
<li>the misclassification error: <span class="math inline">\(1-\underset{j}{\text{max}}\, p_j;\)</span><br />
</li>
<li>entropy: <span class="math inline">\(-\sum_{j=1}^J\log(p_j)p_j.\)</span></li>
</ul>
<p>The Gini index is nothing but one minus the Herfindahl index which measures the diversification of a portfolio. Trees seek partitions that are the least diversified. The minimum value of the Gini index is zero and reached when one <span class="math inline">\(p_j=1\)</span> and all others are equal to zero. The maximum value is equal to <span class="math inline">\(1-1/J\)</span> and is reached when all <span class="math inline">\(p_j=1/J\)</span>. Similar relationships hold for the other two losses. One drawback of the misclassification error is its lack of differentiability which explains why the other two options are often favored.</p>
<p>Once the tree is grown, new instances automatically belong to one final leaf. This leaf is associated to the proportions of classes it nests.</p>
</div>
<div id="pruning-criteria" class="section level3">
<h3><span class="header-section-number">7.1.3</span> Pruning criteria</h3>
<p>When building a tree, the splitting process can be pursued until the full tree is grown, that is, when:<br />
- when all instances belong to separate leaves, and/or<br />
- when all leaves comprise instances that cannot be further segregated based on the current set of features.</p>
<p>At this stage, the splitting process cannot be pursued.</p>
<p>Obviously, fully grown trees often lead to almost perfect fits when the predictors are relevant, numerous and numerical. Nonetheless, the fine grained idiosyncrasies of the training sample are of little interest for out-of-sample predictions. For instance, being able to perfectly match the patterns of 2000 to 2006 will probably not be very interesting in 2007 to 2009. The most reliable sections of the trees are those closest to the root because they embed large portions of the data: the average values in the early clusters are trustworthy because the are computed on a large number of observations. The first splits are those that matter the most because they highlight the most general patterns. The deepest splits only deal with the peculiarities of the sample.</p>
<p>Thus, it is imperative to limit the size of the tree. There are several ways to prune the tree and all depend on some particular criteria. We list a few of them below:</p>
<ul>
<li>Impose a minimum number of instances for each terminal node (leaf). This ensures that each final cluster is composed by a sufficient number of observations. Hence, the average value of the label will be reliable because calculated on a large amount of data.<br />
</li>
<li>Similarly, it can be imposed that a cluster has a minimal size before even considering any further split. This criterion is of course related to the one above.<br />
</li>
<li>Require a certain threshold of improvement in the fit. If a split does not sufficiently reduce the loss, then it can be deemed unnecessary. The user specifies a small number <span class="math inline">\(\epsilon&gt;0\)</span> and a split is only validated if the loss obtained post-split is smaller than <span class="math inline">\(1-\epsilon\)</span> times the loss before the split.<br />
</li>
<li>Limit the depth of the tree. The depth is defined as the overal maximum number of splits between the root and any leaf of the tree.</li>
</ul>
<p>In the example below, we implement all of these criteria at the same time, but usually, two of them at most should suffice.</p>
</div>
<div id="code-and-interpretation" class="section level3">
<h3><span class="header-section-number">7.1.4</span> Code and interpretation</h3>
<p>We start with a simple tree and its interpretation. The label is the future 1 month return and the features are all predictors available in the sample. The tree is trained on the full sample.</p>

<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb41-1" data-line-number="1"><span class="kw">library</span>(rpart)              <span class="co"># Tree package </span></a>
<a class="sourceLine" id="cb41-2" data-line-number="2"><span class="kw">library</span>(rpart.plot)         <span class="co"># Tree plot package</span></a>
<a class="sourceLine" id="cb41-3" data-line-number="3">formula &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">&quot;R1M_Usd ~&quot;</span>, <span class="kw">paste</span>(features, <span class="dt">collapse =</span> <span class="st">&quot; + &quot;</span>)) <span class="co"># Defines the model </span></a>
<a class="sourceLine" id="cb41-4" data-line-number="4">formula &lt;-<span class="st"> </span><span class="kw">as.formula</span>(formula)                                   <span class="co"># Forcing formula object</span></a>
<a class="sourceLine" id="cb41-5" data-line-number="5">fit_tree &lt;-<span class="st"> </span><span class="kw">rpart</span>(formula,</a>
<a class="sourceLine" id="cb41-6" data-line-number="6">             <span class="dt">data =</span> data_ml,     <span class="co"># Data source: full sample</span></a>
<a class="sourceLine" id="cb41-7" data-line-number="7">             <span class="dt">minbucket =</span> <span class="dv">1500</span>,   <span class="co"># Min nb of obs required in each terminal node (leaf)</span></a>
<a class="sourceLine" id="cb41-8" data-line-number="8">             <span class="dt">minsplit =</span> <span class="dv">4000</span>,    <span class="co"># Min nb of obs required to continue splitting</span></a>
<a class="sourceLine" id="cb41-9" data-line-number="9">             <span class="dt">cp =</span> <span class="fl">0.0001</span>,        <span class="co"># Precision: smaller = more leaves</span></a>
<a class="sourceLine" id="cb41-10" data-line-number="10">             <span class="dt">maxdepth =</span> <span class="dv">3</span>        <span class="co"># Maximum depth (i.e. tree levels)</span></a>
<a class="sourceLine" id="cb41-11" data-line-number="11">             ) </a>
<a class="sourceLine" id="cb41-12" data-line-number="12"><span class="kw">rpart.plot</span>(fit_tree)             <span class="co"># Plot the tree</span></a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:rpart1"></span>
<img src="ML_factor_files/figure-html/rpart1-1.png" alt="Simple tree. The dependent variable is the 1 month future return." width="400px" />
<p class="caption">
FIGURE 7.2: Simple tree. The dependent variable is the 1 month future return.
</p>
</div>
<p></p>
<p>There usually exists a convention in the representation of trees. At each node, a condition describes the split with a boolean expression. If the expression is true, then the instance goes to the left cluster, if not, it goes to the right cluster. Given the whole sample, the initial split in this tree (Figure <a href="trees.html#fig:rpart1">7.2</a>) is performed according to market capitalization. If the 6 month market capitalization score (or value) of the instance is above 0.025, then the instance is placed in the left bucket, otherwise, it goes in the right bucket.</p>
<p>At each node, there are two important metrics. The first one is the average value of the label in the cluster and the second one is the proportion of instances in the cluster. At the top of the tree, all instances (100%) are present and the average 1 month future return is 1.3%. One level below, the left cluster is by far the most crowded, with roughly 98% of observations averaging a 1.2% return. The right cluster is much smaller (2%) but concentrates instances with a much higher average return (6.1%).</p>
<p>The splitting process continues similarly at each node until some condition is satisfied (typically here: the maximum depth is reached). A color codes the average return: from white (low return) to blue (high return). The cluster with the lowest average return consists of firms that satisfy <em>all</em> the following criteria:</p>
<ul>
<li>have a 6 month market capitalization score above 0.025;<br />
</li>
<li>have a 3 month market capitalization score above 0.19;<br />
</li>
<li>have a score of recurring earnings over total assets above 0.025.</li>
</ul>
<p>Notice that one peculiarity of trees is their possible heterogeneity in cluster sizes. Sometimes, a few clusters gather almost all of the observations while a few small groups embed some outliers. This is not a favorable property of trees as small groups are more likely to be flukes and may fail to generalize out-of-sample.</p>
<p>This is why we imposed restrictions during the construction of the tree. The first one (minbucket = 1500 in the code) imposes that each cluster consists of at least 1500 instances. The second one (minsplit) further imposes that a cluster comprises at least 4000 observation in order to pursue the splitting process. The cp = 0.0001 parameter in the code requires any split to reduce the loss below 0.9999 times its original value before the split. Finally, the maximum depth of three essentially means that there are at most three splits between the root of the tree and any terminal leaf.</p>
<p>The complexity of the tree (measured by the number of terminal leaves) is a decreasing function of minbucket, minsplit and cp and an increasing function of maximum depth.</p>
<p>Once the model has be trained (i.e., the tree is grown), a prediction for any instance is the average value of the label within the cluster where the instance should land.</p>

<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb42-1" data-line-number="1"><span class="kw">predict</span>(fit_tree, data_ml[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,]) <span class="co"># Test (prediction) on the first five instances of the sample</span></a></code></pre></div>
<pre><code>##           1           2           3           4           5 
## 0.009529823 0.009529823 0.009529823 0.009529823 0.009529823</code></pre>
<p></p>
<p>Given the figure, we immediately conclude that the first three instances belong to the second cluster (starting from the left) and the last two belong to the fourth.</p>
<p>As a verification of the first split, we plot the smoothed average of future returns, conditionally on market capitalization, past return and price-to-book scores.</p>

<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb44-1" data-line-number="1">data_ml <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ggplot</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb44-2" data-line-number="2"><span class="st">    </span><span class="kw">stat_smooth</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Mkt_Cap_6M_Usd, <span class="dt">y =</span> R1M_Usd, <span class="dt">color =</span> <span class="st">&quot;Market Cap&quot;</span>), <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb44-3" data-line-number="3"><span class="st">    </span><span class="kw">stat_smooth</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Pb, <span class="dt">y =</span> R1M_Usd, <span class="dt">color =</span> <span class="st">&quot;Price-to-Book&quot;</span>), <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb44-4" data-line-number="4"><span class="st">    </span><span class="kw">stat_smooth</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Mom_Sharp_11M_Usd, <span class="dt">y =</span> R1M_Usd, <span class="dt">color =</span> <span class="st">&quot;Momentum&quot;</span>), <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb44-5" data-line-number="5"><span class="st">    </span><span class="kw">xlab</span>(<span class="st">&quot;Predictor&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:rpart3mkt"></span>
<img src="ML_factor_files/figure-html/rpart3mkt-1.png" alt="Average of 1 month future returns, conditionally on market capitalization and price-to-book scores" width="400px" />
<p class="caption">
FIGURE 7.3: Average of 1 month future returns, conditionally on market capitalization and price-to-book scores
</p>
</div>
<p></p>
<p>Indeed, we acknowledge a strong impact of market capitalization for the smallest firms: they appear to earn a very high return (close to +5% on a monthly basis). The pattern is much more pronounced compared to other well documented anomalies namely the value premium and the (cross-sectional) momentum effect.</p>
<p>Finally, we assess the predictive quality of a single tree on the testing set (the tree is grown on the training set). We use a deeper tree, with a maximum depth of five.</p>

<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb45-1" data-line-number="1">fit_tree2 &lt;-<span class="st"> </span><span class="kw">rpart</span>(formula, </a>
<a class="sourceLine" id="cb45-2" data-line-number="2">             <span class="dt">data =</span> training_sample,     <span class="co"># Data source: training sample</span></a>
<a class="sourceLine" id="cb45-3" data-line-number="3">             <span class="dt">minbucket =</span> <span class="dv">1500</span>,           <span class="co"># Min nb of obs required in each terminal node (leaf)</span></a>
<a class="sourceLine" id="cb45-4" data-line-number="4">             <span class="dt">minsplit =</span> <span class="dv">4000</span>,            <span class="co"># Min nb of obs required to continue splitting</span></a>
<a class="sourceLine" id="cb45-5" data-line-number="5">             <span class="dt">cp =</span> <span class="fl">0.0001</span>,                <span class="co"># Precision: smaller = more leaves</span></a>
<a class="sourceLine" id="cb45-6" data-line-number="6">             <span class="dt">maxdepth =</span> <span class="dv">5</span>                <span class="co"># Maximum depth (i.e. tree levels)</span></a>
<a class="sourceLine" id="cb45-7" data-line-number="7">             ) </a>
<a class="sourceLine" id="cb45-8" data-line-number="8"><span class="kw">mean</span>((<span class="kw">predict</span>(fit_tree2, testing_sample) <span class="op">-</span><span class="st"> </span>testing_sample<span class="op">$</span>R1M_Usd)<span class="op">^</span><span class="dv">2</span>) <span class="co"># MSE</span></a></code></pre></div>
<pre><code>## [1] 0.03700039</code></pre>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb47-1" data-line-number="1"><span class="kw">mean</span>(<span class="kw">predict</span>(fit_tree2, testing_sample) <span class="op">*</span><span class="st"> </span>testing_sample<span class="op">$</span>R1M_Usd <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>) <span class="co"># Hit ratio</span></a></code></pre></div>
<pre><code>## [1] 0.5416619</code></pre>
<p></p>
<p>The mean squared error is usually hard to interpret. Itâ€™s not easy to map an error on returns into the impact on investment decisions. The hit ratio is a more intuitive indicator because it evaluates the proportion of correct guesses (and hence profitable investments). Obviously, it is not perfect: 55% of small gains can be mitigated by 45% of large losses. Nonetheless, it is a popular metric and moreover it corresponds to the usual accuracy measure often computed in binary classification exercises. Here, an accuracy of 0.5416619 is satisfactory. Even if any number above 50% may seem valuable, it must not be forgottent that transaction costs will curtail benefits. Hence, the benchmark threshold is probably at least at 52%.</p>
</div>
</div>
<div id="random-forests" class="section level2">
<h2><span class="header-section-number">7.2</span> Random forests</h2>
<p>While trees give intuitive representations of relationships between <span class="math inline">\(\mathbf{Y}\)</span> and <span class="math inline">\(\mathbf{X}\)</span>, they can be improved via the simple idea of ensembles (which is discussed both more generally and in more details in Chapter <a href="ensemble.html#ensemble">12</a>).</p>
<div id="principle-1" class="section level3">
<h3><span class="header-section-number">7.2.1</span> Principle</h3>
<p>One intuitive extension of predicting tools is a <em>combination</em> of such tools. Naturally, it is not obvious upfront which individual model is the best, hence a combination seems a reasonable path towards the diversification of prediction errors (when they are not too correlated). Some theoretical foundations of model combinations were laid out in <span class="citation">Schapire (<a href="#ref-schapire1990strength">1990</a>)</span>.</p>
<p>More practical considerations were proposed later in <span class="citation">Ho (<a href="#ref-ho1995random">1995</a>)</span> and more importantly in <span class="citation">Breiman (<a href="#ref-breiman2001random">2001</a>)</span> which is the major reference for random forests. There are two ways to create multiple predictors from simple trees and random forests combines both:</p>
<ul>
<li>first, the model can be trained on similar yet different datasets. One way to achieve this is via bootstrap: the instances are resampled with or without replacement, yielding new training data each time a new tree is built.</li>
<li>second, the data can be altered by curtailing the number of predictors. Alternative models are built based on different sets of features. The user chooses how many features to retain and then the algorithm selects these features randomly at each try.</li>
</ul>
<p>Hence, it becomes simple to grow many different trees and the ensemble is simply a weigthed combination of all trees. Usually, equal weights are used, which is an agnostic and robust choice. We illustrate the idea of simple combinations (also referred to as bagging) in Figure  below. The terminal prediction is simply the mean of all intermediate predictions.</p>
<div class="figure"><span id="fig:RF"></span>
<img src="images/tree_RF.png" alt="Combining tree outputs via random forests." width="826" />
<p class="caption">
FIGURE 7.4: Combining tree outputs via random forests.
</p>
</div>
<p>Random forests, because they are built on the idea of bootstrapping are more efficient than simple trees. They are used by <span class="citation">Ballings et al. (<a href="#ref-ballings2015evaluating">2015</a>)</span>, <span class="citation">Patel, Shah, Thakkar, and Kotecha (<a href="#ref-patel2015predicting">2015</a><a href="#ref-patel2015predicting">a</a>)</span>, <span class="citation">Krauss, Do, and Huck (<a href="#ref-krauss2017deep">2017</a>)</span>, and <span class="citation">Huck (<a href="#ref-huck2019large">2019</a>)</span> and they are shown to perform very well in these papers. The original theoretical properties of random forests are demonstrated in <span class="citation">Breiman (<a href="#ref-breiman2001random">2001</a>)</span> for classification trees and are fairly straightforward (itâ€™s likely similar formulations hold for other ensemble types). The inaccuracy of the aggregation (as measured by the so-called generalization error) is bounded by <span class="math inline">\(\bar{\rho}(1-s^2)/s^2\)</span>, where<br />
- <span class="math inline">\(s\)</span> is the strength (average quality) of the individual classifiers and<br />
- <span class="math inline">\(\bar{\rho}\)</span> is the average correlation between the learners.</p>
<p>Notably, <span class="citation">Breiman (<a href="#ref-breiman2001random">2001</a>)</span> also shows that as the number of trees grows to infinity, the inaccuracy converges to some finite number which explains why random forests are not prone to overfitting.</p>
<p>While the original paper of <span class="citation">Breiman (<a href="#ref-breiman2001random">2001</a>)</span> is dedicated to classification models, many articles have since then tackled the problem of regression trees. We refer the interested reader to <span class="citation">Biau (<a href="#ref-biau2012analysis">2012</a>)</span> and to <span class="citation">Scornet et al. (<a href="#ref-scornet2015consistency">2015</a>)</span>. Finally, further results on classifying ensembles can be obtained in <span class="citation">Biau, Devroye, and Lugosi (<a href="#ref-biau2008consistency">2008</a>)</span> and we mention the short survey paper <span class="citation">Denil, Matheson, and De Freitas (<a href="#ref-denil2014narrowing">2014</a>)</span> which sums up recent results in this field.</p>
</div>
<div id="code-and-results-1" class="section level3">
<h3><span class="header-section-number">7.2.2</span> Code and results</h3>
<p>Several implementations of random forests exist. For simplicity, we choose to work with the original R library, but another choice could be the one developed by h2o, which is a highly efficient meta-environment for machine learning (coded in Java).</p>
<p>The syntax of randomForest follows that of many ML libraries. The full list of options for some random forest implementations is prohibitively large.<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a> Below, we train a model and exhibit the predictions for the first 5 instances of the testing sample.</p>

<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb49-1" data-line-number="1"><span class="kw">library</span>(randomForest) </a>
<a class="sourceLine" id="cb49-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">42</span>)                                <span class="co"># Sets the random seed</span></a>
<a class="sourceLine" id="cb49-3" data-line-number="3">fit_RF &lt;-<span class="st"> </span><span class="kw">randomForest</span>(formula,             <span class="co"># Same formula as for simple trees!</span></a>
<a class="sourceLine" id="cb49-4" data-line-number="4">                 <span class="dt">data =</span> training_sample,    <span class="co"># Data source: training sample</span></a>
<a class="sourceLine" id="cb49-5" data-line-number="5">                 <span class="dt">sampsize =</span> <span class="dv">10000</span>,          <span class="co"># Size of (random) sample for each tree</span></a>
<a class="sourceLine" id="cb49-6" data-line-number="6">                 <span class="dt">replace =</span> <span class="ot">FALSE</span>,           <span class="co"># Is the sampling done with replacement?</span></a>
<a class="sourceLine" id="cb49-7" data-line-number="7">                 <span class="dt">nodesize =</span> <span class="dv">250</span>,            <span class="co"># Minimum size of terminal cluster</span></a>
<a class="sourceLine" id="cb49-8" data-line-number="8">                 <span class="dt">ntree =</span> <span class="dv">40</span>,                <span class="co"># Nb of random trees</span></a>
<a class="sourceLine" id="cb49-9" data-line-number="9">                 <span class="dt">mtry =</span> <span class="dv">30</span>                  <span class="co"># Nb of predictive variables for each tree</span></a>
<a class="sourceLine" id="cb49-10" data-line-number="10">    )</a>
<a class="sourceLine" id="cb49-11" data-line-number="11"><span class="kw">predict</span>(fit_RF, testing_sample[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,])       <span class="co"># Prediction over the first 5 test instances </span></a></code></pre></div>
<pre><code>##            1            2            3            4            5 
##  0.009787728  0.012507087  0.008722386  0.009398814 -0.011511758</code></pre>
<p></p>
<p>One first comment is that each instance has its own prediction, which contrasts with the outcome of simple tree-based outcomes. Combining many trees leads to tailored forecasts. Note that the second line of the chunk freezes the random number generation. Indeed, random forests are by construction contingent on the arbitrary combinations of instances and features that are chosen to build the individual learners.</p>
<p>In the above example, each individual learner (tree) is built on 10,000 randomly chosen instances (without replacement) and each terminal leaf (cluster) must comprise at least 240 elements (observations). In total, 40 trees are aggregated and each tree is constructed based on 30 randomly chosen predictors (out of the whole set of features).</p>
<p>Unlike for simple trees, it is not possible to simply illustrate the outcome of the learning process. It could be possible to extract all 40 trees but a synthetic visualization is out-of-reach. A simplified view can be obtained via variable importance, as is discussed in Section <a href="interp.html#variable-importance">14.1.1</a>.</p>
<p>Finally, we can assess the accuracy of the model.</p>

<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb51-1" data-line-number="1"><span class="kw">mean</span>((<span class="kw">predict</span>(fit_RF, testing_sample) <span class="op">-</span><span class="st"> </span>testing_sample<span class="op">$</span>R1M_Usd)<span class="op">^</span><span class="dv">2</span>) <span class="co"># MSE</span></a></code></pre></div>
<pre><code>## [1] 0.03698197</code></pre>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb53-1" data-line-number="1"><span class="kw">mean</span>(<span class="kw">predict</span>(fit_RF, testing_sample) <span class="op">*</span><span class="st"> </span>testing_sample<span class="op">$</span>R1M_Usd <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>) <span class="co"># Hit ratio</span></a></code></pre></div>
<pre><code>## [1] 0.5370186</code></pre>
<p></p>
<p>The MSE is smaller than 4% and the hit ratio is close to 54%, which is reasonably above both 50% and 52% thresholds.</p>
<p>Letâ€™s see if we can improve the hit ratio by resorting to a classification exercise. We start by training the model on a new formula (the label is R1M_Usd_C).</p>

<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb55-1" data-line-number="1">formula_C &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">&quot;R1M_Usd_C ~&quot;</span>, <span class="kw">paste</span>(features, <span class="dt">collapse =</span> <span class="st">&quot; + &quot;</span>)) <span class="co"># Defines the model </span></a>
<a class="sourceLine" id="cb55-2" data-line-number="2">formula_C &lt;-<span class="st"> </span><span class="kw">as.formula</span>(formula_C)                                   <span class="co"># Forcing formula object</span></a>
<a class="sourceLine" id="cb55-3" data-line-number="3">fit_RF_C &lt;-<span class="st"> </span><span class="kw">randomForest</span>(formula_C,         <span class="co"># New formula! </span></a>
<a class="sourceLine" id="cb55-4" data-line-number="4">                 <span class="dt">data =</span> training_sample,    <span class="co"># Data source: training sample</span></a>
<a class="sourceLine" id="cb55-5" data-line-number="5">                 <span class="dt">sampsize =</span> <span class="dv">20000</span>,          <span class="co"># Size of (random) sample for each tree</span></a>
<a class="sourceLine" id="cb55-6" data-line-number="6">                 <span class="dt">replace =</span> <span class="ot">FALSE</span>,           <span class="co"># Is the sampling done with replacement?</span></a>
<a class="sourceLine" id="cb55-7" data-line-number="7">                 <span class="dt">nodesize =</span> <span class="dv">250</span>,            <span class="co"># Minimum size of terminal cluster</span></a>
<a class="sourceLine" id="cb55-8" data-line-number="8">                 <span class="dt">ntree =</span> <span class="dv">40</span>,                <span class="co"># Number of random trees</span></a>
<a class="sourceLine" id="cb55-9" data-line-number="9">                 <span class="dt">mtry =</span> <span class="dv">30</span>                  <span class="co"># Number of predictive variables for each tree </span></a>
<a class="sourceLine" id="cb55-10" data-line-number="10">    )</a></code></pre></div>
<p></p>
<p>We can then assess the proportion of correct (binary) guesses.
</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb56-1" data-line-number="1"><span class="kw">mean</span>(<span class="kw">predict</span>(fit_RF_C, testing_sample) <span class="op">==</span><span class="st"> </span>testing_sample<span class="op">$</span>R1M_Usd_C) <span class="co"># Hit ratio</span></a></code></pre></div>
<pre><code>## [1] 0.4984475</code></pre>
<p></p>
<p>The accuracy is disappointing. There are two potential explanations for this (beyond the possibility of very different patterns in the training and testing sets). The first one is the sample size, which may be too small. The original training set has more than 200,000 observations, hence we retain only one in 10 in the above training specification. We are thus probably sideline relevant information and the cost can be heavy. The second reason is the number of predictors, which is set to 30, i.e., one third of the total at our disposal. Unfortunately, this leaves room for the algorithm to pick less pertinent predictors. The default numbers of predictors chosen by the routines are <span class="math inline">\(\sqrt{p}\)</span> and <span class="math inline">\(p/3\)</span> for classification and regression tasks, respectively. Here <span class="math inline">\(p\)</span> is the total number of features.</p>
</div>
</div>
<div id="adaboost" class="section level2">
<h2><span class="header-section-number">7.3</span> Boosted trees: Adaboost</h2>
<p>The idea of boosting is slightly more advanced compared to agnostic aggregation. In random forest, we hope that the diversification through many trees will improve the overall quality of the model. In boosting, it is sought to iteratively improve the model whenever a new tree is added. There are many ways to boost learning and we present two that can easily be implemented with trees. The first one (Adaboost, for adaptive boosting) improves the learning process by progressively focusing on the instances that yield the largest errors. The second one (xgboost) is a flexible algorithm in which each new tree is only focused on the minimization of the training sample loss.</p>
<div id="methodology" class="section level3">
<h3><span class="header-section-number">7.3.1</span> Methodology</h3>
<p>The origins of adaboost go back to <span class="citation">Freund and Schapire (<a href="#ref-freund1997decision">1997</a>)</span>, <span class="citation">Freund and Schapire (<a href="#ref-freund1996experiments">1996</a>)</span> and for the sake of completeness, we also mention the book dedicated on boosting <span class="citation">Schapire and Freund (<a href="#ref-schapire2012boosting">2012</a>)</span>. Extensions of these ideas are proposed in <span class="citation">Friedman et al. (<a href="#ref-friedman2000additive">2000</a>)</span> (the so-called real Adaboost algorithm) and in <span class="citation">Drucker (<a href="#ref-drucker1997improving">1997</a>)</span> (for regression analysis). Theoretical treatments were derived by <span class="citation">Breiman and others (<a href="#ref-breiman2004population">2004</a>)</span>.</p>
<p>We start by directly stating the general structure of the algorithm:</p>
<ul>
<li>set equal weights <span class="math inline">\(w_i=I^{-1}\)</span>;<br />
</li>
<li>For <span class="math inline">\(m=1,\dots,M\)</span> do:</li>
</ul>
<ol style="list-style-type: decimal">
<li>Find a learner <span class="math inline">\(l_m\)</span> that minimizes the weighted loss <span class="math inline">\(\sum_{i=1}^Iw_iL(l_m(\textbf{x}_i),\textbf{y}_i)\)</span>;</li>
<li>Compute a learner weight
<span class="math display" id="eq:adaboostam">\[\begin{equation}
\tag{7.2}
a_m=f_a(\textbf{w},l_m(\textbf{x}),\textbf{y});
\end{equation}\]</span></li>
<li>Update the instance weights
<span class="math display" id="eq:adaboostw">\[\begin{equation}
\tag{7.3}
w_i \leftarrow w_ie^{f_w(l_m(\textbf{x}_i), \textbf{y}_i)};
\end{equation}\]</span></li>
<li>Normalize the <span class="math inline">\(w_i\)</span> to sum to one;</li>
</ol>
<ul>
<li>The output for instance <span class="math inline">\(\textbf{x}_i\)</span> is a simple function of <span class="math inline">\(\sum_{m=1}^M a_ml_m(\textbf{x}_i)\)</span>,
<span class="math display" id="eq:adaboosty">\[\begin{equation}
\tag{7.4}
\tilde{y}_i=f_y\left(\sum_{m=1}^M a_ml_m(\textbf{x}_i) \right).
\end{equation}\]</span></li>
</ul>
<p>Let us comment on the steps of the algorithm. The formulation holds for many variations of Adaboost and we will specify the functions <span class="math inline">\(f_a\)</span> and <span class="math inline">\(f_w\)</span> below.</p>
<ol style="list-style-type: decimal">
<li>The first step seeks to find a learner (tree) <span class="math inline">\(l_m\)</span> that minimizes a weighted loss. Here the base loss function <span class="math inline">\(L\)</span> essentially depends on the task (regression versus classification).</li>
<li>The second and third steps are the most interesting because they are the heart of Adaboost: they define the way the algorithm adapts sequentially. Because the purpose is to aggregate models, a more sophisticated approach compared to uniform weights for learners is a tailored weight for each learner. A natural property (for <span class="math inline">\(f_a\)</span>) should be that a learner that yields a smaller error should have a larger weight because it is more accurate.</li>
<li>The third step is to change the weights of observations. In this case, because the model aims at improving the learning process, <span class="math inline">\(f_w\)</span> is constructed to give more weight on observations for which the current model does not do a good job (i.e., generates the largest errors). Hence, the next learner will be incentivized to pay more attention on these pathological cases.</li>
<li>The third step is a simple scaling procedure.</li>
</ol>
<p>In Table <a href="trees.html#tab:adaboost">7.1</a>, we detail two examples of weighting functions used in the literature. For the original Adaboost (<span class="citation">Freund and Schapire (<a href="#ref-freund1996experiments">1996</a>)</span>, <span class="citation">Freund and Schapire (<a href="#ref-freund1997decision">1997</a>)</span>), the label is binary with values +1 and -1 only. The second example stems from <span class="citation">Drucker (<a href="#ref-drucker1997improving">1997</a>)</span> and is dedicated to regression analysis (with real-valued label). The interested reader can have a look at other possibilities in <span class="citation">Schapire (<a href="#ref-schapire2003boosting">2003</a>)</span> and <span class="citation">Ridgeway, Madigan, and Richardson (<a href="#ref-ridgeway1999boosting">1999</a>)</span>.</p>
<table>
<caption><span id="tab:adaboost">TABLE 7.1: </span> Examples of functions for Adaboost-like algorithms.</caption>
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Bin. classif. (orig. Adaboost)</th>
<th>Regression (Drucker (1997))</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Individual error</td>
<td><span class="math inline">\(\epsilon_i=\textbf{1}_{\left\{y_i\neq l_m(\textbf{x}_i) \right\}}\)</span></td>
<td><span class="math inline">\(\epsilon_i=\frac{|y_i- l_m(\textbf{x}_i)|}{\underset{i}{\max}|y_i- l_m(\textbf{x}_i)|}\)</span></td>
</tr>
<tr class="even">
<td>Weight of learner via <span class="math inline">\(f_a\)</span></td>
<td><span class="math inline">\(f_a=\log\left(\frac{1-\epsilon}{\epsilon} \right)\)</span>,with <span class="math inline">\(\epsilon=I^{-1}\sum_{i=1}^Iw_i \epsilon_i\)</span></td>
<td><span class="math inline">\(f_a=\log\left(\frac{1-\epsilon}{\epsilon} \right)\)</span>,with <span class="math inline">\(\epsilon=I^{-1}\sum_{i=1}^Iw_i \epsilon_i\)</span></td>
</tr>
<tr class="odd">
<td>Weight of instances via <span class="math inline">\(f_w(i)\)</span></td>
<td><span class="math inline">\(f_w=f_a\epsilon_i\)</span></td>
<td><span class="math inline">\(f_w=f_a\epsilon_i\)</span></td>
</tr>
<tr class="even">
<td>Output function via <span class="math inline">\(f_y\)</span></td>
<td><span class="math inline">\(f_y(x) = \text{sign}(x)\)</span></td>
<td>weighted median of predictions</td>
</tr>
</tbody>
</table>
<p>Let us comment on the original Adaboost specification. The basic error term <span class="math inline">\(\epsilon_i=\textbf{1}_{\left\{y_i\neq l_m(\textbf{x}_i) \right\}}\)</span> is a dummy number indicating if the prediction is correct (we recall only two values are possible, +1 and -1). The average error <span class="math inline">\(\epsilon\in [0,1]\)</span> is simply a weighted average of individual errors and the weight of the <span class="math inline">\(m^{th}\)</span> learner defined in Equation <a href="trees.html#eq:adaboostam">(7.2)</a> is given by <span class="math inline">\(a_m=\log\left(\frac{1-\epsilon}{\epsilon} \right)\)</span>. The function <span class="math inline">\(x\mapsto \log((1-x)x^{-1})\)</span> decreases on <span class="math inline">\([0,1]\)</span> and switches sign (from positive to negative) at <span class="math inline">\(x=1/2\)</span>. Hence, when the average error is small, the learner has a large positive weight but when the error becomes large, the learner can even obtain a negative weight. Indeed, the threshold <span class="math inline">\(\epsilon&gt;1/2\)</span> indicated that the learner is wrong more than 50% of the time. Obviously, this indicates a problem and the learner should even be discarded.</p>
<p>The change in instance weights follows a similar logic. The new weight is proportional to <span class="math inline">\(w_i\left(\frac{1-\epsilon}{\epsilon} \right)^{\epsilon_i}\)</span>. If the prediction is right and <span class="math inline">\(\epsilon_i=0\)</span>, the weight is unchanged. If the prediction is wrong and <span class="math inline">\(\epsilon_i=1\)</span>, the weight is adjusted depending on the aggregate error <span class="math inline">\(\epsilon\)</span>. If the error is small and the learner efficient (<span class="math inline">\(\epsilon&lt;1/2\)</span>), then <span class="math inline">\((1-\epsilon)/\epsilon&gt;1\)</span> and the weight of the instance increases. This means that for the next round, the learner will have to focus more on instance <span class="math inline">\(i\)</span>.</p>
<p>Lastly, the final prediction of the model corresponds to the sign of the weighted sums of individual predictions: if the sum is positive, the model will predict +1 and it will yield -1 otherwise.<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a> The odds of a zero sum are negligible. In the case of numerical labels, the process is slightly more complicated and we refer to Section 3, step 8 of <span class="citation">Drucker (<a href="#ref-drucker1997improving">1997</a>)</span> for more details on how to proceed.</p>
<p>We end this presentation with one word on instance weighting. There are two ways to deal with this topic. The first one works at the level of the loss functions. For regression trees, Equation <a href="trees.html#eq:node">(7.1)</a> would naturally generalize to
<span class="math display">\[V^{(k)}_N(c^{(k}), \textbf{w})= \sum_{x_i^{(k)}&lt;c^{(k)}}w_i\left(y_i-m_N^{k,-}(c^{(k)}) \right)^2 + \sum_{x_i^{(k)}&gt;c^{(k)}}w_i\left(y_i-m_N^{k,+}(c^{(k)}) \right)^2,\]</span></p>
<p>and hence an instance with a large weight <span class="math inline">\(w_i\)</span> would contribute more to the dispersion of its cluster. For classification objectives, the alteration is more complex and we refer to <span class="citation">Ting (<a href="#ref-ting2002instance">2002</a>)</span> for one example of an instance-weighted tree-growing algorithm. The idea is closely linked to the alteration of the misclassification risk via a loss matrix (see Section 9.2.4 in <span class="citation">Hastie, Tibshirani, and Friedman (<a href="#ref-friedman2009elements">2009</a>)</span>).</p>
<p>The second way to enforce instance weighting is via random sampling. If instances have weights <span class="math inline">\(w_i\)</span>, then the training of learners can be performed over a sample that is randomly extracted with distribution equal to <span class="math inline">\(w_i\)</span>. In this case, an instance with a larger weight will have more chances to be represented in the training sample. The original adaboost algorithm relies on this method.</p>
</div>
<div id="illustration" class="section level3">
<h3><span class="header-section-number">7.3.2</span> Illustration</h3>
<p>Below, we test an implementation of the original adaboost classifier. As such, we work with the R1M_Usd_C variable and change the model formula. The computational cost of adaboost is high on large datasets, thus we work with a smaller sample and we only impose three iterations.</p>

<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb58-1" data-line-number="1"><span class="kw">library</span>(fastAdaboost)                                                     <span class="co"># Adaboost package </span></a>
<a class="sourceLine" id="cb58-2" data-line-number="2">subsample &lt;-<span class="st"> </span>(<span class="dv">1</span><span class="op">:</span><span class="dv">52000</span>)<span class="op">*</span><span class="dv">4</span>                                                  <span class="co"># Target small sample</span></a>
<a class="sourceLine" id="cb58-3" data-line-number="3">fit_adaboost_C &lt;-<span class="st"> </span><span class="kw">adaboost</span>(formula_C,                                     <span class="co"># Model spec.</span></a>
<a class="sourceLine" id="cb58-4" data-line-number="4">                         <span class="dt">data =</span> <span class="kw">data.frame</span>(training_sample[subsample,]),  <span class="co"># Data source</span></a>
<a class="sourceLine" id="cb58-5" data-line-number="5">                         <span class="dt">nIter =</span> <span class="dv">3</span>)                                       <span class="co"># Number of trees              </span></a></code></pre></div>
<p></p>
<p>Finally, we evaluate the performance of the classifier.</p>

<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb59-1" data-line-number="1"><span class="kw">mean</span>(testing_sample<span class="op">$</span>R1M_Usd_C <span class="op">==</span><span class="st"> </span><span class="kw">predict</span>(fit_adaboost_C, testing_sample)<span class="op">$</span>class)</a></code></pre></div>
<pre><code>## [1] 0.5028202</code></pre>
<p></p>
<p>The accuracy (as evaluated by the hit ratio) is clearly not satisfactory. One reason for this may be the restrictions we enforced for the training (smaller sample and only three trees).</p>
</div>
</div>
<div id="boosted-trees-extreme-gradient-boosting" class="section level2">
<h2><span class="header-section-number">7.4</span> Boosted trees: extreme gradient boosting</h2>
<p>The ideas behind tree boosting were popularized, among others, by <span class="citation">Mason et al. (<a href="#ref-mason2000boosting">2000</a>)</span>, <span class="citation">Friedman (<a href="#ref-friedman2001greedy">2001</a>)</span>, and <span class="citation">Friedman (<a href="#ref-friedman2002stochastic">2002</a>)</span>. In this case, the combination of learners (prediction tools) is not agnostic as in random forest, but adapted (or optimized) at the learner level. At each step <span class="math inline">\(s\)</span>, the sum of models <span class="math inline">\(M_S=\sum_{s=1}^{S-1}m_s+m_S\)</span> is such that the last learner <span class="math inline">\(m_S\)</span> was precisely designed to reduce the loss of <span class="math inline">\(M_S\)</span> on the training sample.</p>
<p>Below, we follow closely the original work of <span class="citation">T. Chen and Guestrin (<a href="#ref-chen2016xgboost">2016</a>)</span> because their algorithm yields incredibly accurate predictions and also because it is highly customizable. It is their implementation that we use in our empirical section. The other popular alternative is lightgbm (see <span class="citation">Ke et al. (<a href="#ref-ke2017lightgbm">2017</a>)</span>). What XGBoost seeks to minimise is the objective
<span class="math display">\[O=\underbrace{\sum_{i=1}^I \text{loss}(y_i,\tilde{y}_i)}_{\text{error term}} \quad + \underbrace{\sum_{j=1}^J\Omega(T_j)}_{\text{regularisation term}}.\]</span>
The first term (over all instances) measures the distance between the true label and the output from the model. The second term (over all trees) penalises models that are too complex.</p>
<p>For simplicity, we propose the full derivation with the simplest loss function <span class="math inline">\(\text{loss}(y,\tilde{y})=(y-\tilde{y})^2\)</span>, so that:
<span class="math display">\[O=\sum_{i=1}^I \left(y_i-m_{J-1}(\mathbf{x}_i)-T_J(\mathbf{x}_i)\right)^2+ \sum_{j=1}^J\Omega(T_j).\]</span></p>
<div id="managing-loss" class="section level3">
<h3><span class="header-section-number">7.4.1</span> Managing Loss</h3>
<p>Let us assume that we have already built all trees <span class="math inline">\(T_{j}\)</span> up to <span class="math inline">\(j=1,\dots,J-1\)</span> (and hence model <span class="math inline">\(M_{J-1}\)</span>): how to choose tree <span class="math inline">\(T_J\)</span> optimally? We rewrite
<span class="math display">\[\begin{align*}
O&amp;=\sum_{i=1}^I \left(y_i-m_{J-1}(\mathbf{x}_i)-T_J(\mathbf{x}_i)\right)^2+ \sum_{j=1}^J\Omega(T_j) \\
&amp;=\sum_{i=1}^I\left\{y_i^2+m_{J-1}(\mathbf{x}_i)^2+T_J(\mathbf{x}_i)^2 \right\} + \sum_{j=1}^{J-1}\Omega(T_j)+\Omega(T_J) \quad \text{(squared terms + penalisation)}\\
&amp; \quad -2 \sum_{i=1}^I\left\{y_im_{J-1}(\mathbf{x}_i)+y_iT_J(\mathbf{x}_i)-m_{J-1}(\mathbf{x}_i) T_J(\mathbf{x}_i))\right\}\quad \text{(cross terms)} \\
&amp;= \sum_{i=1}^I\left\{-2 y_iT_J(\mathbf{x}_i)+2m_{J-1}(\mathbf{x}_i) T_J(\mathbf{x}_i))+T_J(\mathbf{x}_i)^2 \right\} +\Omega(T_J) + c
\end{align*}\]</span>
All terms known at step <span class="math inline">\(J\)</span> (i.e., indexed by <span class="math inline">\(J-1\)</span>) vanish because they do not enter the optimisation scheme. The are embedded in the constant <span class="math inline">\(c\)</span>.</p>
<p>Things are fairly simple with quadratic loss. For more complicated loss functions, Taylor expansions are used (see the original paper).</p>
</div>
<div id="penalisation" class="section level3">
<h3><span class="header-section-number">7.4.2</span> Penalisation</h3>
<p>In order to go any further, we need to specify the way the penalisation works. For a given tree <span class="math inline">\(T\)</span>, we specify its structure by <span class="math inline">\(T(x)=w_{q(x)}\)</span>, where <span class="math inline">\(w\)</span> is the output value of some leaf and <span class="math inline">\(q(\cdot)\)</span> is the function that maps an input to its final leaf. This encoding is illustrated in Figure <a href="trees.html#fig:treeq">7.5</a>. The function <span class="math inline">\(q\)</span> indicates the path, while the vector <span class="math inline">\(\textbf{w}=w_i\)</span> codes the terminal leaf values.</p>
<div class="figure" style="text-align: center"><span id="fig:treeq"></span>
<img src="images/tree_q.png" alt="Coding a decision tree" width="400px" />
<p class="caption">
FIGURE 7.5: Coding a decision tree
</p>
</div>
<p>We write <span class="math inline">\(l=1,\dots,L\)</span> for the indices of the leafs of the tree. In XGBoost, complexity is defined as:
<span class="math display">\[\Omega(T)=\gamma L+\frac{\lambda}{2}\sum_{l=1}^Lw_l^2,\]</span>
where</p>
<ul>
<li>the first term penalises the <strong>total number of leaves</strong>;<br />
</li>
<li>the second term penalises the <strong>magnitude of output values</strong> (this helps reduce variance).</li>
</ul>
<p>The first penalization reduces the depth of the tree while the second shrinks the size of the adjustments that will come from the latest tree.</p>
</div>
<div id="aggregation" class="section level3">
<h3><span class="header-section-number">7.4.3</span> Aggregation</h3>
<p>We aggregate both sections of the objective (loss and penalization). We write <span class="math inline">\(I_l\)</span> for the set of the indices of the instances belonging to leaf <span class="math inline">\(l\)</span>. Then,<br />
<span class="math display">\[\begin{align*}
O&amp;= 2\sum_{i=1}^I\left\{ -y_iT_J(\mathbf{x}_i)+m_{J-1}(\mathbf{x}_i) T_J(\mathbf{x}_i))+\frac{T_J(\mathbf{x}_i)^2}{2} \right\} + \gamma L+\frac{\lambda}{2}\sum_{l=1}^Lw_l^2 \\
&amp;=2\sum_{i=1}^I\left\{- y_iw_{q(\mathbf{x}_i)}+m_{J-1}(\mathbf{x}_i)w_{q(\mathbf{x}_i)})+\frac{w_{q(\mathbf{x}_i)}^2}{2} \right\} + \gamma L+\frac{\lambda}{2}\sum_{l=1}^Lw_l^2 \\
&amp;=2 \sum_{l=1}^L \left(w_l\sum_{i\in I_l}(-y_i +m_{J-1}(\mathbf{x}_i))+ \frac{w_l^2}{2}\sum_{i\in I_l}\left(1+\frac{\lambda}{2}\right)\right)+ \gamma L
\end{align*}\]</span><br />
The function is of the form <span class="math inline">\(aw_l+\frac{b}{2}w_l^2\)</span>, which has minimum values <span class="math inline">\(-\frac{a^2}{2b}\)</span> at point <span class="math inline">\(w_l=-a/b\)</span>. Thus, writing #(.) for the cardinal function that counts the number of items in a set,
<span class="math display" id="eq:xgbweight">\[\begin{align}
\tag{7.5}
\mathbf{\rightarrow} \quad w^*_l&amp;=\frac{\sum_{i\in I_l}(y_i -m_{J-1}(\mathbf{x}_i))}{\left(1+\frac{\lambda}{2}\right)\#\{i\in I_l\}}, \text{ so that} \\
O_L(q)&amp;=-\frac{1}{2}\sum_{l=1}^L \frac{\left(\sum_{i\in I_l}(y_i -m_{J-1}(\mathbf{x}_i))\right)^2}{\left(1+\frac{\lambda}{2}\right)\#\{i\in I_l\}}+\gamma L, \nonumber
\end{align}\]</span>
where we added the dependence of the objective both in <span class="math inline">\(q\)</span> (structure of tree) and <span class="math inline">\(L\)</span> (number of leaves). Indeed, the meta-shape of the tree remains to be determined.</p>
</div>
<div id="tree-structure" class="section level3">
<h3><span class="header-section-number">7.4.4</span> Tree structure</h3>
<p>Final problem: the <strong>tree structure</strong>! Let us take a step back. In the construction of a simple regression tree, the output value at each node is equal to the average value of the label within the node (or cluster). When adding a new tree in order to reduce the loss, the nodes values must be computed completely differently, which is the purpose of Equation <a href="trees.html#eq:xgbweight">(7.5)</a>.</p>
<p>Nonetheless, the growing of the iterative trees follows similar lines as simple trees. Features must be tested in order to pick the one that minimizes the objective for each given split. The final question is then: whatâ€™s the best depth and when to stop growing the tree? The method is to</p>
<ul>
<li>proceed node by node;<br />
</li>
<li>for each node, look at whether a split is useful (in terms of objective) or not: <span class="math display">\[\text{Gain}=\frac{1}{2}\left(\text{Gain}_L+\text{Gain}_R+\text{Gain}_O \right)-\gamma\]</span><br />
</li>
<li>each gain is computed with respect to the instances in each bucket (cluster): <span class="math display">\[\text{Gain}_\mathcal{X}= \frac{\left(\sum_{i\in I_\mathcal{X}}(y_i -m_{J-1}(\mathbf{x}_i))\right)^2}{\left(1+\frac{\lambda}{2}\right)\#\{i\in I_\mathcal{X}\}},\]</span>
where <span class="math inline">\(I_\mathcal{X}\)</span> is the set of instances within cluster <span class="math inline">\(\mathcal{X}\)</span>.</li>
</ul>
<p><span class="math inline">\(\text{Gain}_O\)</span> is the original gain (no split) and <span class="math inline">\(\text{Gain}_L\)</span> and <span class="math inline">\(\text{Gain}_R\)</span> are the gains of the left and right cluster, respectively. One work about the <span class="math inline">\(-\gamma\)</span> adjustment in the above formula: there is one unit of new leaves (two new minus one old)! This makes a one leaf difference, hence <span class="math inline">\(\Delta L =1\)</span> and the penalization intensity for each new leaf is equal to <span class="math inline">\(\gamma\)</span>.</p>
<p>Lastly, we underline the fact that XGBoost also applies a learning rate: each new tree is scaled by a factor <span class="math inline">\(\eta\)</span>, with <span class="math inline">\(\eta \in (0,1]\)</span>. This is very useful because a pure aggregation of 100 optimized trees is the best way to overfit the training sample.</p>
</div>
<div id="boostext" class="section level3">
<h3><span class="header-section-number">7.4.5</span> Extensions</h3>
<p>Several additional features are available to further prevent boosted trees to overfit. Indeed, given a sufficiently large number of trees, the aggregation is able to match the training sample very well, but may fail to generalize well out-of-sample.</p>
<p>Following the pioneering work of <span class="citation">Srivastava et al. (<a href="#ref-srivastava2014dropout">2014</a>)</span>, the DART (Dropout for Additive Regression Trees) model was proposed by <span class="citation">Rashmi and Gilad-Bachrach (<a href="#ref-rashmi2015dart">2015</a>)</span>. The idea is to omit a specified number of trees during training. The trees that are removed from the model are chosen randomly. The full specifications can be found at <a href="https://xgboost.readthedocs.io/en/latest/tutorials/dart.html" class="uri">https://xgboost.readthedocs.io/en/latest/tutorials/dart.html</a>.</p>
<p>Monotonicity constraints are another element that is featured both in xgboost and lightgbm. Sometimes, it is expected that one particular feature has a monotonous impact on the label. For instance, if one deeply believes in momentum, then past returns should have an increasing impact on future returns (in the cross-section of stocks).</p>
<p>Given the recursive nature of the splitting algorithm, it is possible to choose when to perform a split (according to a particular variable) and when not to. In Figure , we show how the algorithm proceeds. All splits are performed according to the same feature. For the first split, things are easy because it suffices to verify that the averages of each cluster are ranked in the right direction. Things are more complicated for the splits that occur below. Indeed, the average values set by all above splits matter as they give bounds for acceptable values for the future average values in lower splits. If a split violates these bounds, then it is overlooked and another variable will be chosen instead.</p>
<div class="figure" style="text-align: center"><span id="fig:monotonic"></span>
<img src="images/tree_monotonic.png" alt="Imposing monotonic constraints. The constraints are shown in bold blue in the bottom leaves." width="590" />
<p class="caption">
FIGURE 7.6: Imposing monotonic constraints. The constraints are shown in bold blue in the bottom leaves.
</p>
</div>
</div>
<div id="boostcode" class="section level3">
<h3><span class="header-section-number">7.4.6</span> Code and results</h3>
<p>In this section, we train a model using the <em>XGBoost</em> library. Other options include <em>catboost</em>, <em>gbm</em>, <em>lightgbm</em>, and <em>h2o</em>â€™s own version of boosted machines. Unlike many other packages, the XGBoost function requires a particular syntax and dedicated formats. The first step is thus to encapsulate the data accordingly.</p>
<p>Moreover, because training times can be long, we shorten the training sample as advocated in <span class="citation">Coqueret and Guida (<a href="#ref-coqueret2019training">2019</a>)</span>. We retain only the 40% most extreme observations (in terms of label values: top 20% and bottom 20%) and work with the small subset of features. In all coding sections dedicated to boosted trees in this book, the models will be trained with only 7 features.</p>

<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb61-1" data-line-number="1"><span class="kw">library</span>(xgboost)                                                <span class="co"># The package for boosted trees</span></a>
<a class="sourceLine" id="cb61-2" data-line-number="2">train_features_xgb &lt;-<span class="st"> </span>training_sample <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb61-3" data-line-number="3"><span class="st">    </span><span class="kw">filter</span>(R1M_Usd <span class="op">&lt;</span><span class="st"> </span><span class="kw">quantile</span>(R1M_Usd, <span class="fl">0.2</span>) <span class="op">|</span><span class="st"> </span></a>
<a class="sourceLine" id="cb61-4" data-line-number="4"><span class="st">               </span>R1M_Usd <span class="op">&gt;</span><span class="st"> </span><span class="kw">quantile</span>(R1M_Usd, <span class="fl">0.8</span>)) <span class="op">%&gt;%</span><span class="st">            </span><span class="co"># Extreme values only!</span></a>
<a class="sourceLine" id="cb61-5" data-line-number="5"><span class="st">    </span><span class="kw">select</span>(features_short) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.matrix</span>()                      <span class="co"># Independent variable</span></a>
<a class="sourceLine" id="cb61-6" data-line-number="6">train_label_xgb &lt;-<span class="st"> </span>training_sample <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb61-7" data-line-number="7"><span class="st">    </span><span class="kw">filter</span>(R1M_Usd <span class="op">&lt;</span><span class="st"> </span><span class="kw">quantile</span>(R1M_Usd, <span class="fl">0.2</span>) <span class="op">|</span><span class="st"> </span></a>
<a class="sourceLine" id="cb61-8" data-line-number="8"><span class="st">               </span>R1M_Usd <span class="op">&gt;</span><span class="st"> </span><span class="kw">quantile</span>(R1M_Usd, <span class="fl">0.8</span>)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb61-9" data-line-number="9"><span class="st">    </span><span class="kw">select</span>(R1M_Usd) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.matrix</span>()                             <span class="co"># Dependent variable</span></a>
<a class="sourceLine" id="cb61-10" data-line-number="10">train_matrix_xgb &lt;-<span class="st"> </span><span class="kw">xgb.DMatrix</span>(<span class="dt">data =</span> train_features_xgb, </a>
<a class="sourceLine" id="cb61-11" data-line-number="11">                                <span class="dt">label =</span> train_label_xgb)        <span class="co"># XGB format!</span></a></code></pre></div>
<p></p>
<p>The second (optional) step is to determine the monotonicity constraints that we want to impose. For simplicity, we will only enforce three constraints on</p>
<ol style="list-style-type: decimal">
<li>market capitalization (negative, because large firms have smaller returns under the size anomaly);<br />
</li>
<li>price-to-book ratio (negative, because overvalued forms also have smaller returns under the value anomaly);<br />
</li>
<li>past annual returns (positive, because winner outperform losers under the momentum anomaly).</li>
</ol>

<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb62-1" data-line-number="1">mono_const &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">length</span>(features))                   <span class="co"># Initialize the vector</span></a>
<a class="sourceLine" id="cb62-2" data-line-number="2">mono_const[<span class="kw">which</span>(features <span class="op">==</span><span class="st"> &quot;Mkt_Cap_12M_Usd&quot;</span>)] &lt;-<span class="st"> </span>(<span class="op">-</span><span class="dv">1</span>) <span class="co"># Decreasing in market cap</span></a>
<a class="sourceLine" id="cb62-3" data-line-number="3">mono_const[<span class="kw">which</span>(features <span class="op">==</span><span class="st"> &quot;Pb&quot;</span>)] &lt;-<span class="st"> </span>(<span class="op">-</span><span class="dv">1</span>)              <span class="co"># Decreasing in price-to-book</span></a>
<a class="sourceLine" id="cb62-4" data-line-number="4">mono_const[<span class="kw">which</span>(features <span class="op">==</span><span class="st"> &quot;Mom_11M_Usd&quot;</span>)] &lt;-<span class="st"> </span><span class="dv">1</span>        <span class="co"># Increasing in past return</span></a></code></pre></div>
<p></p>
<p>The third step is to train the model on the formatted training data.</p>

<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb63-1" data-line-number="1">fit_xgb &lt;-<span class="st"> </span><span class="kw">xgb.train</span>(<span class="dt">data =</span> train_matrix_xgb,     <span class="co"># Data source </span></a>
<a class="sourceLine" id="cb63-2" data-line-number="2">              <span class="dt">eta =</span> <span class="fl">0.3</span>,                          <span class="co"># Learning rate</span></a>
<a class="sourceLine" id="cb63-3" data-line-number="3">              <span class="dt">objective =</span> <span class="st">&quot;reg:linear&quot;</span>,           <span class="co"># Objective function</span></a>
<a class="sourceLine" id="cb63-4" data-line-number="4">              <span class="dt">max_depth =</span> <span class="dv">4</span>,                      <span class="co"># Maximum depth of trees</span></a>
<a class="sourceLine" id="cb63-5" data-line-number="5">              <span class="dt">lambda =</span> <span class="dv">1</span>,                         <span class="co"># Penalisation of leaf values</span></a>
<a class="sourceLine" id="cb63-6" data-line-number="6">              <span class="dt">gamma =</span> <span class="fl">0.1</span>,                        <span class="co"># Penalisation of number of leaves</span></a>
<a class="sourceLine" id="cb63-7" data-line-number="7">              <span class="dt">nrounds =</span> <span class="dv">30</span>,                       <span class="co"># Number of trees used (rather low here)</span></a>
<a class="sourceLine" id="cb63-8" data-line-number="8">              <span class="dt">monotone_constraints =</span> mono_const,  <span class="co"># Monotonicity constraints</span></a>
<a class="sourceLine" id="cb63-9" data-line-number="9">              <span class="dt">verbose =</span> <span class="dv">0</span>                         <span class="co"># No comment from the algo </span></a>
<a class="sourceLine" id="cb63-10" data-line-number="10">    )</a></code></pre></div>
<p></p>
<p>Finally, we evaluate the performance of the model. Note that before that, a proper formatting of the testing sample is required.</p>

<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb64-1" data-line-number="1">xgb_test &lt;-<span class="st"> </span>testing_sample <span class="op">%&gt;%</span><span class="st">                                </span><span class="co"># Test sample =&gt; XGB format</span></a>
<a class="sourceLine" id="cb64-2" data-line-number="2"><span class="st">    </span><span class="kw">select</span>(features_short) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb64-3" data-line-number="3"><span class="st">    </span><span class="kw">as.matrix</span>() </a>
<a class="sourceLine" id="cb64-4" data-line-number="4"><span class="kw">mean</span>((<span class="kw">predict</span>(fit_xgb, xgb_test) <span class="op">-</span><span class="st"> </span>testing_sample<span class="op">$</span>R1M_Usd)<span class="op">^</span><span class="dv">2</span>) <span class="co"># MSE</span></a></code></pre></div>
<pre><code>## [1] 0.03804396</code></pre>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb66-1" data-line-number="1"><span class="kw">mean</span>(<span class="kw">predict</span>(fit_xgb, xgb_test) <span class="op">*</span><span class="st"> </span>testing_sample<span class="op">$</span>R1M_Usd <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>) <span class="co"># Hit ratio</span></a></code></pre></div>
<pre><code>## [1] 0.5047146</code></pre>
<p></p>
<p>The performance is comparable to those observed for other predictive tools. As a final exercise, we show one implementation of a classification task under XGBoost. Only the label changes. In XGBoost, labels must be coded with integer number, starting at zero exactly. In R, factors are numerically coded as integers numbers starting from one, hence the mapping is simple.</p>

<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb68-1" data-line-number="1">train_label_C &lt;-<span class="st"> </span>training_sample <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb68-2" data-line-number="2"><span class="st">    </span><span class="kw">filter</span>(R1M_Usd <span class="op">&lt;</span><span class="st"> </span><span class="kw">quantile</span>(R1M_Usd, <span class="fl">0.2</span>) <span class="op">|</span><span class="st">          </span><span class="co"># Either low 20% returns </span></a>
<a class="sourceLine" id="cb68-3" data-line-number="3"><span class="st">               </span>R1M_Usd <span class="op">&gt;</span><span class="st"> </span><span class="kw">quantile</span>(R1M_Usd, <span class="fl">0.8</span>)) <span class="op">%&gt;%</span><span class="st">   </span><span class="co"># Or top 20% returns</span></a>
<a class="sourceLine" id="cb68-4" data-line-number="4"><span class="st">    </span><span class="kw">select</span>(R1M_Usd_C)</a>
<a class="sourceLine" id="cb68-5" data-line-number="5">train_matrix_C &lt;-<span class="st"> </span><span class="kw">xgb.DMatrix</span>(<span class="dt">data =</span> train_features_xgb, </a>
<a class="sourceLine" id="cb68-6" data-line-number="6">                              <span class="dt">label =</span> <span class="kw">as.numeric</span>(train_label_C <span class="op">==</span><span class="st"> &quot;TRUE&quot;</span>)) <span class="co"># XGB format!</span></a></code></pre></div>
<p></p>
<p>When working with categories, the loss function is usually the softmax function (see Section <a href="notdata.html#notations">2.1</a>).</p>

<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb69-1" data-line-number="1">fit_xgb_C &lt;-<span class="st">  </span><span class="kw">xgb.train</span>(<span class="dt">data =</span> train_matrix_C,  <span class="co"># Data source (pipe input)</span></a>
<a class="sourceLine" id="cb69-2" data-line-number="2">              <span class="dt">eta =</span> <span class="fl">0.8</span>,                        <span class="co"># Learning rate</span></a>
<a class="sourceLine" id="cb69-3" data-line-number="3">              <span class="dt">objective =</span> <span class="st">&quot;multi:softmax&quot;</span>,      <span class="co"># Objective function</span></a>
<a class="sourceLine" id="cb69-4" data-line-number="4">              <span class="dt">num_class =</span> <span class="dv">2</span>,                    <span class="co"># Number of classes</span></a>
<a class="sourceLine" id="cb69-5" data-line-number="5">              <span class="dt">max_depth =</span> <span class="dv">4</span>,                    <span class="co"># Maximum depth of trees</span></a>
<a class="sourceLine" id="cb69-6" data-line-number="6">              <span class="dt">nrounds =</span> <span class="dv">10</span>,                     <span class="co"># Number of trees used</span></a>
<a class="sourceLine" id="cb69-7" data-line-number="7">              <span class="dt">verbose =</span> <span class="dv">0</span>                       <span class="co"># No warning message </span></a>
<a class="sourceLine" id="cb69-8" data-line-number="8">    )</a></code></pre></div>
<p></p>
<p>We can then proceed to the assessment of the quality of the model. We adjust the prediction to the value of the true label and count the proportion of accurate forecasts.</p>

<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb70-1" data-line-number="1"><span class="kw">mean</span>(<span class="kw">predict</span>(fit_xgb_C, xgb_test) <span class="op">+</span><span class="st"> </span><span class="dv">1</span> <span class="op">==</span><span class="st"> </span><span class="kw">as.numeric</span>(testing_sample<span class="op">$</span>R1M_Usd_C)) <span class="co"># Hit ratio</span></a></code></pre></div>
<pre><code>## [1] 0.495613</code></pre>
<p></p>
<p>Consistently with the previous classification attempts, the results are underwhelming, as if switching to binary labels incurred a loss of information.</p>
</div>
<div id="instweight" class="section level3">
<h3><span class="header-section-number">7.4.7</span> Instance weighting</h3>
<p>In the computation of the aggregate loss, it is possible to introduce some flexibility and assign weight to instances:
<span class="math display">\[O=\underbrace{\sum_{i=1}^I\mathcal{W}_i \times \text{loss}(y_i,\tilde{y}_i)}_{\text{weighted error term}} \quad + \underbrace{\sum_{j=1}^J\Omega(T_j)}_{\text{regularisation term (unchanged)}}.\]</span></p>
<p>In factor investing these weights can very well depend on the feature values (<span class="math inline">\(\mathcal{W}_i=\mathcal{W}_i(\textbf{x}_i)\)</span>). For instance, for one particular characteristic <span class="math inline">\(\textbf{x}^k\)</span>, weights can be increasing thereby giving more importance to assets with high values of this characteristic (e.g., value stocks are favored compared to growth stocks). One other option is to increase weights when the values of the characteristic become more extreme (deep value and deep growth stocks have larger weight). If the features are uniform, the weights can simply be <span class="math inline">\(\mathcal{W}_i(x_i^k)\propto|x_i^k-0.5|\)</span>: firms with median value 0.5 have zero weight and as the feature value shifts towards 0 or 1, the weight increases. Specifying weights on instances biases the learning process just like views introduced Ã  la <span class="citation">Black and Litterman (<a href="#ref-black1992global">1992</a>)</span> influence the asset allocation process. The difference is that the nudge is performed well ahead of the portfolio choice problem.</p>
<p>In xgboost, the implementation instance weighting is done very early, in the definition of the xgb.DMatrix:</p>

<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb72-1" data-line-number="1">inst_weights &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="kw">nrow</span>(train_features_xgb))               <span class="co"># Random weights</span></a>
<a class="sourceLine" id="cb72-2" data-line-number="2">inst_weights &lt;-<span class="st"> </span>inst_weights <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(inst_weights)              <span class="co"># Normalization</span></a>
<a class="sourceLine" id="cb72-3" data-line-number="3">train_matrix_xgb &lt;-<span class="st"> </span><span class="kw">xgb.DMatrix</span>(<span class="dt">data =</span> train_features_xgb, </a>
<a class="sourceLine" id="cb72-4" data-line-number="4">                                <span class="dt">label =</span> train_label_xgb,</a>
<a class="sourceLine" id="cb72-5" data-line-number="5">                                <span class="dt">weight =</span> inst_weights)        <span class="co"># Weights!</span></a></code></pre></div>
<p></p>
<p>Then, in the subsequent stages, the optimization will be performed with these hard-coded weights. The splitting points can be altered (via the total weighted loss in clusters) and the terminal weight values <a href="trees.html#eq:xgbweight">(7.5)</a> are also impacted.</p>
</div>
</div>
<div id="discussion" class="section level2">
<h2><span class="header-section-number">7.5</span> Discussion</h2>
<p>We end this chapter by a discussion on the choice of predictive engine with a view towards portfolio construction. As recalled in Chapter <a href="intro.html#intro">3</a>, the ML signal is just one building stage of construction of the investment strategy. At some point, this signal must be translated into portfolio weights.</p>
<p>From this perspective, simple trees appear suboptimal. Tree depth are usually set between 3 and 6. This implies between 8 and 64 terminal leaves at most, with possibly very unbalanced clusters. The likelihood of having one cluster with 20% to 30% of the sample is high. This means that when it comes to predictions, roughly 20% to 30% of the instances will be given the same value.</p>
<p>On the other side of the process, portfolio policies commonly have a fixed number of assets. Thus, having assets with equal signal does not permit to discriminate and select a subset to be included in the portfolio. For instance, if the policy requires exactly 100 stocks and 105 stocks have the same signal, the signal cannot be used for selection purposes. It would have to be combined with exogenous information such as the covariance matrix in a mean-variance type allocation.</p>
<p>Overall, this is one reason to prefer aggregate models. When the number of learners is sufficiently large (5 is almost enough), the predictions for assets will be unique and tailored to these assets. It then becomes possible to discriminate via the signal and select only those assets that have the most favorable signal. In practice, random forests and boosted trees are probably the best choices.</p>
</div>
<div id="coding-exercises-3" class="section level2">
<h2><span class="header-section-number">7.6</span> Coding exercises</h2>

</div>
</div>
<h3><span class="header-section-number">C</span> References</h3>
<div id="refs" class="references">
<div id="ref-breiman1984classification">
<p>Breiman, Leo, Jerome Friedman, Charles J. Stone, and R.A. Olshen. 1984. <em>Classification and Regression Trees</em>. Chapman; Hall.</p>
</div>
<div id="ref-ballings2015evaluating">
<p>Ballings, Michel, Dirk Van den Poel, Nathalie Hespeels, and Ruben Gryp. 2015. â€œEvaluating Multiple Classifiers for Stock Price Direction Prediction.â€ <em>Expert Systems with Applications</em> 42 (20): 7046â€“56.</p>
</div>
<div id="ref-patel2015predicting">
<p>Patel, Jigar, Sahil Shah, Priyank Thakkar, and K Kotecha. 2015a. â€œPredicting Stock and Stock Price Index Movement Using Trend Deterministic Data Preparation and Machine Learning Techniques.â€ <em>Expert Systems with Applications</em> 42 (1): 259â€“68.</p>
</div>
<div id="ref-patel2015bpredicting">
<p>Patel, Jigar, Sahil Shah, Priyank Thakkar, and Ketan Kotecha. 2015b. â€œPredicting Stock Market Index Using Fusion of Machine Learning Techniques.â€ <em>Expert Systems with Applications</em> 42 (4): 2162â€“72.</p>
</div>
<div id="ref-moritz2016tree">
<p>Moritz, Benjamin, and Tom Zimmermann. 2016. â€œTree-Based Conditional Portfolio Sorts: The Relation Between Past and Future Stock Returns.â€ <em>SSRN Working Paper</em> 2740751.</p>
</div>
<div id="ref-krauss2017deep">
<p>Krauss, Christopher, Xuan Anh Do, and Nicolas Huck. 2017. â€œDeep Neural Networks, Gradient-Boosted Trees, Random Forests: Statistical Arbitrage on the S&amp;P 500.â€ <em>European Journal of Operational Research</em> 259 (2): 689â€“702.</p>
</div>
<div id="ref-gu2018empirical">
<p>Gu, Shihao, Bryan T Kelly, and Dacheng Xiu. 2018. â€œEmpirical Asset Pricing via Machine Learning.â€ <em>SSRN Working Paper</em> 3159577.</p>
</div>
<div id="ref-guida2019big">
<p>Guida, Tony, and Guillaume Coqueret. 2018a. â€œEnsemble Learning Applied to Quant Equity: Gradient Boosting in a Multifactor Framework.â€ In <em>Big Data and Machine Learning in Quantitative Investment</em>, 129â€“48. Wiley.</p>
</div>
<div id="ref-coqueret2019training">
<p>Coqueret, Guillaume, and Tony Guida. 2019. â€œTraining Trees on Tails with Applications to Portfolio Choice.â€ <em>SSRN Working Paper</em> 3403009.</p>
</div>
<div id="ref-simonian2019machine">
<p>Simonian, Joseph, Chenwei Wu, Daniel Itano, and Vyshaal Narayanam. 2019. â€œA Machine Learning Approach to Risk Factors: A Case Study Using the Fama-French-Carhart Model.â€ <em>Journal of Financial Data Science</em> 1 (1): 32â€“44.</p>
</div>
<div id="ref-bryzgalova2019forest">
<p>Bryzgalova, Svetlana, Markus Pelger, and Jason Zhu. 2019. â€œForest Through the Trees: Building Cross-Sections of Stock Returns.â€ <em>SSRN Working Paper</em> 3493458.</p>
</div>
<div id="ref-friedman2009elements">
<p>Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning</em>. Springer.</p>
</div>
<div id="ref-schapire1990strength">
<p>Schapire, Robert E. 1990. â€œThe Strength of Weak Learnability.â€ <em>Machine Learning</em> 5 (2): 197â€“227.</p>
</div>
<div id="ref-ho1995random">
<p>Ho, Tin Kam. 1995. â€œRandom Decision Forests.â€ In <em>Proceedings of 3rd International Conference on Document Analysis and Recognition</em>, 1:278â€“82. IEEE.</p>
</div>
<div id="ref-breiman2001random">
<p>Breiman, Leo. 2001. â€œRandom Forests.â€ <em>Machine Learning</em> 45 (1): 5â€“32.</p>
</div>
<div id="ref-huck2019large">
<p>Huck, Nicolas. 2019. â€œLarge Data Sets and Machine Learning: Applications to Statistical Arbitrage.â€ <em>European Journal of Operational Research</em> 278 (1). Elsevier: 330â€“42.</p>
</div>
<div id="ref-biau2012analysis">
<p>Biau, GÃ©rard. 2012. â€œAnalysis of a Random Forests Model.â€ <em>Journal of Machine Learning Research</em> 13 (Apr): 1063â€“95.</p>
</div>
<div id="ref-scornet2015consistency">
<p>Scornet, Erwan, GÃ©rard Biau, Jean-Philippe Vert, and others. 2015. â€œConsistency of Random Forests.â€ <em>Annals of Statistics</em> 43 (4). Institute of Mathematical Statistics: 1716â€“41.</p>
</div>
<div id="ref-biau2008consistency">
<p>Biau, GÃ©rard, Luc Devroye, and GAbor Lugosi. 2008. â€œConsistency of Random Forests and Other Averaging Classifiers.â€ <em>Journal of Machine Learning Research</em> 9 (Sep): 2015â€“33.</p>
</div>
<div id="ref-denil2014narrowing">
<p>Denil, Misha, David Matheson, and Nando De Freitas. 2014. â€œNarrowing the Gap: Random Forests in Theory and in Practice.â€ In <em>International Conference on Machine Learning</em>, 665â€“73.</p>
</div>
<div id="ref-freund1997decision">
<p>Freund, Yoav, and Robert E Schapire. 1997. â€œA Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting.â€ <em>Journal of Computer and System Sciences</em> 55 (1): 119â€“39.</p>
</div>
<div id="ref-freund1996experiments">
<p>Freund, Yoav, and Robert E Schapire. 1996. â€œExperiments with a New Boosting Algorithm.â€ In <em>Machine Learning: Proceedings of the Thirteenth International Conference</em>, 96:148â€“56.</p>
</div>
<div id="ref-schapire2012boosting">
<p>Schapire, Robert E, and Yoav Freund. 2012. <em>Boosting: Foundations and Algorithms</em>. MIT press.</p>
</div>
<div id="ref-friedman2000additive">
<p>Friedman, Jerome, Trevor Hastie, Robert Tibshirani, and others. 2000. â€œAdditive Logistic Regression: A Statistical View of Boosting (with Discussion and a Rejoinder by the Authors).â€ <em>Annals of Statistics</em> 28 (2): 337â€“407.</p>
</div>
<div id="ref-drucker1997improving">
<p>Drucker, Harris. 1997. â€œImproving Regressors Using Boosting Techniques.â€ In <em>International Conference on Machine Learning</em>, 97:107â€“15.</p>
</div>
<div id="ref-breiman2004population">
<p>Breiman, Leo, and others. 2004. â€œPopulation Theory for Boosting Ensembles.â€ <em>Annals of Statistics</em> 32 (1): 1â€“11.</p>
</div>
<div id="ref-schapire2003boosting">
<p>Schapire, Robert E. 2003. â€œThe Boosting Approach to Machine Learning: An Overview.â€ In <em>Nonlinear Estimation and Classification</em>, 149â€“71. Springer.</p>
</div>
<div id="ref-ridgeway1999boosting">
<p>Ridgeway, Greg, David Madigan, and Thomas Richardson. 1999. â€œBoosting Methodology for Regression Problems.â€ In <em>AISTATS</em>.</p>
</div>
<div id="ref-ting2002instance">
<p>Ting, Kai Ming. 2002. â€œAn Instance-Weighting Method to Induce Cost-Sensitive Trees.â€ <em>IEEE Transactions on Knowledge &amp; Data Engineering</em>, no. 3: 659â€“65.</p>
</div>
<div id="ref-mason2000boosting">
<p>Mason, Llew, Jonathan Baxter, Peter L Bartlett, and Marcus R Frean. 2000. â€œBoosting Algorithms as Gradient Descent.â€ In <em>Advances in Neural Information Processing Systems</em>, 512â€“18.</p>
</div>
<div id="ref-friedman2001greedy">
<p>Friedman, Jerome H. 2001. â€œGreedy Function Approximation: A Gradient Boosting Machine.â€ <em>Annals of Statistics</em>, 1189â€“1232.</p>
</div>
<div id="ref-friedman2002stochastic">
<p>Friedman, Jerome H. 2002. â€œStochastic Gradient Boosting.â€ <em>Computational Statistics &amp; Data Analysis</em> 38 (4): 367â€“78.</p>
</div>
<div id="ref-chen2016xgboost">
<p>Chen, Tianqi, and Carlos Guestrin. 2016. â€œXgboost: A Scalable Tree Boosting System.â€ In <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, 785â€“94. ACM.</p>
</div>
<div id="ref-ke2017lightgbm">
<p>Ke, Guolin, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 2017. â€œLightgbm: A Highly Efficient Gradient Boosting Decision Tree.â€ In <em>Advances in Neural Information Processing Systems</em>, 3146â€“54.</p>
</div>
<div id="ref-srivastava2014dropout">
<p>Srivastava, Nitish, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. â€œDropout: A Simple Way to Prevent Neural Networks from Overfitting.â€ <em>Journal of Machine Learning Research</em> 15 (1): 1929â€“58.</p>
</div>
<div id="ref-rashmi2015dart">
<p>Rashmi, Korlakai Vinayak, and Ran Gilad-Bachrach. 2015. â€œDART: Dropouts Meet Multiple Additive Regression Trees.â€ In <em>AISTATS</em>, 489â€“97.</p>
</div>
<div id="ref-black1992global">
<p>Black, Fischer, and Robert Litterman. 1992. â€œGlobal Portfolio Optimization.â€ <em>Financial Analysts Journal</em> 48 (5): 28â€“43.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="9">
<li id="fn9"><p>See www.kaggle.com. <a href="trees.html#fnref9" class="footnote-back">â†©</a></p></li>
<li id="fn10"><p>See, e.g., <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-r/docs/reference/h2o.randomForest.html" class="uri">http://docs.h2o.ai/h2o/latest-stable/h2o-r/docs/reference/h2o.randomForest.html</a><a href="trees.html#fnref10" class="footnote-back">â†©</a></p></li>
<li id="fn11"><p>The Real Adaboost of <span class="citation">Friedman et al. (<a href="#ref-friedman2000additive">2000</a>)</span> has a different output: the probability of belonging to a particular class.<a href="trees.html#fnref11" class="footnote-back">â†©</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="lasso.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="NN.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": true,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"],
"instapaper": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"download": ["ML_factor.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true,
"info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
